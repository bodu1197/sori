# MusicGram Backend API
# 200Îßå DAU ÎåÄÏùë ÌôïÏû• Í∞ÄÎä• ÏÑ§Í≥Ñ

from fastapi import FastAPI, Request, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from datetime import datetime, timedelta, timezone
# Trigger CI/CD Test: 2025-12-24 (Retry: Fixed google-generativeai dependency)
import os
import json
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
from ai_agent import (
    generate_artist_persona, chat_with_artist, generate_artist_post,
    generate_artist_comment, generate_artist_dm,
    detect_language, translate_text, generate_artist_post_factbased,
    generate_post_image, check_artist_status,
    gather_artist_context, generate_contextual_post
)
import uuid as uuid_lib
import random

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# Constants - Avoid duplicate string literals (SonarCloud fix)
# =============================================================================
CONTENT_TYPE_JSON = "application/json"
ERROR_DB_NOT_CONFIGURED = "Database not configured"
ERROR_DB_NOT_AVAILABLE = "DB not available"
ERROR_ARTIST_NOT_FOUND = "Artist not found"
THUMBNAIL_MAXRES = "maxresdefault.jpg"
TIMEZONE_SUFFIX = "+00:00"

# =============================================================================
# Supabase ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ (ÏòÅÍµ¨ Ï†ÄÏû•ÏÜå)
# =============================================================================
supabase_client = None
try:
    from supabase import create_client, Client
    supabase_url = os.getenv("SUPABASE_URL")
    supabase_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
    if supabase_url and supabase_key:
        supabase_client: Client = create_client(supabase_url, supabase_key)
        logger.info("Supabase connected!")
except ImportError:
    logger.warning("Supabase not installed")
except Exception as e:
    logger.warning(f"Supabase connection failed: {e}")

# Redis Ï†úÍ±∞ - Supabase DBÎßå ÏÇ¨Ïö©
redis_client = None  # ÏÇ¨Ïö© ÏïàÌï®

# YTMusic Ïù∏Ïä§ÌÑ¥Ïä§ (Íµ≠Í∞ÄÎ≥Ñ)
ytmusic_instances = {}

# YouTube Music API ÏßÄÏõê Ïñ∏Ïñ¥: en, pt, ru, zh_CN, de, ja, ar, cs, tr, es, ur, it, hi, ko, nl, zh_TW, fr
# ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Ïñ∏Ïñ¥Îäî ÏòÅÏñ¥(en)Î°ú ÎåÄÏ≤¥
COUNTRY_LANGUAGE_MAP = {
    # ÏïÑÏãúÏïÑ
    'KR': 'ko', 'JP': 'ja', 'CN': 'zh_CN', 'TW': 'zh_TW', 'HK': 'zh_TW',
    'TH': 'en', 'VN': 'en', 'ID': 'en', 'MY': 'en', 'SG': 'en',
    'PH': 'en', 'IN': 'hi', 'PK': 'ur', 'BD': 'en', 'NP': 'en',
    'LK': 'en', 'MM': 'en', 'KH': 'en', 'LA': 'en', 'MN': 'en',
    # Ï§ëÎèô
    'SA': 'ar', 'AE': 'ar', 'EG': 'ar', 'IQ': 'ar', 'JO': 'ar',
    'KW': 'ar', 'LB': 'ar', 'OM': 'ar', 'QA': 'ar', 'YE': 'ar',
    'IL': 'en', 'IR': 'en', 'TR': 'tr',
    # Ïú†ÎüΩ
    'US': 'en', 'GB': 'en', 'AU': 'en', 'NZ': 'en', 'IE': 'en', 'CA': 'en',
    'DE': 'de', 'AT': 'de', 'CH': 'de',
    'FR': 'fr', 'BE': 'fr', 'LU': 'fr',
    'ES': 'es', 'MX': 'es', 'AR': 'es', 'CO': 'es', 'CL': 'es', 'PE': 'es',
    'IT': 'it', 'PT': 'pt', 'BR': 'pt',
    'NL': 'nl', 'PL': 'en', 'RU': 'ru', 'UA': 'en', 'CZ': 'cs', 'SK': 'en',
    'HU': 'en', 'RO': 'en', 'BG': 'en', 'HR': 'en', 'RS': 'en', 'SI': 'en',
    'GR': 'en', 'SE': 'en', 'NO': 'en', 'DK': 'en', 'FI': 'en', 'IS': 'en',
    'EE': 'en', 'LV': 'en', 'LT': 'en',
    # ÏïÑÌîÑÎ¶¨Ïπ¥
    'ZA': 'en', 'NG': 'en', 'KE': 'en', 'GH': 'en', 'TZ': 'en',
    'MA': 'ar', 'DZ': 'ar', 'TN': 'ar', 'LY': 'ar',
    'ET': 'en', 'UG': 'en', 'ZW': 'en', 'SN': 'fr', 'CI': 'fr', 'CM': 'fr',
}

def get_ytmusic(country: str = "US"):
    """Íµ≠Í∞ÄÎ≥Ñ YTMusic Ïù∏Ïä§ÌÑ¥Ïä§ (Ïã±Í∏ÄÌÜ§)"""
    from ytmusicapi import YTMusic

    country = country.upper() if country else "US"

    if country not in ytmusic_instances:
        lang = COUNTRY_LANGUAGE_MAP.get(country, 'en')
        ytmusic_instances[country] = YTMusic(language=lang, location=country)

    return ytmusic_instances[country]

def cache_get(key: str):
    """Ï∫êÏãú ÎπÑÌôúÏÑ±Ìôî - Ìï≠ÏÉÅ None Î∞òÌôò"""
    del key  # Intentionally unused - caching disabled
    return None

def cache_set(key: str, value, ttl: int = 3600):
    """Ï∫êÏãú ÎπÑÌôúÏÑ±Ìôî - ÏïÑÎ¨¥ ÎèôÏûë ÏïàÌï®"""
    del key, value, ttl  # Intentionally unused - caching disabled

# =============================================================================
# Helper: Run sync code in thread
# =============================================================================

async def run_in_thread(func, *args, **kwargs):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, lambda: func(*args, **kwargs))

# Error message constants
ERROR_ACCESS_TOKEN_REQUIRED = "access_token required"

# =============================================================================
# Ïñ∏Ïñ¥ Í∞êÏßÄ Ìï®Ïàò
# =============================================================================

# ÏïåÎ†§ÏßÑ K-pop Í∑∏Î£π (ÏòÅÏñ¥ Ïù¥Î¶ÑÏù¥ÏßÄÎßå ÌïúÍµ≠Ïñ¥ ÏÇ¨Ïö©)
KPOP_GROUPS = {
    "BTS", "BLACKPINK", "LE SSERAFIM", "NewJeans", "STAYC", "IVE", "aespa",
    "TWICE", "EXO", "NCT", "ENHYPEN", "TXT", "ITZY", "NMIXX", "(G)I-DLE",
    "Red Velvet", "SEVENTEEN", "Stray Kids", "ATEEZ", "THE BOYZ", "Kep1er",
    "fromis_9", "VIVIZ", "MAMAMOO", "Oh My Girl", "Lovelyz", "GFRIEND",
    "MONSTA X", "TREASURE", "iKON", "WINNER", "2NE1", "Big Bang", "BIGBANG",
    "Girls' Generation", "SNSD", "SHINee", "Super Junior", "f(x)", "miss A",
    "Wonder Girls", "2PM", "GOT7", "DAY6", "Xdinary Heroes", "ILLIT",
    "KISS OF LIFE", "BABYMONSTER", "tripleS", "ZEROBASEONE", "BOYNEXTDOOR",
    "RIIZE", "TWS", "QWER", "H1-KEY", "Billlie", "CLASS:y", "Weeekly"
}

def detect_artist_language(name: str, description: str = "") -> str:
    """
    ÏïÑÌã∞Ïä§Ìä∏ Ïù¥Î¶Ñ/ÏÑ§Î™ÖÏóêÏÑú Ï£ºÏöî Ïñ∏Ïñ¥ Í∞êÏßÄ

    Returns:
        "ko" - ÌïúÍµ≠Ïñ¥ (ÌïúÍ∏Ä Ìè¨Ìï® ÎòêÎäî K-pop Í∑∏Î£π)
        "ja" - ÏùºÎ≥∏Ïñ¥ (ÌûàÎùºÍ∞ÄÎÇò/Í∞ÄÌÉÄÏπ¥ÎÇò Ìè¨Ìï®)
        "zh" - Ï§ëÍµ≠Ïñ¥ (ÌïúÏûêÎßå Ìè¨Ìï®)
        "en" - ÏòÅÏñ¥ (Í∏∞Î≥∏Í∞í)
    """
    import re

    if not name:
        return "en"

    # 1. K-pop Í∑∏Î£π Ï≤¥ÌÅ¨ (ÏòÅÏñ¥ Ïù¥Î¶ÑÏù¥ÏßÄÎßå ÌïúÍµ≠ ÏïÑÌã∞Ïä§Ìä∏)
    name_normalized = name.strip()
    for kpop in KPOP_GROUPS:
        if kpop.lower() in name_normalized.lower():
            return "ko"

    # 2. ÌïúÍ∏Ä Í∞êÏßÄ (Í∞Ä-Ìû£)
    if re.search(r'[Í∞Ä-Ìû£]', name):
        return "ko"

    # 3. ÏùºÎ≥∏Ïñ¥ Í∞êÏßÄ (ÌûàÎùºÍ∞ÄÎÇò, Í∞ÄÌÉÄÏπ¥ÎÇò)
    if re.search(r'[\u3040-\u309F\u30A0-\u30FF]', name):
        return "ja"

    # 4. Ï§ëÍµ≠Ïñ¥ Í∞êÏßÄ (ÌïúÏûêÎßå ÏûàÍ≥† ÌïúÍ∏Ä/ÏùºÎ≥∏Ïñ¥ ÏóÜÏùÑ Îïå)
    if re.search(r'[\u4E00-\u9FFF]', name) and not re.search(r'[Í∞Ä-Ìû£\u3040-\u30FF]', name):
        return "zh"

    # 5. DescriptionÏóêÏÑúÎèÑ ÌôïÏù∏
    if description:
        desc_sample = description[:500]
        if re.search(r'[Í∞Ä-Ìû£]', desc_sample):
            return "ko"
        if re.search(r'[\u3040-\u309F\u30A0-\u30FF]', desc_sample):
            return "ja"

    return "en"


# =============================================================================
# Supabase DB Ìó¨Ìçº Ìï®Ïàò
# =============================================================================

def db_save_artist(artist_data: dict) -> str | None:
    """ÏïÑÌã∞Ïä§Ìä∏Î•º DBÏóê Ï†ÄÏû• (upsert) + ÏûêÎèô Í∞ÄÏÉÅÌöåÏõê ÏÉùÏÑ± + Ïñ∏Ïñ¥ ÏûêÎèô Í∞êÏßÄ"""
    if not supabase_client:
        return None

    try:
        browse_id = artist_data.get("browseId") or artist_data.get("browse_id")
        if not browse_id:
            return None

        thumbnails = artist_data.get("thumbnails") or []
        artist_name = artist_data.get("name") or artist_data.get("artist") or ""
        description = artist_data.get("description") or ""
        thumbnail_url = get_best_thumbnail(thumbnails)

        # ÏûêÎèô Ïñ∏Ïñ¥ Í∞êÏßÄ
        primary_language = detect_artist_language(artist_name, description)

        data = {
            "browse_id": browse_id,
            "name": artist_name,
            "thumbnails": json.dumps(thumbnails),
            "thumbnail_url": thumbnail_url,
            "description": description,
            "subscribers": artist_data.get("subscribers") or "",
            "primary_language": primary_language,  # Ïñ∏Ïñ¥ ÏûêÎèô ÏÑ§Ï†ï
            "last_updated": datetime.now(timezone.utc).isoformat()
        }

        result = supabase_client.table("music_artists").upsert(
            data, on_conflict="browse_id"
        ).execute()

        # ÏûêÎèô Í∞ÄÏÉÅÌöåÏõê ÏÉùÏÑ± (ÎπÑÎèôÍ∏∞ Î∞±Í∑∏ÎùºÏö¥Îìú)
        if result.data:
            try:
                # Check if virtual member already exists
                existing = supabase_client.table("profiles").select("id").eq("artist_browse_id", browse_id).execute()
                if not existing.data or len(existing.data) == 0:
                    # Create virtual member in background
                    create_virtual_member_sync(browse_id, artist_name, thumbnail_url)
            except Exception as vm_error:
                logger.warning(f"Virtual member auto-creation skipped: {vm_error}")

        if result.data:
            return result.data[0].get("id")
        return None
    except Exception as e:
        logger.warning(f"DB save artist error: {e}")
        return None


def create_virtual_member_sync(browse_id: str, artist_name: str, thumbnail_url: str) -> str | None:
    """Synchronously create a virtual member for an artist"""
    if not supabase_client:
        return None

    try:
        import uuid
        import requests

        virtual_email = f"{browse_id}@sori.virtual"
        random_password = str(uuid.uuid4())

        supabase_url = os.getenv("SUPABASE_URL")
        service_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

        if not supabase_url or not service_key:
            logger.warning("Supabase credentials not configured for virtual member creation")
            return None

        # Create auth user
        create_user_response = requests.post(
            f"{supabase_url}/auth/v1/admin/users",
            headers={
                "apikey": service_key,
                "Authorization": f"Bearer {service_key}",
                "Content-Type": CONTENT_TYPE_JSON
            },
            json={
                "email": virtual_email,
                "password": random_password,
                "email_confirm": True,
                "user_metadata": {
                    "full_name": artist_name,
                    "avatar_url": thumbnail_url,
                    "member_type": "artist",
                    "artist_browse_id": browse_id
                }
            },
            timeout=10
        )

        if create_user_response.status_code not in [200, 201]:
            return None

        user_data = create_user_response.json()
        user_id = user_data.get("id")

        # Generate username (ensure minimum 3 chars)
        base_username = artist_name.lower().replace(" ", "").replace(".", "").replace("-", "")[:20]
        if len(base_username) < 3:
            base_username = f"{base_username}_official"

        # Update profiles
        supabase_client.table("profiles").upsert({
            "id": user_id,
            "username": base_username,
            "full_name": artist_name,
            "avatar_url": thumbnail_url,
            "member_type": "artist",
            "artist_browse_id": browse_id,
            "is_verified": True,
            "bio": f"Official SORI profile for {artist_name}"
        }).execute()

        logger.info(f"Virtual member auto-created: {artist_name} ({browse_id})")
        return user_id

    except Exception as e:
        logger.warning(f"Virtual member sync creation failed: {e}")
        return None

def db_save_album(album_data: dict, artist_id: str = None) -> str | None:
    """Ïï®Î≤îÏùÑ DBÏóê Ï†ÄÏû• (upsert)"""
    if not supabase_client:
        return None

    try:
        browse_id = album_data.get("browseId") or album_data.get("browse_id")
        if not browse_id:
            return None

        # ÏïÑÌã∞Ïä§Ìä∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú
        artists = album_data.get("artists") or []
        artist_browse_id = None
        if artists and isinstance(artists, list) and len(artists) > 0:
            artist_browse_id = artists[0].get("id") or artists[0].get("browseId")

        data = {
            "browse_id": browse_id,
            "artist_browse_id": artist_browse_id or album_data.get("artist_bid"),
            "title": album_data.get("title") or "",
            "album_type": album_data.get("type") or "Album",
            "year": album_data.get("year") or "",
            "thumbnails": json.dumps(album_data.get("thumbnails") or []),
            "track_count": len(album_data.get("tracks") or []),
            "last_updated": datetime.now(timezone.utc).isoformat()
        }

        if artist_id:
            data["artist_id"] = artist_id

        result = supabase_client.table("music_albums").upsert(
            data, on_conflict="browse_id"
        ).execute()

        if result.data:
            return result.data[0].get("id")
        return None
    except Exception as e:
        logger.warning(f"DB save album error: {e}")
        return None

def db_save_track(track_data: dict, album_id: str = None, artist_id: str = None) -> str | None:
    """Ìä∏ÎûôÏùÑ DBÏóê Ï†ÄÏû• (upsert)"""
    if not supabase_client:
        return None

    try:
        video_id = track_data.get("videoId") or track_data.get("video_id")
        if not video_id:
            return None

        # ÏïÑÌã∞Ïä§Ìä∏ Ïù¥Î¶Ñ Ï∂îÏ∂ú
        artists = track_data.get("artists") or []
        artist_name = ""
        artist_browse_id = None
        if artists and isinstance(artists, list) and len(artists) > 0:
            artist_name = artists[0].get("name") or ""
            artist_browse_id = artists[0].get("id") or artists[0].get("browseId")

        # Ïï®Î≤î Ï†ïÎ≥¥ Ï∂îÏ∂ú
        album = track_data.get("album") or {}
        album_title = album.get("name") or ""
        album_browse_id = album.get("id") or album.get("browseId")

        # Ïû¨ÏÉù ÏãúÍ∞Ñ ÌååÏã±
        duration = track_data.get("duration") or ""
        duration_seconds = track_data.get("duration_seconds") or 0
        if duration and not duration_seconds:
            try:
                parts = duration.split(":")
                if len(parts) == 2:
                    duration_seconds = int(parts[0]) * 60 + int(parts[1])
                elif len(parts) == 3:
                    duration_seconds = int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
            except ValueError:
                pass

        data = {
            "video_id": video_id,
            "artist_browse_id": artist_browse_id or track_data.get("artist_bid"),
            "album_browse_id": album_browse_id,
            "title": track_data.get("title") or "",
            "artist_name": artist_name,
            "album_title": album_title,
            "duration": duration,
            "duration_seconds": duration_seconds,
            "thumbnails": json.dumps(track_data.get("thumbnails") or []),
            "is_explicit": track_data.get("isExplicit") or False
        }

        if album_id:
            data["album_id"] = album_id
        if artist_id:
            data["artist_id"] = artist_id

        result = supabase_client.table("music_tracks").upsert(
            data, on_conflict="video_id"
        ).execute()

        if result.data:
            return result.data[0].get("id")
        return None
    except Exception as e:
        logger.warning(f"DB save track error: {e}")
        return None

# =============================================================================
# Ï†ïÍ∑úÌôîÎêú DB Ìï®ÏàòÎì§ (ÏÉàÎ°úÏö¥ ÌÖåÏù¥Î∏î Íµ¨Ï°∞)
# =============================================================================

def db_get_artist_by_browse_id(browse_id: str) -> dict | None:
    """ÏïÑÌã∞Ïä§Ìä∏Î•º browse_idÎ°ú Ï°∞Ìöå"""
    if not supabase_client or not browse_id:
        return None
    try:
        result = supabase_client.table("music_artists").select("*").eq(
            "browse_id", browse_id
        ).single().execute()
        return result.data
    except Exception as e:
        logger.warning(f"DB get artist error: {e}")
        return None

def db_check_artist_needs_sync(browse_id: str, days: int = 7) -> bool:
    """ÏïÑÌã∞Ïä§Ìä∏Í∞Ä ÎèôÍ∏∞Ìôî ÌïÑÏöîÌïúÏßÄ ÌôïÏù∏ (last_synced_atÏù¥ NÏùº Ï¥àÍ≥º)"""
    if not supabase_client or not browse_id:
        return True
    try:
        result = supabase_client.table("music_artists").select(
            "last_synced_at"
        ).eq("browse_id", browse_id).single().execute()

        if not result.data or not result.data.get("last_synced_at"):
            return True

        last_synced = result.data.get("last_synced_at")
        last_time = datetime.fromisoformat(last_synced.replace("Z", TIMEZONE_SUFFIX))
        return datetime.now(timezone.utc) - last_time > timedelta(days=days)
    except Exception:
        return True

def db_save_artist_full(artist_data: dict) -> bool:
    """ÏïÑÌã∞Ïä§Ìä∏ Ï†ïÎ≥¥Î•º Ï†ïÍ∑úÌôî ÌÖåÏù¥Î∏îÏóê Ï†ÄÏû• (upsert) + Ïñ∏Ïñ¥ ÏûêÎèô Í∞êÏßÄ"""
    if not supabase_client or not artist_data:
        return False
    try:
        browse_id = artist_data.get("browseId")
        if not browse_id:
            return False

        # Ïù∏Í∏∞Í≥° ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ IDÎßå Ï∂îÏ∂ú (YouTube IFrame APIÏö©)
        songs_playlist_id = artist_data.get("songsPlaylistId")
        artist_name = artist_data.get("artist") or artist_data.get("name") or ""
        description = artist_data.get("description") or ""

        # ÏûêÎèô Ïñ∏Ïñ¥ Í∞êÏßÄ
        primary_language = detect_artist_language(artist_name, description)

        data = {
            "browse_id": browse_id,
            "name": artist_name,
            "thumbnail_url": get_best_thumbnail(artist_data.get("thumbnails", [])),
            "subscribers": artist_data.get("subscribers") or "",
            "description": description,
            "primary_language": primary_language,  # Ïñ∏Ïñ¥ ÏûêÎèô ÏÑ§Ï†ï
            "top_songs_json": artist_data.get("topSongs") or [],
            "related_artists_json": artist_data.get("related") or [],
            "songs_playlist_id": songs_playlist_id,
            "updated_at": datetime.now(timezone.utc).isoformat(),
            "last_synced_at": datetime.now(timezone.utc).isoformat()
        }

        supabase_client.table("music_artists").upsert(
            data, on_conflict="browse_id"
        ).execute()

        logger.info(f"Artist saved: {data['name']} ({browse_id}) lang: {primary_language}")

        # ÏûêÎèô Í∞ÄÏÉÅÌöåÏõê ÏÉùÏÑ± (ÎπÑÎèôÍ∏∞ Î∞±Í∑∏ÎùºÏö¥Îìú)
        try:
            existing = supabase_client.table("profiles").select("id").eq("artist_browse_id", browse_id).execute()
            if not existing.data or len(existing.data) == 0:
                create_virtual_member_sync(browse_id, data['name'], data['thumbnail_url'])
        except Exception as vm_error:
            logger.warning(f"Virtual member auto-creation skipped: {vm_error}")

        return True
    except Exception as e:
        logger.warning(f"DB save artist error: {e}")
        return False

def db_save_album(album_data: dict, artist_browse_id: str) -> bool:
    """Ïï®Î≤î Ï†ïÎ≥¥ Ï†ÄÏû•"""
    if not supabase_client or not album_data or not artist_browse_id:
        return False
    try:
        browse_id = album_data.get("browseId")
        if not browse_id:
            return False

        data = {
            "browse_id": browse_id,
            "artist_browse_id": artist_browse_id,
            "title": album_data.get("title") or "",
            "type": album_data.get("type") or "Album",
            "year": album_data.get("year") or "",
            "thumbnail_url": get_best_thumbnail(album_data.get("thumbnails", [])),
            "track_count": len(album_data.get("tracks", []))
        }

        supabase_client.table("music_albums").upsert(
            data, on_conflict="browse_id"
        ).execute()
        return True
    except Exception as e:
        logger.warning(f"DB save album error: {e}")
        return False

def db_save_track(track_data: dict, album_browse_id: str, artist_browse_id: str, track_number: int = 0) -> bool:
    """Ìä∏Îûô Ï†ïÎ≥¥ Ï†ÄÏû•"""
    if not supabase_client or not track_data or not artist_browse_id:
        return False
    try:
        video_id = track_data.get("videoId")
        if not video_id:
            return False

        # durationÏùÑ Ï¥à Îã®ÏúÑÎ°ú Î≥ÄÌôò
        duration = track_data.get("duration") or ""
        duration_seconds = 0
        if duration:
            parts = duration.split(":")
            if len(parts) == 2:
                duration_seconds = int(parts[0]) * 60 + int(parts[1])
            elif len(parts) == 3:
                duration_seconds = int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])

        data = {
            "video_id": video_id,
            "album_browse_id": album_browse_id,
            "artist_browse_id": artist_browse_id,
            "title": track_data.get("title") or "",
            "duration": duration,
            "duration_seconds": duration_seconds,
            "track_number": track_number,
            "thumbnail_url": get_best_thumbnail(track_data.get("thumbnails", [])),
            "is_explicit": track_data.get("isExplicit", False)
        }

        supabase_client.table("music_tracks").upsert(
            data, on_conflict="video_id"
        ).execute()
        return True
    except Exception as e:
        logger.warning(f"DB save track error: {e}")
        return False

def db_save_search_keyword(keyword: str, country: str, artist_browse_id: str):
    """Í≤ÄÏÉâÏñ¥-ÏïÑÌã∞Ïä§Ìä∏ Îß§Ìïë Ï†ÄÏû•"""
    if not supabase_client or not keyword or not artist_browse_id:
        return
    try:
        data = {
            "keyword": keyword,
            "keyword_normalized": keyword.lower(),
            "country": country,
            "artist_browse_id": artist_browse_id,
            "search_count": 1,
            "last_searched_at": datetime.now(timezone.utc).isoformat()
        }

        supabase_client.table("search_keywords").upsert(
            data, on_conflict="keyword_normalized,country,artist_browse_id"
        ).execute()
    except Exception as e:
        logger.warning(f"DB save search keyword error: {e}")

def db_increment_search_count(keyword: str, country: str):
    """Í≤ÄÏÉâ ÌöüÏàò Ï¶ùÍ∞Ä"""
    if not supabase_client:
        return
    try:
        keyword_normalized = keyword.lower()
        supabase_client.rpc("increment_search_count", {
            "p_keyword": keyword_normalized,
            "p_country": country
        }).execute()
    except Exception:
        pass  # Ïã§Ìå®Ìï¥ÎèÑ Î¨¥Ïãú

def db_get_artists_by_keyword(keyword: str, country: str) -> list:
    """Í≤ÄÏÉâÏñ¥Î°ú Îß§ÌïëÎêú ÏïÑÌã∞Ïä§Ìä∏ Î™©Î°ù Ï°∞Ìöå"""
    if not supabase_client:
        return []
    try:
        keyword_normalized = keyword.lower()

        # search_keywordsÏóêÏÑú artist_browse_id Î™©Î°ù Ï°∞Ìöå
        result = supabase_client.table("search_keywords").select(
            "artist_browse_id"
        ).eq("keyword_normalized", keyword_normalized).eq("country", country).execute()

        if not result.data:
            return []

        artist_browse_ids = [r["artist_browse_id"] for r in result.data]
        return artist_browse_ids
    except Exception as e:
        logger.warning(f"DB get artists by keyword error: {e}")
        return []

def db_get_full_artist_data(browse_id: str) -> dict | None:
    """ÏïÑÌã∞Ïä§Ìä∏Ïùò Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå (Ïï®Î≤î, Ìä∏Îûô Ìè¨Ìï®)"""
    if not supabase_client or not browse_id:
        return None
    try:
        # ÏïÑÌã∞Ïä§Ìä∏ Í∏∞Î≥∏ Ï†ïÎ≥¥
        artist_result = supabase_client.table("music_artists").select("*").eq(
            "browse_id", browse_id
        ).single().execute()

        if not artist_result.data:
            return None

        artist = artist_result.data

        # Ïï®Î≤î Î™©Î°ù
        albums_result = supabase_client.table("music_albums").select("*").eq(
            "artist_browse_id", browse_id
        ).order("year", desc=True).execute()

        albums = albums_result.data or []

        # Ï†ÑÏ≤¥ Ìä∏Îûô Î™©Î°ù
        tracks_result = supabase_client.table("music_tracks").select("*").eq(
            "artist_browse_id", browse_id
        ).order("created_at", desc=True).execute()

        all_tracks = tracks_result.data or []

        # Ïï®Î≤îÎ≥ÑÎ°ú Ìä∏Îûô Í∑∏Î£πÌôî
        album_tracks_map = {}
        for track in all_tracks:
            album_id = track.get("album_browse_id")
            if album_id:
                if album_id not in album_tracks_map:
                    album_tracks_map[album_id] = []
                album_tracks_map[album_id].append({
                    "videoId": track.get("video_id"),
                    "title": track.get("title"),
                    "duration": track.get("duration"),
                    "trackNumber": track.get("track_number"),
                    "thumbnails": [{"url": track.get("thumbnail_url")}] if track.get("thumbnail_url") else []
                })

        # Ïï®Î≤îÏóê Ìä∏Îûô Ï∂îÍ∞Ä
        albums_with_tracks = []
        for album in albums:
            album_entry = {
                "browseId": album.get("browse_id"),
                "title": album.get("title"),
                "type": album.get("type"),
                "year": album.get("year"),
                "thumbnails": [{"url": album.get("thumbnail_url")}] if album.get("thumbnail_url") else [],
                "tracks": album_tracks_map.get(album.get("browse_id"), [])
            }
            albums_with_tracks.append(album_entry)

        return {
            "browseId": artist.get("browse_id"),
            "artist": artist.get("name"),
            "name": artist.get("name"),
            "thumbnails": [{"url": artist.get("thumbnail_url")}] if artist.get("thumbnail_url") else [],
            "subscribers": artist.get("subscribers"),
            "description": artist.get("description"),
            "topSongs": artist.get("top_songs_json") or [],
            "related": artist.get("related_artists_json") or [],
            "albums": albums_with_tracks,
            "allTracks": [{
                "videoId": t.get("video_id"),
                "title": t.get("title"),
                "duration": t.get("duration"),
                "albumTitle": next((a.get("title") for a in albums if a.get("browse_id") == t.get("album_browse_id")), ""),
                "thumbnails": [{"url": t.get("thumbnail_url")}] if t.get("thumbnail_url") else []
            } for t in all_tracks],
            "last_synced_at": artist.get("last_synced_at"),
            # Ïù∏Í∏∞Í≥° ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ IDÎßå Î∞òÌôò (YouTube IFrame APIÏö©)
            "songsPlaylistId": artist.get("songs_playlist_id")
        }
    except Exception as e:
        logger.warning(f"DB get full artist data error: {e}")
        return None

def upscale_thumbnail_url(url: str, size: int = 544) -> str:
    """
    Í∏∞Ï°¥ Ïç∏ÎÑ§Ïùº URLÏùÑ Í≥†Ìï¥ÏÉÅÎèÑÎ°ú Î≥ÄÌôò

    YouTube Music Ïç∏ÎÑ§Ïùº URL ÌòïÏãù:
    - lh3.googleusercontent.com/...=w60-h60-l90-rj -> w{size}-h{size}Î°ú Î≥ÄÌôò
    - i.ytimg.com/vi/ID/sddefault.jpg -> maxresdefault.jpgÎ°ú Î≥ÄÌôò

    Args:
        url: Ïç∏ÎÑ§Ïùº URL
        size: ÏõêÌïòÎäî Ìï¥ÏÉÅÎèÑ (Í∏∞Î≥∏ 544px)
    """
    import re

    if not url:
        return ""

    # 1. Google CDN (lh3.googleusercontent.com, yt3.googleusercontent.com)
    if "googleusercontent.com" in url:
        # w60-h60 ÎòêÎäî w120-h120 Îì±ÏùÑ w{size}-h{size}Î°ú Î≥ÄÌôò
        url = re.sub(r'=w\d+-h\d+', f'=w{size}-h{size}', url)
        # s60, s120 Îì± Îã®Ïùº ÌÅ¨Í∏∞ ÌååÎùºÎØ∏ÌÑ∞ÎèÑ Î≥ÄÌôò
        url = re.sub(r'=s\d+', f'=s{size}', url)
        # ÌÅ¨Í∏∞ ÌååÎùºÎØ∏ÌÑ∞Í∞Ä ÏóÜÏúºÎ©¥ Ï∂îÍ∞Ä
        if f'=w{size}' not in url and f'=s{size}' not in url:
            if '=' in url:
                url = re.sub(r'=([^&]+)$', f'=w{size}-h{size}-\\1', url)
            else:
                url = f"{url}=w{size}-h{size}"

    # 2. YouTube Ïù¥ÎØ∏ÏßÄ (i.ytimg.com)
    elif "i.ytimg.com" in url:
        # sddefault, hqdefault -> maxresdefault
        url = url.replace("sddefault.jpg", THUMBNAIL_MAXRES)
        url = url.replace("hqdefault.jpg", THUMBNAIL_MAXRES)
        url = url.replace("mqdefault.jpg", THUMBNAIL_MAXRES)
        url = url.replace("/default.jpg", f"/{THUMBNAIL_MAXRES}")

    return url


def get_best_thumbnail(thumbnails: list, size: int = 544) -> str:
    """Ïç∏ÎÑ§Ïùº Î™©Î°ùÏóêÏÑú Í∞ÄÏû• Ï¢ãÏùÄ URL ÏÑ†ÌÉù + Í≥†Ìï¥ÏÉÅÎèÑÎ°ú Î≥ÄÌôò"""
    if not thumbnails:
        return ""

    # Í∞ÄÏû• ÌÅ∞ Ïù¥ÎØ∏ÏßÄ ÏÑ†ÌÉù
    best = thumbnails[-1]
    url = best.get("url", "") if isinstance(best, dict) else ""

    return upscale_thumbnail_url(url, size)


def _process_album_list(ytmusic, album_list: list, existing_album_ids: set,
                        existing_video_ids: set, artist_browse_id: str) -> tuple[int, int]:
    """Process album list and save new albums/tracks. Returns (new_albums, new_tracks)."""
    new_albums = 0
    new_tracks = 0

    for album in album_list:
        album_browse_id = album.get("browseId")
        if not album_browse_id or album_browse_id in existing_album_ids:
            continue

        try:
            album_detail = ytmusic.get_album(album_browse_id)
            if not album_detail:
                continue

            album_data = {
                "browseId": album_browse_id,
                "title": album_detail.get("title") or "",
                "type": album_detail.get("type") or "Album",
                "year": album_detail.get("year") or "",
                "thumbnails": album_detail.get("thumbnails") or []
            }

            db_save_album(album_data, artist_browse_id)
            new_albums += 1

            for idx, track in enumerate(album_detail.get("tracks") or []):
                video_id = track.get("videoId")
                if not video_id or video_id in existing_video_ids:
                    continue

                track_data = {
                    "videoId": video_id,
                    "title": track.get("title") or "",
                    "duration": track.get("duration") or "",
                    "thumbnails": track.get("thumbnails") or [],
                    "isExplicit": track.get("isExplicit", False)
                }
                db_save_track(track_data, album_browse_id, artist_browse_id, idx + 1)
                new_tracks += 1

        except Exception as e:
            logger.warning(f"Background album fetch error: {e}")

    return new_albums, new_tracks


def _get_section_list(ytmusic, section: dict) -> list:
    """Get album/single list from section with proper API call."""
    if not section or not isinstance(section, dict):
        return []

    params = section.get("params")
    browse_id = section.get("browseId")

    if params and browse_id:
        try:
            return ytmusic.get_artist_albums(browse_id, params) or []
        except Exception:
            return section.get("results") or []
    return section.get("results") or []


def _extract_top_songs(songs_section: dict) -> list:
    """Extract top songs from artist info."""
    if not songs_section or not isinstance(songs_section, dict):
        return []

    top_songs = []
    for song in songs_section.get("results", []):
        if isinstance(song, dict) and song.get("videoId"):
            top_songs.append({
                "videoId": song.get("videoId"),
                "title": song.get("title") or "",
                "duration": song.get("duration") or "",
                "thumbnails": song.get("thumbnails") or []
            })
    return top_songs


def _extract_related_artists(related_section: dict) -> list:
    """Extract related artists from artist info."""
    if not related_section or not isinstance(related_section, dict):
        return []

    related_artists = []
    for rel in related_section.get("results", [])[:15]:
        if isinstance(rel, dict):
            related_artists.append({
                "browseId": rel.get("browseId") or "",
                "name": rel.get("title") or rel.get("name") or "",
                "subscribers": rel.get("subscribers") or "",
                "thumbnails": rel.get("thumbnails") or []
            })
    return related_artists


def background_update_artist(artist_browse_id: str, country: str):
    """
    Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú ÏïÑÌã∞Ïä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ (7Ïùº Í≤ΩÍ≥º Ïãú Ìò∏Ï∂ú)
    - ÏÉà Ïï®Î≤î/Ïã±Í∏Ä ÌôïÏù∏
    - ÏÉà Ìä∏ÎûôÎßå DBÏóê Ï∂îÍ∞Ä (Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ Ïú†ÏßÄ)
    """
    try:
        from ytmusicapi import YTMusic

        logger.info(f"Background update started: {artist_browse_id}")

        existing_album_ids = db_get_existing_album_ids(artist_browse_id)
        existing_video_ids = db_get_existing_video_ids(artist_browse_id)

        lang = COUNTRY_LANGUAGE_MAP.get(country.upper(), 'en')
        ytmusic = YTMusic(language=lang, location=country.upper())

        artist_info = ytmusic.get_artist(artist_browse_id)
        if not artist_info:
            logger.warning(f"Background update: Artist not found {artist_browse_id}")
            return

        # Process albums
        album_list = _get_section_list(ytmusic, artist_info.get("albums"))
        albums_count, tracks_from_albums = _process_album_list(
            ytmusic, album_list, existing_album_ids, existing_video_ids, artist_browse_id
        )

        # Process singles
        singles_list = _get_section_list(ytmusic, artist_info.get("singles"))
        singles_count, tracks_from_singles = _process_album_list(
            ytmusic, singles_list, existing_album_ids, existing_video_ids, artist_browse_id
        )

        new_albums_count = albums_count + singles_count
        new_tracks_count = tracks_from_albums + tracks_from_singles

        # Extract metadata
        top_songs = _extract_top_songs(artist_info.get("songs"))
        related_artists = _extract_related_artists(artist_info.get("related"))

        # Update last_synced_at
        if supabase_client:
            try:
                supabase_client.table("music_artists").update({
                    "top_songs_json": top_songs,
                    "related_artists_json": related_artists,
                    "last_synced_at": datetime.now(timezone.utc).isoformat()
                }).eq("browse_id", artist_browse_id).execute()
            except Exception as e:
                logger.warning(f"Background update artist error: {e}")

        logger.info(f"Background update completed: {artist_browse_id} - {new_albums_count} new albums, {new_tracks_count} new tracks")

    except Exception as e:
        logger.error(f"Background update error: {e}")


def db_get_existing_video_ids(artist_browse_id: str) -> set:
    """ÏïÑÌã∞Ïä§Ìä∏Ïùò Í∏∞Ï°¥ video_id Î™©Î°ù Ï°∞Ìöå (Ï§ëÎ≥µ Î∞©ÏßÄÏö©)"""
    if not supabase_client or not artist_browse_id:
        return set()

    try:
        result = supabase_client.table("music_tracks").select("video_id").eq(
            "artist_browse_id", artist_browse_id
        ).execute()

        return {r.get("video_id") for r in (result.data or []) if r.get("video_id")}
    except Exception as e:
        logger.warning(f"DB get existing video_ids error: {e}")
        return set()

def db_get_existing_album_ids(artist_browse_id: str) -> set:
    """ÏïÑÌã∞Ïä§Ìä∏Ïùò Í∏∞Ï°¥ album browse_id Î™©Î°ù Ï°∞Ìöå (Ï§ëÎ≥µ Î∞©ÏßÄÏö©)"""
    if not supabase_client or not artist_browse_id:
        return set()

    try:
        result = supabase_client.table("music_albums").select("browse_id").eq(
            "artist_browse_id", artist_browse_id
        ).execute()

        return {r.get("browse_id") for r in (result.data or []) if r.get("browse_id")}
    except Exception as e:
        logger.warning(f"DB get existing album_ids error: {e}")
        return set()

def db_artist_needs_update(browse_id: str, hours: int = 6) -> bool:
    """ÏïÑÌã∞Ïä§Ìä∏Í∞Ä ÏóÖÎç∞Ïù¥Ìä∏ ÌïÑÏöîÌïúÏßÄ ÌôïÏù∏ (ÎßàÏßÄÎßâ ÏóÖÎç∞Ïù¥Ìä∏ ÌõÑ NÏãúÍ∞Ñ Í≤ΩÍ≥º)"""
    if not supabase_client or not browse_id:
        return True

    try:
        result = supabase_client.table("music_artists").select("last_updated").eq(
            "browse_id", browse_id
        ).single().execute()

        if not result.data:
            return True  # ÏóÜÏúºÎ©¥ ÏóÖÎç∞Ïù¥Ìä∏ ÌïÑÏöî

        last_updated = result.data.get("last_updated")
        if not last_updated:
            return True

        last_time = datetime.fromisoformat(last_updated.replace("Z", TIMEZONE_SUFFIX))
        return datetime.now(timezone.utc) - last_time > timedelta(hours=hours)
    except Exception as e:
        logger.warning(f"DB artist needs update check error: {e}")
        return True

def db_save_related_artists(main_artist_browse_id: str, related_artists: list):
    """Ïú†ÏÇ¨ ÏïÑÌã∞Ïä§Ìä∏ Í¥ÄÍ≥Ñ Ï†ÄÏû•"""
    if not supabase_client or not related_artists:
        return

    try:
        for related in related_artists:
            related_browse_id = related.get("browseId")
            if not related_browse_id:
                continue

            # Î®ºÏ†Ä Í¥ÄÎ†® ÏïÑÌã∞Ïä§Ìä∏ÎèÑ music_artists ÌÖåÏù¥Î∏îÏóê Ï†ÄÏû•
            db_save_artist(related)

            # Í¥ÄÍ≥Ñ Ï†ÄÏû•
            data = {
                "main_artist_browse_id": main_artist_browse_id,
                "related_artist_browse_id": related_browse_id,
                "relation_type": "similar"
            }

            supabase_client.table("artist_relations").upsert(
                data, on_conflict="main_artist_browse_id,related_artist_browse_id"
            ).execute()

        logger.info(f"Saved {len(related_artists)} related artists for {main_artist_browse_id}")
    except Exception as e:
        logger.warning(f"DB save related artists error: {e}")

def db_save_search_cache(keyword: str, country: str, artist_ids: list):
    """Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º Ï∫êÏãúÏóê Ï†ÄÏû•"""
    if not supabase_client or not artist_ids:
        return

    try:
        data = {
            "keyword": keyword,
            "keyword_normalized": keyword.lower(),
            "country": country,
            "artist_ids": artist_ids,
            "result_count": len(artist_ids),
            "last_searched": datetime.now(timezone.utc).isoformat()
        }

        supabase_client.table("music_search_cache").upsert(
            data, on_conflict="keyword_normalized,country"
        ).execute()

    except Exception as e:
        logger.warning(f"DB save search cache error: {e}")

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ÏãúÏûë Ïãú
    logger.info("üöÄ MusicGram API starting...")
    yield
    # Ï¢ÖÎ£å Ïãú
    logger.info("üëã MusicGram API shutting down...")

# FastAPI Ïï±
app = FastAPI(
    title="MusicGram API",
    description="YouTube Music Í∏∞Î∞ò ÏùåÏïÖ ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ÏÜåÏÖú ÌîåÎû´Ìèº API",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ÏÑ§Ï†ï (ÌîÑÎ°†Ìä∏ÏóîÎìú ÌóàÏö©)
allowed_origins = os.getenv("ALLOWED_ORIGINS", "*").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =============================================================================
# Ìó¨Ïä§ Ï≤¥ÌÅ¨
# =============================================================================

@app.get("/")
async def root():
    return {
        "service": "MusicGram API",
        "status": "running",
        "version": "1.0.0"
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "database": "connected" if supabase_client else "not configured"
    }

# =============================================================================
# ÏùåÏïÖ Í≤ÄÏÉâ API
# =============================================================================

@app.get("/api/search")
async def search_music(
    request: Request,
    q: str,
    filter: str = "songs",
    limit: int = 20,
    country: str = None
):
    """ÏùåÏïÖ Í≤ÄÏÉâ"""
    # Íµ≠Í∞Ä Í∞êÏßÄ (Ìó§Îçî ÎòêÎäî ÌååÎùºÎØ∏ÌÑ∞)
    if not country:
        country = request.headers.get("CF-IPCountry", "US")
    
    # Ï∫êÏãú ÌÇ§
    cache_key = f"search:{country}:{filter}:{q}"
    
    # Ï∫êÏãú ÌôïÏù∏
    cached = cache_get(cache_key)
    if cached:
        logger.info(f"Cache hit: {cache_key}")
        return {"source": "cache", "results": cached}
    
    # API Ìò∏Ï∂ú
    try:
        ytmusic = get_ytmusic(country)
        results = ytmusic.search(q, filter=filter, limit=limit)
        
        # Ï∫êÏãú Ï†ÄÏû• (30Î∂Ñ)
        cache_set(cache_key, results, ttl=1800)
        
        return {"source": "api", "results": results}
    except Exception as e:
        logger.error(f"Search error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# Ï∞®Ìä∏ API
# =============================================================================

@app.get("/api/charts")
async def get_charts(request: Request, country: str = None):
    """Íµ≠Í∞ÄÎ≥Ñ Ïù∏Í∏∞ Ï∞®Ìä∏"""
    if not country:
        country = request.headers.get("CF-IPCountry", "US")
    
    cache_key = f"charts:{country}"
    
    cached = cache_get(cache_key)
    if cached:
        return {"country": country, "source": "cache", "charts": cached}
    
    try:
        ytmusic = get_ytmusic(country)
        charts = ytmusic.get_charts(country=country)
        
        # 1ÏãúÍ∞Ñ Ï∫êÏãú
        cache_set(cache_key, charts, ttl=3600)
        
        return {"country": country, "source": "api", "charts": charts}
    except Exception as e:
        logger.error(f"Charts error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# Ïã†Í∑ú Ïï®Î≤î API
# =============================================================================

@app.get("/api/new-albums")
async def get_new_albums(request: Request, country: str = None):
    """Íµ≠Í∞ÄÎ≥Ñ Ïã†Í∑ú Ïï®Î≤î"""
    if not country:
        country = request.headers.get("CF-IPCountry", "US")
    
    cache_key = f"new_albums:{country}"
    
    cached = cache_get(cache_key)
    if cached:
        return {"country": country, "source": "cache", "albums": cached}
    
    try:
        ytmusic = get_ytmusic(country)
        albums = ytmusic.get_new_albums()
        
        # 1ÏãúÍ∞Ñ Ï∫êÏãú
        cache_set(cache_key, albums, ttl=3600)
        
        return {"country": country, "source": "api", "albums": albums}
    except Exception as e:
        logger.error(f"New albums error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# Mood & Genre API (Ï∂îÏ≤ú ÏùåÏïÖ)
# =============================================================================

@app.get("/api/moods")
async def get_moods(request: Request, country: str = None):
    """Î¨¥Îìú & Ïû•Î•¥ Ïπ¥ÌÖåÍ≥†Î¶¨ Î™©Î°ù"""
    if not country:
        country = request.headers.get("CF-IPCountry", "US")

    cache_key = f"moods:{country}"

    cached = cache_get(cache_key)
    if cached:
        return {"country": country, "source": "cache", "moods": cached}

    try:
        ytmusic = get_ytmusic(country)
        moods = ytmusic.get_mood_categories()

        # 6ÏãúÍ∞Ñ Ï∫êÏãú
        cache_set(cache_key, moods, ttl=21600)

        return {"country": country, "source": "api", "moods": moods}
    except Exception as e:
        logger.error(f"Moods error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/mood-playlists")
async def get_mood_playlists(params: str, country: str = None, request: Request = None):
    """ÌäπÏ†ï Î¨¥Îìú/Ïû•Î•¥Ïùò ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ Î™©Î°ù"""
    if not country:
        country = request.headers.get("CF-IPCountry", "US") if request else "US"

    cache_key = f"mood_playlists:{params}:{country}"

    cached = cache_get(cache_key)
    if cached:
        return {"country": country, "source": "cache", "playlists": cached}

    try:
        ytmusic = get_ytmusic(country)
        playlists = ytmusic.get_mood_playlists(params)

        # 1ÏãúÍ∞Ñ Ï∫êÏãú
        cache_set(cache_key, playlists, ttl=3600)

        return {"country": country, "source": "api", "playlists": playlists}
    except Exception as e:
        logger.error(f"Mood playlists error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/home")
async def get_home_feed(request: Request, country: str = None, limit: int = 6):
    """
    Ìôà ÌôîÎ©¥ Ï∂îÏ≤ú ÏΩòÌÖêÏ∏† - ytmusic.get_home() ÏÇ¨Ïö©
    YouTube Music Ìôà ÌôîÎ©¥Í≥º ÎèôÏùºÌïú Ï∂îÏ≤ú ÏÑπÏÖò Î∞òÌôò
    (Quick picks, ÎØπÏä§, Ï∂îÏ≤ú ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏, Ïã†Í∑ú Ïï®Î≤î Îì±)
    """
    if not country:
        country = request.headers.get("CF-IPCountry", "US")

    cache_key = f"home_feed:{country}:{limit}"

    cached = cache_get(cache_key)
    if cached:
        return {"country": country, "source": "cache", "sections": cached}

    # Ïû¨ÏãúÎèÑ Î°úÏßÅ (YTMusic API Í∞ÑÌóêÏ†Å Îπà ÏùëÎãµ ÎåÄÏùë)
    max_retries = 3
    last_error = None

    for attempt in range(max_retries):
        try:
            ytmusic = get_ytmusic(country)

            # get_home()ÏùÄ Ìôà ÌôîÎ©¥Ïùò Î™®Îì† ÏÑπÏÖòÏùÑ Î∞òÌôò
            home_sections = ytmusic.get_home(limit=limit)

            # Ïú†Ìö®Ìïú ÏùëÎãµÏù∏ÏßÄ ÌôïÏù∏
            if home_sections and isinstance(home_sections, list) and len(home_sections) > 0:
                # 30Î∂Ñ Ï∫êÏãú
                cache_set(cache_key, home_sections, ttl=1800)
                return {"country": country, "source": "api", "sections": home_sections}
            else:
                logger.warning(f"Home feed empty response (attempt {attempt + 1}/{max_retries})")
                last_error = "Empty response from YTMusic API"

        except Exception as e:
            logger.warning(f"Home feed attempt {attempt + 1}/{max_retries} failed: {e}")
            last_error = str(e)

        # Ïû¨ÏãúÎèÑ Ï†Ñ ÎåÄÍ∏∞ (exponential backoff)
        if attempt < max_retries - 1:
            await asyncio.sleep(0.5 * (attempt + 1))

    # Î™®Îì† Ïû¨ÏãúÎèÑ Ïã§Ìå® Ïãú Îπà ÏùëÎãµ Î∞òÌôò (500 ÏóêÎü¨ ÎåÄÏã†)
    logger.error(f"Home feed failed after {max_retries} attempts: {last_error}")
    return {"country": country, "source": "fallback", "sections": [], "error": "Temporary service unavailable"}


# =============================================================================
# Summary Search API - Ï†ïÍ∑úÌôî DB ÏÇ¨Ïö© (Ï†ÑÏ≤¥ ÎîîÏä§ÏΩîÍ∑∏ÎûòÌîº)
# =============================================================================

# =============================================================================
# Summary Search API - Ï†ïÍ∑úÌôî DB ÏÇ¨Ïö© (Ï†ÑÏ≤¥ ÎîîÏä§ÏΩîÍ∑∏ÎûòÌîº)
# =============================================================================

def save_full_artist_data_background(artist_id: str, artist_info: dict, country: str):
    """Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú ÏïÑÌã∞Ïä§Ìä∏Ïùò Ï†ÑÏ≤¥ Ïï®Î≤î/Ïã±Í∏Ä/Ìä∏Îûô Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏ÏôÄ DBÏóê Ï†ÄÏû•"""
    try:
        ytmusic = get_ytmusic(country)

        # 1. ÏïÑÌã∞Ïä§Ìä∏ Í∏∞Î≥∏ Ï†ïÎ≥¥ Ï†ÄÏû•
        artist_name = artist_info.get("name") or ""
        search_thumbnails = artist_info.get("thumbnails") or []

        # Ïù∏Í∏∞Í≥° ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ID Ï∂îÏ∂ú (ÌïµÏã¨!)
        songs_playlist_id = None
        songs_browse_id = None
        songs_section = artist_info.get("songs")
        if songs_section and isinstance(songs_section, dict):
            songs_browse_id = songs_section.get("browseId")
            if songs_browse_id and songs_browse_id.startswith("VL"):
                songs_playlist_id = songs_browse_id[2:]
            elif songs_browse_id:
                songs_playlist_id = songs_browse_id

        # Ïù∏Í∏∞Í≥° Ï∂îÏ∂ú & Ï†ÄÏû• (Í≤ÄÏÉâÏö© Î©îÌÉÄÎç∞Ïù¥ÌÑ∞)
        top_songs = []
        if songs_section and isinstance(songs_section, dict):
            for song in songs_section.get("results", []):
                if isinstance(song, dict) and song.get("videoId"):
                    top_songs.append({
                        "videoId": song.get("videoId"),
                        "title": song.get("title") or "",
                        "duration": song.get("duration") or "",
                        "thumbnails": song.get("thumbnails") or []
                    })
        
        # Ïú†ÏÇ¨ ÏïÑÌã∞Ïä§Ìä∏ Ï∂îÏ∂ú & Ï†ÄÏû•
        related_artists = []
        related_section = artist_info.get("related")
        if related_section and isinstance(related_section, dict):
            for rel in related_section.get("results", [])[:15]:
                if isinstance(rel, dict):
                    related_artists.append({
                        "browseId": rel.get("browseId") or "",
                        "name": rel.get("title") or rel.get("name") or "",
                        "subscribers": rel.get("subscribers") or "",
                        "thumbnails": rel.get("thumbnails") or []
                    })
        
        artist_data = {
            "browseId": artist_id,
            "artist": artist_name,
            "name": artist_name,
            "thumbnails": search_thumbnails,
            "subscribers": artist_info.get("subscribers") or "",
            "description": artist_info.get("description") or "",
            "topSongs": top_songs,
            "related": related_artists,
            "songsPlaylistId": songs_playlist_id  # YouTube IFrame APIÏö© ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ IDÎßå
        }

        db_save_artist_full(artist_data)
        
        # 2. Ïï®Î≤î/Ïã±Í∏Ä Ï†ÑÏ≤¥ Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞ & Ï†ÄÏû•
        try:
            # Ïï®Î≤î
            albums_section = artist_info.get("albums")
            if albums_section and isinstance(albums_section, dict):
                params = albums_section.get("params")
                browse_id = albums_section.get("browseId")
                
                album_list = []
                if params and browse_id:
                    album_list = ytmusic.get_artist_albums(browse_id, params) or []
                else:
                    album_list = albums_section.get("results") or []
                    
                for album in album_list:
                     if isinstance(album, dict) and album.get("browseId"):
                        album_data = {
                            "browseId": album.get("browseId"),
                            "title": album.get("title") or "",
                            "type": album.get("type") or "Album",
                            "year": album.get("year") or "",
                            "thumbnails": album.get("thumbnails") or [],
                            "tracks": []
                        }
                        db_save_album(album_data, artist_id)
            
            # Ïã±Í∏Ä
            singles_section = artist_info.get("singles")
            if singles_section and isinstance(singles_section, dict):
                params = singles_section.get("params")
                browse_id = singles_section.get("browseId")
                
                singles_list = []
                if params and browse_id:
                    singles_list = ytmusic.get_artist_albums(browse_id, params) or []
                else:
                    singles_list = singles_section.get("results") or []
                    
                for single in singles_list:
                     if isinstance(single, dict) and single.get("browseId"):
                        single_data = {
                            "browseId": single.get("browseId"),
                            "title": single.get("title") or "",
                            "type": "Single",
                            "year": single.get("year") or "",
                            "thumbnails": single.get("thumbnails") or [],
                            "tracks": []
                        }
                        db_save_album(single_data, artist_id)
                        
        except Exception as e:
            logger.warning(f"Background album save error: {e}")
            
        logger.info(f"Background save completed for artist: {artist_name}")

    except Exception as e:
        logger.error(f"Background save error: {e}")

def parse_artist_data_lightweight(artist_id: str, artist_info: dict) -> dict:
    """
    ÏïÑÌã∞Ïä§Ìä∏ Ï†ïÎ≥¥Î•º Îπ†Î•¥Í≤å ÌååÏã±ÌïòÏó¨ ÏùëÎãµÏö©ÏúºÎ°ú Î∞òÌôò (DB Ï†ÄÏû• X, Ï∂îÍ∞Ä Fetch X)
    """
    artist_name = artist_info.get("name") or ""
    search_thumbnails = artist_info.get("thumbnails") or []

    # Ïù∏Í∏∞Í≥°
    top_songs = []
    songs_section = artist_info.get("songs")
    if songs_section and isinstance(songs_section, dict):
        for song in songs_section.get("results", []):
            if isinstance(song, dict) and song.get("videoId"):
                top_songs.append({
                    "videoId": song.get("videoId"),
                    "title": song.get("title") or "",
                    "duration": song.get("duration") or "",
                    "thumbnails": song.get("thumbnails") or []
                })

    # Ïú†ÏÇ¨ ÏïÑÌã∞Ïä§Ìä∏
    related_artists = []
    related_section = artist_info.get("related")
    if related_section and isinstance(related_section, dict):
        for rel in related_section.get("results", [])[:15]:
            if isinstance(rel, dict):
                related_artists.append({
                    "browseId": rel.get("browseId") or "",
                    "name": rel.get("title") or rel.get("name") or "",
                    "subscribers": rel.get("subscribers") or "",
                    "thumbnails": rel.get("thumbnails") or []
                })

    # Ïï®Î≤î Î™©Î°ù (Î©îÏù∏ ÌéòÏù¥ÏßÄÏóê ÏûàÎäî Í≤ÉÎßå Ïö∞ÏÑ† Î∞òÌôò)
    all_albums = []
    
    albums_section = artist_info.get("albums")
    if albums_section and isinstance(albums_section, dict):
        for album in albums_section.get("results") or []:
            if isinstance(album, dict) and album.get("browseId"):
                all_albums.append({
                    "browseId": album.get("browseId"),
                    "title": album.get("title") or "",
                    "type": album.get("type") or "Album",
                    "year": album.get("year") or "",
                    "thumbnails": album.get("thumbnails") or [],
                    "tracks": []
                })

    singles_section = artist_info.get("singles")
    if singles_section and isinstance(singles_section, dict):
        for single in singles_section.get("results") or []:
             if isinstance(single, dict) and single.get("browseId"):
                all_albums.append({
                    "browseId": single.get("browseId"),
                    "title": single.get("title") or "",
                    "type": "Single",
                    "year": single.get("year") or "",
                    "thumbnails": single.get("thumbnails") or [],
                    "tracks": []
                })

    return {
        "browseId": artist_id,
        "artist": artist_name,
        "name": artist_name,
        "thumbnails": search_thumbnails,
        "subscribers": artist_info.get("subscribers") or "",
        "description": artist_info.get("description") or "",
        "topSongs": top_songs,
        "related": related_artists,
        "albums": all_albums,
        "allTracks": top_songs # Ï¥àÍ∏∞ ÏùëÎãµÏóêÎäî top songsÎßå Ìè¨Ìï®
    }


# =============================================================================
# Helper functions for search_summary
# =============================================================================

def _process_db_artists(artist_browse_ids: list, background_tasks, country: str) -> dict | None:
    """Process artists from database and prepare response data."""
    artists_data = []
    all_songs = []
    all_albums = []
    needs_background_update = False

    for browse_id in artist_browse_ids:
        artist_full = db_get_full_artist_data(browse_id)
        if not artist_full:
            continue
        
        artists_data.append(artist_full)

        for song in artist_full.get("topSongs", []):
            song["artist_bid"] = browse_id
            song["resultType"] = "song"
            all_songs.append(song)

        for album in artist_full.get("albums", []):
            album["artist_bid"] = browse_id
            all_albums.append(album)

        if db_check_artist_needs_sync(browse_id, days=7):
            needs_background_update = True
            background_tasks.add_task(background_update_artist, browse_id, country)

    if not artists_data:
        return None

    return {
        "artists": artists_data,
        "songs": all_songs,
        "albums": all_albums,
        "allTracks": [t for a in artists_data for t in a.get("allTracks", [])],
        "updating": needs_background_update
    }


def _find_best_artist(artists_search: list, search_query: str) -> dict | None:
    """Find the best matching artist from search results."""
    search_lower = search_query.lower().strip()
    
    for artist in artists_search[:5]:
        if not isinstance(artist, dict):
            continue
        artist_name = (artist.get("artist") or artist.get("name") or "").lower()
        if search_lower in artist_name or artist_name in search_lower:
            return artist
    
    return artists_search[0] if artists_search else None


def _filter_songs_by_artist(songs_search: list, top_songs: list, artist_name: str) -> list:
    """Filter and prioritize songs by matching artist."""
    target_name = artist_name.lower()
    filtered_search = []
    other_songs = []

    for song in songs_search:
        song_artists = song.get("artists") or []
        is_match = any(a.get("name", "").lower() == target_name for a in song_artists)
        if is_match:
            filtered_search.append(song)
        else:
            other_songs.append(song)

    result = top_songs + filtered_search
    if len(result) < 5:
        result += other_songs[:5]
    
    return result


@app.get("/api/search/summary/deprecated")
async def search_summary_deprecated(
    request: Request,
    q: str,
    background_tasks: BackgroundTasks,
    country: str = None,
    force_refresh: bool = False
):
    """
    ÏïÑÌã∞Ïä§Ìä∏ Í≤ÄÏÉâ Î∞è Ï†ÑÏ≤¥ ÎîîÏä§ÏΩîÍ∑∏ÎûòÌîº Î∞òÌôò

    Data Flow:
    1. DBÏóêÏÑú Í≤ÄÏÉâÏñ¥ Îß§Ìïë ÌôïÏù∏ (search_keywords)
    2. Îß§Ìïë ÏûàÏùå ‚Üí DBÏóêÏÑú Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ Î∞òÌôò
       - 7Ïùº Í≤ΩÍ≥º Ïãú ‚Üí Î∞òÌôò ÌõÑ Î∞±Í∑∏ÎùºÏö¥Îìú ÏóÖÎç∞Ïù¥Ìä∏
    3. Îß§Ìïë ÏóÜÏùå ‚Üí ytmusicapi Ìò∏Ï∂ú ‚Üí Ï†ÑÏ≤¥ ÎîîÏä§ÏΩîÍ∑∏ÎûòÌîº ‚Üí DB Ï†ÄÏû•
    """
    if not country:
        country = request.headers.get("CF-IPCountry", "US")

    cache_key = f"summary:{country}:{q}"

    # ==========================================================================
    # 1Îã®Í≥Ñ: DBÏóêÏÑú Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏
    # ==========================================================================
    if not force_refresh:
        # Redis Ï∫êÏãú Î®ºÏ†Ä ÌôïÏù∏ (Îπ†Î•∏ ÏùëÎãµ)
        cached = cache_get(cache_key)
        if cached:
            logger.info(f"Redis cache hit: {cache_key}")
            cached["source"] = "redis"
            return cached

        # DBÏóêÏÑú Í≤ÄÏÉâÏñ¥ Îß§Ìïë ÌôïÏù∏
        artist_browse_ids = db_get_artists_by_keyword(q, country)
        if artist_browse_ids:
            logger.info(f"DB hit for keyword: {q} ({len(artist_browse_ids)} artists)")
            db_result = _process_db_artists(artist_browse_ids, background_tasks, country)
            
            if db_result:
                result = {
                    "keyword": q,
                    "country": country,
                    "artists": db_result["artists"],
                    "songs": db_result["songs"],
                    "albums": [],
                    "albums2": db_result["albums"],
                    "allTracks": db_result["allTracks"],
                    "source": "database",
                    "updating": db_result["updating"]
                }
                cache_set(cache_key, result, ttl=1800)
                if db_result["updating"]:
                    logger.info(f"Background update triggered for: {q}")
                return result

    # ==========================================================================
    # 2Îã®Í≥Ñ: ytmusicapiÏóêÏÑú ÏÉàÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞ (Î≥ëÎ†¨ Ï≤òÎ¶¨ + Î∞±Í∑∏ÎùºÏö¥Îìú Ï†ÄÏû•)
    # ==========================================================================
    logger.info(f"Fetching from ytmusicapi: {q}")

    try:
        ytmusic = get_ytmusic(country)

        # 1. ÏïÑÌã∞Ïä§Ìä∏ÏôÄ ÎÖ∏Îûò ÎèôÏãú Í≤ÄÏÉâ (Î≥ëÎ†¨ Ïã§Ìñâ)
        # run_in_threadÎ•º ÏÇ¨Ïö©ÌïòÏó¨ blocking I/OÎ•º Î≥ÑÎèÑ Ïä§Î†àÎìúÏóêÏÑú Ïã§Ìñâ
        future_artists = run_in_thread(ytmusic.search, q, filter="artists", limit=5)
        future_songs = run_in_thread(ytmusic.search, q, filter="songs", limit=20) # 30 -> 20 limit Ï∂ïÏÜå
        
        # Î≥ëÎ†¨ ÎåÄÍ∏∞
        artists_results, direct_song_results = await asyncio.gather(future_artists, future_songs)
        
        # Í≤∞Í≥º Ï≤òÎ¶¨
        artists_search = artists_results or []
        if not artists_search:
            # Fallback: ÏùºÎ∞ò Í≤ÄÏÉâÏóêÏÑú ÏïÑÌã∞Ïä§Ìä∏ ÌïÑÌÑ∞ÎßÅ
            general_results = await run_in_thread(ytmusic.search, q, limit=40)
            artists_search = [r for r in general_results if r.get("resultType") == "artist"][:5]

        # ÎÖ∏Îûò Í≤∞Í≥º Ï†ïÏ†ú
        songs_search = []
        direct_song_results = direct_song_results or []
        for song in direct_song_results:
            if isinstance(song, dict) and song.get("videoId"):
                song_copy = dict(song)
                song_copy["resultType"] = "song"
                song_copy["from_direct_search"] = True
                songs_search.append(song_copy)

        # Í∞ÄÏû• Í¥ÄÎ†®ÏÑ± ÎÜíÏùÄ ÏïÑÌã∞Ïä§Ìä∏ 1Î™ÖÎßå Ï≤òÎ¶¨
        artists_data = []
        albums_data = []
        all_tracks = []
        
        # ÏµúÏ†ÅÏùò ÏïÑÌã∞Ïä§Ìä∏ Ï∞æÍ∏∞ (Ìó¨Ìçº Ìï®Ïàò ÏÇ¨Ïö©)
        best_artist = _find_best_artist(artists_search, q)

        if best_artist and isinstance(best_artist, dict):
            artist_id = best_artist.get("browseId")
            if artist_id:
                try:
                    # ÏïÑÌã∞Ïä§Ìä∏ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞
                    artist_info = await run_in_thread(ytmusic.get_artist, artist_id)
                    
                    if artist_info and isinstance(artist_info, dict):
                        # 1. ÏùëÎãµÏö© Í∞ÄÎ≤ºÏö¥ Îç∞Ïù¥ÌÑ∞ ÌååÏã± (Blocking ÏóÜÏùå)
                        artist_full = parse_artist_data_lightweight(artist_id, artist_info)
                        
                        if artist_full:
                            artists_data.append(artist_full)

                            # Ïù∏Í∏∞Í≥° ÌÉúÍ∑∏ Ï∂îÍ∞Ä
                            top_songs = []
                            for song in artist_full.get("topSongs", []):
                                song["artist_bid"] = artist_id
                                song["resultType"] = "song"
                                top_songs.append(song)
                            
                            # Ìó¨Ìçº Ìï®ÏàòÎ°ú ÎÖ∏Îûò ÌïÑÌÑ∞ÎßÅ
                            songs_search = _filter_songs_by_artist(
                                songs_search, top_songs, artist_full.get("name", "")
                            )

                            # Ïï®Î≤î Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
                            for album in artist_full.get("albums", []):
                                album["artist_bid"] = artist_id
                                albums_data.append(album)

                            # 3. Î¨¥Í±∞Ïö¥ ÏûëÏóÖ(DB Ï†ÄÏû•, Ï†ÑÏ≤¥ Ïï®Î≤î Fetch)ÏùÄ Î∞±Í∑∏ÎùºÏö¥ÎìúÎ°ú ÏúÑÏûÑ
                            background_tasks.add_task(save_full_artist_data_background, artist_id, artist_info, country)
                            background_tasks.add_task(db_save_search_keyword, q, country, artist_id)

                except Exception as artist_err:
                    logger.warning(f"Artist fetch error for {artist_id}: {artist_err}")

        result = {
            "keyword": q,
            "country": country,
            "artists": artists_data,
            "songs": songs_search,
            "albums": [],
            "albums2": albums_data,
            "allTracks": all_tracks,
            "source": "ytmusicapi"
        }

        # Redis Ï∫êÏãú Ï†ÄÏû• (Î∞±Í∑∏ÎùºÏö¥Îìú)
        # cache_setÎèÑ Í∞ÄÎ≤ºÏö¥ Ïó∞ÏÇ∞Ïù¥ÎØÄÎ°ú Í∑∏ÎÉ• Ïã§ÌñâÌïòÍ±∞ÎÇò ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨ Í∞ÄÎä•ÌïòÎÇò Ïó¨Í∏∞ÏÑ† Ïú†ÏßÄ
        cache_set(cache_key, result, ttl=1800)

        return result

    except Exception as e:
        logger.error(f"Summary search error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# ÏïÑÌã∞Ïä§Ìä∏ Ï†ïÎ≥¥ API
# =============================================================================

@app.get("/api/artist/{artist_id}/songs-playlist")
async def get_artist_songs_playlist(artist_id: str, country: str = "US"):
    """
    ÏïÑÌã∞Ïä§Ìä∏Ïùò "Ïù∏Í∏∞Í≥° Î™®Îëê ÌëúÏãú" ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ID Î∞òÌôò

    YouTube Music ÏïÑÌã∞Ïä§Ìä∏ ÌéòÏù¥ÏßÄÏóêÏÑú "Ïù∏Í∏∞Í≥°" ÏÑπÏÖòÏùò "Î™®Îëê ÌëúÏãú" Î≤ÑÌäº ÎßÅÌÅ¨Ïóê ÏûàÎäî
    ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ IDÎ•º Ï∂îÏ∂úÌïòÏó¨ Î∞òÌôòÌï©ÎãàÎã§.

    Ïù¥ IDÎ•º YouTube IFrame APIÏùò loadPlaylist()Ïóê Ï†ÑÎã¨ÌïòÎ©¥
    Ìï¥Îãπ ÏïÑÌã∞Ïä§Ìä∏Ïùò Ï†ÑÏ≤¥ Ïù∏Í∏∞Í≥°ÏùÑ ÏûêÎèôÏúºÎ°ú Ïû¨ÏÉùÌï† Ïàò ÏûàÏäµÎãàÎã§.

    Returns:
        - playlistId: ÏàúÏàò ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ID (VL Ï†ëÎëêÏÇ¨ Ï†úÍ±∞Îê®)
        - browseId: ÏõêÎ≥∏ browseId (VL Ìè¨Ìï®)
        - artistName: ÏïÑÌã∞Ïä§Ìä∏ Ïù¥Î¶Ñ
        - trackCount: Ïù∏Í∏∞Í≥° ÏÑπÏÖòÏóê ÌëúÏãúÎêú Í≥° Ïàò (Ï†ÑÏ≤¥ ÏïÑÎãò)
    """
    cache_key = f"artist_songs_playlist:{artist_id}:{country}"

    cached = cache_get(cache_key)
    if cached:
        return {"source": "cache", **cached}

    try:
        ytmusic = get_ytmusic(country)
        artist = ytmusic.get_artist(artist_id)

        if not artist:
            raise HTTPException(status_code=404, detail=ERROR_ARTIST_NOT_FOUND)

        artist_name = artist.get("name") or ""
        songs_section = artist.get("songs")

        if not songs_section or not isinstance(songs_section, dict):
            raise HTTPException(status_code=404, detail="Songs section not found")

        browse_id = songs_section.get("browseId")
        if not browse_id:
            raise HTTPException(status_code=404, detail="Songs playlist ID not found")

        # "VL" Ï†ëÎëêÏÇ¨ Ï†úÍ±∞ÌïòÏó¨ ÏàúÏàò ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ID Ï∂îÏ∂ú
        # Ïòà: "VLOLAK5uy_xxx" -> "OLAK5uy_xxx"
        playlist_id = browse_id
        if browse_id.startswith("VL"):
            playlist_id = browse_id[2:]

        # Ïù∏Í∏∞Í≥° ÏÉòÌîå (Ï≤òÏùå Î™á Í∞ú)
        top_songs = songs_section.get("results") or []

        result = {
            "playlistId": playlist_id,
            "browseId": browse_id,
            "artistId": artist_id,
            "artistName": artist_name,
            "trackCount": len(top_songs),
            "topSongs": [{
                "videoId": s.get("videoId"),
                "title": s.get("title"),
                "thumbnails": s.get("thumbnails", [])
            } for s in top_songs[:5] if isinstance(s, dict)]
        }

        # 1ÏãúÍ∞Ñ Ï∫êÏãú
        cache_set(cache_key, result, ttl=3600)

        logger.info(f"Songs playlist extracted: {artist_name} -> {playlist_id}")
        return {"source": "api", **result}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Songs playlist error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Ï¥àÍ≥†ÏÜç Í≤ÄÏÉâ API (search()Îßå Ìò∏Ï∂ú, get_artist() Í±¥ÎÑàÎõ∞Í∏∞)
# =============================================================================


async def _fetch_all_albums(ytmusic, section: dict, artist_name: str, album_type: str = "Album") -> list:
    """Fetch all albums/singles from a section with proper API call."""
    albums = []
    if not section or not isinstance(section, dict):
        return albums

    browse_id = section.get("browseId")
    params = section.get("params")

    if browse_id and params:
        try:
            all_items = await run_in_thread(ytmusic.get_artist_albums, browse_id, params)
            for item in (all_items or []):
                albums.append({
                    "browseId": item.get("browseId"),
                    "title": item.get("title"),
                    "artists": [{"name": artist_name}],
                    "thumbnails": item.get("thumbnails", []),
                    "year": item.get("year"),
                    "type": album_type
                })
        except Exception as e:
            logger.warning(f"Failed to get all {album_type.lower()}s: {e}")
            # Fallback to initial results
            for item in section.get("results", []):
                albums.append({
                    "browseId": item.get("browseId"),
                    "title": item.get("title"),
                    "artists": [{"name": artist_name}],
                    "thumbnails": item.get("thumbnails", []),
                    "year": item.get("year"),
                    "type": album_type
                })
    else:
        for item in section.get("results", []):
            albums.append({
                "browseId": item.get("browseId"),
                "title": item.get("title"),
                "artists": [{"name": artist_name}],
                "thumbnails": item.get("thumbnails", []),
                "year": item.get("year"),
                "type": album_type
            })

    return albums


def _extract_similar_artists(related_section: dict, exclude_id: str = None, limit: int = 10) -> list:
    """Extract similar artists from related section."""
    similar = []
    if not related_section or not isinstance(related_section, dict):
        return similar

    for ra in related_section.get("results", []):
        if len(similar) >= limit:
            break
        ra_id = ra.get("browseId")
        if ra_id and ra_id != exclude_id:
            similar.append({
                "browseId": ra_id,
                "name": ra.get("title") or ra.get("name"),
                "thumbnail": get_best_thumbnail(ra.get("thumbnails", []))
            })

    return similar


@app.get("/api/search/quick")
async def search_quick(request: Request, q: str, country: str = None):
    """ÌÜµÌï© Í≤ÄÏÉâ - ÏïÑÌã∞Ïä§Ìä∏ + Ï†ÑÏ≤¥ Ïï®Î≤î/Ïã±Í∏Ä + Ïù∏Í∏∞Í≥° 5Í∞ú + ÎπÑÏä∑Ìïú ÏïÑÌã∞Ïä§Ìä∏ 10Î™Ö

    get_artist()Î°ú Ï†ÑÏ≤¥ ÎîîÏä§ÏΩîÍ∑∏ÎûòÌîº Ï°∞Ìöå.
    """
    if not q or len(q.strip()) < 1:
        raise HTTPException(status_code=400, detail="Query required")

    if not country:
        country = request.headers.get("CF-IPCountry", "US")

    cache_key = f"quick:{country}:{q.strip().lower()}"

    # 1. Redis Ï∫êÏãú ÌôïÏù∏
    cached = cache_get(cache_key)
    if cached:
        cached["source"] = "cache"
        return cached

    # 2. ytmusicapi Í≤ÄÏÉâ (search()Îßå Ìò∏Ï∂ú - Îπ†Î¶Ñ!)
    # DB Ï∫êÏãú ÏÇ¨Ïö© ÏïàÌï® - Ïï®Î≤î/ÎπÑÏä∑Ìïú ÏïÑÌã∞Ïä§Ìä∏ÎèÑ Î∞òÌôòÌï¥Ïïº ÌïòÎØÄÎ°ú
    try:
        ytmusic = get_ytmusic(country)

        # Î≥ëÎ†¨Î°ú ÏïÑÌã∞Ïä§Ìä∏, ÎÖ∏Îûò Í≤ÄÏÉâ (ÎπÑÏä∑Ìïú ÏïÑÌã∞Ïä§Ìä∏ 10Î™Ö Ìè¨Ìï®)
        future_artists = run_in_thread(ytmusic.search, q.strip(), filter="artists", limit=11)
        future_songs = run_in_thread(ytmusic.search, q.strip(), filter="songs", limit=5)
        artists_results, songs_results = await asyncio.gather(future_artists, future_songs)

        artist_data = None
        similar_artists = []
        albums = []
        songs_playlist_id = None

        if artists_results and len(artists_results) > 0:
            artist = artists_results[0]
            artist_id = artist.get("browseId")

            # ÏïÑÌã∞Ïä§Ìä∏ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ÏóêÏÑú Ï†ÑÏ≤¥ Ïï®Î≤î/Ïã±Í∏Ä + ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ID Í∞ÄÏ†∏Ïò§Í∏∞
            if artist_id:
                try:
                    artist_detail = await run_in_thread(ytmusic.get_artist, artist_id)

                    # ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ID Ï∂îÏ∂ú
                    songs_section = artist_detail.get("songs", {})
                    if isinstance(songs_section, dict):
                        browse_id = songs_section.get("browseId")
                        if browse_id:
                            songs_playlist_id = browse_id[2:] if browse_id.startswith("VL") else browse_id

                    artist_name = artist.get("artist") or artist.get("name")

                    # Ï†ÑÏ≤¥ Ïï®Î≤î Í∞ÄÏ†∏Ïò§Í∏∞ (Ìó¨Ìçº Ìï®Ïàò ÏÇ¨Ïö©)
                    albums_section = artist_detail.get("albums", {})
                    albums.extend(await _fetch_all_albums(ytmusic, albums_section, artist_name, "Album"))

                    # Ï†ÑÏ≤¥ Ïã±Í∏Ä Í∞ÄÏ†∏Ïò§Í∏∞ (Ìó¨Ìçº Ìï®Ïàò ÏÇ¨Ïö©)
                    singles_section = artist_detail.get("singles", {})
                    albums.extend(await _fetch_all_albums(ytmusic, singles_section, artist_name, "Single"))

                    # Ïú†ÏÇ¨ ÏïÑÌã∞Ïä§Ìä∏ Ï∂îÏ∂ú (Ìó¨Ìçº Ìï®Ïàò ÏÇ¨Ïö©)
                    related_section = artist_detail.get("related", {})
                    similar_artists = _extract_similar_artists(related_section, artist_id, 10)
                except Exception as e:
                    logger.warning(f"Failed to get artist detail: {e}")

            artist_data = {
                "browseId": artist_id,
                "name": artist.get("artist") or artist.get("name"),
                "thumbnail": get_best_thumbnail(artist.get("thumbnails", [])),
                "songsPlaylistId": songs_playlist_id
            }

            # Í≤ÄÏÉâ Í≤∞Í≥ºÏóêÏÑú Ï∂îÍ∞Ä ÎπÑÏä∑Ìïú ÏïÑÌã∞Ïä§Ìä∏ (10Î™Ö ÎØ∏ÎßåÏùº Í≤ΩÏö∞)
            for a in artists_results[1:11]:
                if len(similar_artists) >= 10:
                    break
                a_id = a.get("browseId")
                if a_id and not any(s.get("browseId") == a_id for s in similar_artists):
                    similar_artists.append({
                        "browseId": a_id,
                        "name": a.get("artist") or a.get("name"),
                        "thumbnail": get_best_thumbnail(a.get("thumbnails", []))
                    })

            # Î∞±Í∑∏ÎùºÏö¥ÎìúÏóêÏÑú DBÏóê Ï†ÄÏû• + Í∞ÄÏÉÅÌöåÏõê ÏûêÎèô ÏÉùÏÑ±
            if supabase_client and artist_data.get("browseId"):
                try:
                    browse_id = artist_data["browseId"]
                    artist_name = artist_data["name"]
                    thumbnail_url = artist_data["thumbnail"]

                    supabase_client.table("music_artists").upsert({
                        "browse_id": browse_id,
                        "name": artist_name,
                        "thumbnail_url": thumbnail_url,
                        "songs_playlist_id": songs_playlist_id,
                        "updated_at": datetime.now(timezone.utc).isoformat()
                    }, on_conflict="browse_id").execute()

                    # Í∞ÄÏÉÅÌöåÏõê ÏûêÎèô ÏÉùÏÑ±
                    try:
                        existing = supabase_client.table("profiles").select("id").eq("artist_browse_id", browse_id).execute()
                        if not existing.data or len(existing.data) == 0:
                            create_virtual_member_sync(browse_id, artist_name, thumbnail_url)
                            logger.info(f"Virtual member auto-created via search: {artist_name}")
                    except Exception as vm_error:
                        logger.warning(f"Virtual member auto-creation skipped: {vm_error}")
                except Exception:
                    pass

        # ÎÖ∏Îûò Í≤∞Í≥º Ï†ïÎ¶¨
        songs = []
        for s in (songs_results or [])[:5]:
            songs.append({
                "videoId": s.get("videoId"),
                "title": s.get("title"),
                "artists": s.get("artists", []),
                "thumbnails": s.get("thumbnails", []),
                "duration": s.get("duration"),
                "album": s.get("album")
            })

        # Ïï®Î≤îÏùÄ ÏúÑÏóêÏÑú get_artist()Î°ú Ïù¥ÎØ∏ Ï∂îÏ∂úÎê® (Ï†ÑÏ≤¥ Ïï®Î≤î + Ïã±Í∏Ä)

        response = {
            "artist": artist_data,
            "songs": songs,
            "albums": albums,
            "similarArtists": similar_artists,
            "source": "api"
        }

        cache_set(cache_key, response, ttl=1800)
        return response

    except Exception as e:
        logger.error(f"Quick search error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/artist/playlist-id")
async def get_artist_playlist_id(q: str, country: str = "US"):
    """ÏïÑÌã∞Ïä§Ìä∏ Í≤ÄÏÉâ -> ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ IDÎßå Î∞òÌôò (Ï¥àÍ≥†ÏÜç ÏóîÎìúÌè¨Ïù∏Ìä∏)

    1. SupabaseÏóêÏÑú Î®ºÏ†Ä Í≤ÄÏÉâ (Ï∫êÏãú)
    2. ÏóÜÏúºÎ©¥ ytmusicapi Ìò∏Ï∂ú ÌõÑ SupabaseÏóê Ï†ÄÏû•
    """
    if not q or len(q.strip()) < 1:
        raise HTTPException(status_code=400, detail="Query required")

    try:
        # 1. SupabaseÏóêÏÑú Î®ºÏ†Ä Í≤ÄÏÉâ (Ïù¥Î¶ÑÏúºÎ°ú Í≤ÄÏÉâ)
        if supabase_client:
            try:
                result = supabase_client.table("music_artists").select(
                    "browse_id, name, songs_playlist_id"
                ).ilike("name", f"%{q.strip()}%").limit(1).execute()

                if result.data and result.data[0].get("songs_playlist_id"):
                    cached = result.data[0]
                    logger.info(f"[playlist-id] DB hit: {cached['name']}")
                    return {
                        "playlistId": cached["songs_playlist_id"],
                        "artist": cached["name"],
                        "source": "database"
                    }
            except Exception as db_err:
                logger.warning(f"DB search error: {db_err}")

        # 2. DBÏóê ÏóÜÏúºÎ©¥ ytmusicapi Ìò∏Ï∂ú
        ytmusic = get_ytmusic(country)
        artists = await run_in_thread(ytmusic.search, q.strip(), filter="artists", limit=1)

        if not artists:
            return {"playlistId": None, "artist": None}

        artist = artists[0]
        artist_id = artist.get("browseId")
        artist_name = artist.get("artist") or artist.get("name")

        if not artist_id:
            return {"playlistId": None, "artist": artist_name}

        # 3. ÏïÑÌã∞Ïä§Ìä∏ ÏÉÅÏÑ∏ÏóêÏÑú songs.browseIdÎßå Ï∂îÏ∂ú
        artist_detail = await run_in_thread(ytmusic.get_artist, artist_id)

        songs_section = artist_detail.get("songs", {})
        songs_browse_id = songs_section.get("browseId") if isinstance(songs_section, dict) else None

        # VL Ï†úÍ±∞ -> ÏàúÏàò ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ID
        playlist_id = None
        if songs_browse_id:
            playlist_id = songs_browse_id[2:] if songs_browse_id.startswith("VL") else songs_browse_id

        # 4. SupabaseÏóê Ï†ÄÏû• (Ïû¨Í≤ÄÏÉâ Ïãú Îπ†Î•∏ ÏùëÎãµ)
        if supabase_client and playlist_id:
            try:
                supabase_client.table("music_artists").upsert({
                    "browse_id": artist_id,
                    "name": artist_name,
                    "songs_playlist_id": playlist_id,
                    "thumbnail_url": get_best_thumbnail(artist.get("thumbnails", [])),
                    "updated_at": datetime.now(timezone.utc).isoformat()
                }, on_conflict="browse_id").execute()
                logger.info(f"[playlist-id] Saved to DB: {artist_name} -> {playlist_id}")
            except Exception as save_err:
                logger.warning(f"DB save error: {save_err}")

        return {
            "playlistId": playlist_id,
            "artist": artist_name,
            "source": "api"
        }

    except Exception as e:
        logger.error(f"Playlist ID search error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/artist/{artist_id}")
async def get_artist(artist_id: str, country: str = "US", force_refresh: bool = False):
    """ÏïÑÌã∞Ïä§Ìä∏ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ + On-Demand ÏóÖÎç∞Ïù¥Ìä∏"""
    cache_key = f"artist:{artist_id}:{country}"

    # last_viewed_at ÏóÖÎç∞Ïù¥Ìä∏ (Ï°∞Ìöå Ï∂îÏ†Å)
    if supabase_client:
        try:
            supabase_client.table("music_artists").update({
                "last_viewed_at": datetime.now(timezone.utc).isoformat()
            }).eq("browse_id", artist_id).execute()
        except Exception as view_err:
            logger.debug(f"View tracking update: {view_err}")

    # On-Demand ÏóÖÎç∞Ïù¥Ìä∏: last_synced_atÏù¥ 7Ïùº Ïù¥ÏÉÅÏù¥Î©¥ ÏûêÎèô Í∞±Ïã†
    should_refresh = force_refresh
    if not should_refresh and supabase_client:
        try:
            artist_record = supabase_client.table("music_artists").select(
                "last_synced_at"
            ).eq("browse_id", artist_id).execute()

            if artist_record.data and artist_record.data[0].get("last_synced_at"):
                last_synced = datetime.fromisoformat(
                    artist_record.data[0]["last_synced_at"].replace("Z", TIMEZONE_SUFFIX)
                )
                if datetime.now(timezone.utc) - last_synced > timedelta(days=7):
                    should_refresh = True
                    logger.info(f"[ON-DEMAND] Refreshing stale artist: {artist_id}")
        except Exception as check_err:
            logger.debug(f"Stale check error: {check_err}")

    if not should_refresh:
        cached = cache_get(cache_key)
        if cached:
            return {"source": "cache", "artist": cached}

    try:
        ytmusic = get_ytmusic(country)
        artist = ytmusic.get_artist(artist_id)

        # On-Demand Í∞±Ïã† Ïãú DBÎèÑ ÏóÖÎç∞Ïù¥Ìä∏
        if should_refresh and artist:
            try:
                save_full_artist_data_background(artist_id, artist, country)
                logger.info(f"[ON-DEMAND] Updated: {artist.get('name', artist_id)}")
            except Exception as save_err:
                logger.warning(f"On-demand save failed: {save_err}")

        # 6ÏãúÍ∞Ñ Ï∫êÏãú
        cache_set(cache_key, artist, ttl=21600)

        return {"source": "api", "artist": artist, "refreshed": should_refresh}
    except Exception as e:
        logger.error(f"Artist error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/artist/{artist_id}/albums")
async def get_artist_all_albums(artist_id: str, country: str = "US"):
    """
    ÏïÑÌã∞Ïä§Ìä∏Ïùò Ï†ÑÏ≤¥ Ïï®Î≤î Î™©Î°ù (Ïï®Î≤î + Ïã±Í∏Ä)
    - Í≤ÄÏÉâ Í≤∞Í≥ºÏóêÏÑúÎäî ÏÉòÌîåÎßå Î∞òÌôò
    - Ïù¥ ÏóîÎìúÌè¨Ïù∏Ìä∏ÏóêÏÑú Ï†ÑÏ≤¥ Î™©Î°ù Î∞òÌôò
    """
    cache_key = f"artist_albums:{artist_id}:{country}"

    cached = cache_get(cache_key)
    if cached:
        return {"source": "cache", "albums": cached}

    try:
        ytmusic = get_ytmusic(country)
        artist = ytmusic.get_artist(artist_id)

        if not artist:
            raise HTTPException(status_code=404, detail=ERROR_ARTIST_NOT_FOUND)

        all_albums = []

        # Ï†ÑÏ≤¥ Ïï®Î≤î Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞
        albums_section = artist.get("albums")
        if albums_section and isinstance(albums_section, dict):
            params = albums_section.get("params")
            browse_id = albums_section.get("browseId")

            if params and browse_id:
                try:
                    full_albums = ytmusic.get_artist_albums(browse_id, params) or []
                    for album in full_albums:
                        if isinstance(album, dict) and album.get("browseId"):
                            all_albums.append({
                                "browseId": album.get("browseId"),
                                "title": album.get("title") or "",
                                "type": album.get("type") or "Album",
                                "year": album.get("year") or "",
                                "thumbnails": album.get("thumbnails") or []
                            })
                except Exception as e:
                    logger.warning(f"get_artist_albums error: {e}")
                    # ÏÉòÌîå Î™©Î°ùÏù¥ÎùºÎèÑ Î∞òÌôò
                    for album in albums_section.get("results") or []:
                        if isinstance(album, dict) and album.get("browseId"):
                            all_albums.append({
                                "browseId": album.get("browseId"),
                                "title": album.get("title") or "",
                                "type": album.get("type") or "Album",
                                "year": album.get("year") or "",
                                "thumbnails": album.get("thumbnails") or []
                            })

        # Ï†ÑÏ≤¥ Ïã±Í∏Ä Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞
        singles_section = artist.get("singles")
        if singles_section and isinstance(singles_section, dict):
            params = singles_section.get("params")
            browse_id = singles_section.get("browseId")

            if params and browse_id:
                try:
                    full_singles = ytmusic.get_artist_albums(browse_id, params) or []
                    for single in full_singles:
                        if isinstance(single, dict) and single.get("browseId"):
                            all_albums.append({
                                "browseId": single.get("browseId"),
                                "title": single.get("title") or "",
                                "type": "Single",
                                "year": single.get("year") or "",
                                "thumbnails": single.get("thumbnails") or []
                            })
                except Exception as e:
                    logger.warning(f"get_artist_albums for singles error: {e}")
                    for single in singles_section.get("results") or []:
                        if isinstance(single, dict) and single.get("browseId"):
                            all_albums.append({
                                "browseId": single.get("browseId"),
                                "title": single.get("title") or "",
                                "type": "Single",
                                "year": single.get("year") or "",
                                "thumbnails": single.get("thumbnails") or []
                            })

        # 1ÏãúÍ∞Ñ Ï∫êÏãú
        cache_set(cache_key, all_albums, ttl=3600)

        return {"source": "api", "albums": all_albums, "count": len(all_albums)}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Artist albums error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# Ïï®Î≤î Ï†ïÎ≥¥ API
# =============================================================================

@app.get("/api/album/{album_id}")
async def get_album(album_id: str, country: str = "US"):
    """
    Ïï®Î≤î ÏÉÅÏÑ∏ Ï†ïÎ≥¥ (Ìä∏Îûô Ìè¨Ìï®) - Ïò®ÎîîÎß®Îìú Î°úÎìú
    1. DBÏóêÏÑú Ìä∏ÎûôÏù¥ ÏûàÎäîÏßÄ ÌôïÏù∏
    2. ÏóÜÏúºÎ©¥ ytmusicapiÏóêÏÑú Í∞ÄÏ†∏ÏôÄÏÑú DBÏóê Ï†ÄÏû•
    """
    cache_key = f"album:{album_id}:{country}"

    # Redis Ï∫êÏãú ÌôïÏù∏
    cached = cache_get(cache_key)
    if cached:
        return {"source": "cache", "album": cached}

    # DBÏóêÏÑú Ìä∏Îûô ÌôïÏù∏
    if supabase_client:
        try:
            tracks_result = supabase_client.table("music_tracks").select("*").eq(
                "album_browse_id", album_id
            ).order("track_number").execute()

            if tracks_result.data and len(tracks_result.data) > 0:
                # DBÏóê Ìä∏ÎûôÏù¥ ÏûàÏùå - Ïï®Î≤î Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÎèÑ Í∞ÄÏ†∏Ïò§Í∏∞
                album_result = supabase_client.table("music_albums").select("*").eq(
                    "browse_id", album_id
                ).single().execute()

                if album_result.data:
                    album_data = {
                        "browseId": album_result.data.get("browse_id"),
                        "title": album_result.data.get("title"),
                        "type": album_result.data.get("type"),
                        "year": album_result.data.get("year"),
                        "thumbnails": [{"url": album_result.data.get("thumbnail_url")}] if album_result.data.get("thumbnail_url") else [],
                        "tracks": [{
                            "videoId": t.get("video_id"),
                            "title": t.get("title"),
                            "duration": t.get("duration"),
                            "trackNumber": t.get("track_number"),
                            "thumbnails": [{"url": t.get("thumbnail_url")}] if t.get("thumbnail_url") else [],
                            "isExplicit": t.get("is_explicit", False)
                        } for t in tracks_result.data]
                    }

                    # RedisÏóê Ï∫êÏãú
                    cache_set(cache_key, album_data, ttl=21600)

                    logger.info(f"Album from DB: {album_id} ({len(tracks_result.data)} tracks)")
                    return {"source": "database", "album": album_data}
        except Exception as e:
            logger.warning(f"DB album fetch error: {e}")

    # ytmusicapiÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞
    try:
        ytmusic = get_ytmusic(country)
        album = ytmusic.get_album(album_id)

        if album and supabase_client:
            # DBÏóê Ìä∏Îûô Ï†ÄÏû•
            tracks = album.get("tracks") or []
            artist_browse_id = None

            # ÏïÑÌã∞Ïä§Ìä∏ ID Ï∂îÏ∂ú
            artists = album.get("artists") or []
            if artists and isinstance(artists, list) and len(artists) > 0:
                artist_browse_id = artists[0].get("id") or artists[0].get("browseId")

            # Ïï®Î≤î Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ (Ìä∏Îûô Ïàò Ìè¨Ìï®)
            try:
                album_update = {
                    "browse_id": album_id,
                    "title": album.get("title") or "",
                    "type": album.get("type") or "Album",
                    "year": album.get("year") or "",
                    "thumbnail_url": get_best_thumbnail(album.get("thumbnails", [])),
                    "track_count": len(tracks)
                }
                if artist_browse_id:
                    album_update["artist_browse_id"] = artist_browse_id

                supabase_client.table("music_albums").upsert(
                    album_update, on_conflict="browse_id"
                ).execute()
            except Exception as e:
                logger.warning(f"Album metadata update error: {e}")

            # Ìä∏Îûô Ï†ÄÏû•
            for idx, track in enumerate(tracks):
                if not isinstance(track, dict):
                    continue
                video_id = track.get("videoId")
                if not video_id:
                    continue

                try:
                    track_data = {
                        "video_id": video_id,
                        "album_browse_id": album_id,
                        "artist_browse_id": artist_browse_id,
                        "title": track.get("title") or "",
                        "duration": track.get("duration") or "",
                        "track_number": idx + 1,
                        "thumbnail_url": get_best_thumbnail(track.get("thumbnails") or album.get("thumbnails", [])),
                        "is_explicit": track.get("isExplicit", False)
                    }

                    supabase_client.table("music_tracks").upsert(
                        track_data, on_conflict="video_id"
                    ).execute()
                except Exception as e:
                    logger.warning(f"Track save error: {e}")

            logger.info(f"Album tracks saved to DB: {album_id} ({len(tracks)} tracks)")

        # Ìä∏ÎûôÏóê Ïç∏ÎÑ§ÏùºÏù¥ ÏóÜÏúºÎ©¥ Ïï®Î≤î Ïç∏ÎÑ§Ïùº Ï∂îÍ∞Ä
        album_thumbnails = album.get("thumbnails", [])
        for track in album.get("tracks", []):
            if not track.get("thumbnails") and album_thumbnails:
                track["thumbnails"] = album_thumbnails

        return {"source": "api", "album": album}
    except Exception as e:
        logger.error(f"Album error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ Ï†ïÎ≥¥ API
# =============================================================================

@app.get("/api/playlist/{playlist_id}")
async def get_playlist(playlist_id: str, country: str = "US", limit: int = None):
    """ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ (Ìä∏Îûô Î™©Î°ù Ìè¨Ìï®) - limit=NoneÏù¥Î©¥ Ï†ÑÏ≤¥ Ìä∏Îûô"""
    cache_key = f"playlist:{playlist_id}:{country}:{limit or 'all'}"

    cached = cache_get(cache_key)
    if cached:
        return {"source": "cache", "playlist": cached}

    try:
        ytmusic = get_ytmusic(country)
        # limit=NoneÏù¥Î©¥ Ï†ÑÏ≤¥ Ìä∏ÎûôÏùÑ Í∞ÄÏ†∏Ïò¥
        playlist = ytmusic.get_playlist(playlist_id, limit=limit)

        # Ìä∏ÎûôÏóê Ïç∏ÎÑ§ÏùºÏù¥ ÏóÜÏúºÎ©¥ ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ Ïç∏ÎÑ§Ïùº Ï∂îÍ∞Ä
        playlist_thumbnails = playlist.get("thumbnails", [])
        for track in playlist.get("tracks", []):
            if not track.get("thumbnails") and playlist_thumbnails:
                track["thumbnails"] = playlist_thumbnails

        return {"source": "api", "playlist": playlist}
    except Exception as e:
        logger.error(f"Playlist error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# Ï∫êÏãú Í¥ÄÎ¶¨ API
# =============================================================================

@app.delete("/api/cache/clear")
async def clear_search_cache(secret: str = None):
    """Í≤ÄÏÉâ Ï∫êÏãú Ï†ÑÏ≤¥ ÏÇ≠Ï†ú (Supabase music_search_cache ÌÖåÏù¥Î∏î)"""
    # Í∞ÑÎã®Ìïú Î≥¥Ïïà Ï≤¥ÌÅ¨ (ÌîÑÎ°úÎçïÏÖòÏóêÏÑúÎäî Îçî Í∞ïÎ†•Ìïú Ïù∏Ï¶ù ÌïÑÏöî)
    admin_secret = os.getenv("ADMIN_SECRET", "sori-admin-2024")
    if secret != admin_secret:
        raise HTTPException(status_code=403, detail="Unauthorized")

    if not supabase_client:
        raise HTTPException(status_code=500, detail="Supabase not configured")

    try:
        # music_search_cache ÌÖåÏù¥Î∏î Ï†ÑÏ≤¥ ÏÇ≠Ï†ú
        result = supabase_client.table("music_search_cache").delete().neq("id", "00000000-0000-0000-0000-000000000000").execute()

        # Î©îÎ™®Î¶¨ Ï∫êÏãúÎèÑ ÌÅ¥Î¶¨Ïñ¥
        global memory_cache
        memory_cache = {}

        deleted_count = len(result.data) if result.data else 0
        logger.info(f"Cleared {deleted_count} cache entries")

        return {
            "status": "success",
            "message": f"Cleared {deleted_count} cache entries",
            "deleted_count": deleted_count
        }
    except Exception as e:
        logger.error(f"Cache clear error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/admin/run-sql")
async def run_custom_sql(secret: str = None, access_token: str = None, sql: str = None):
    """ÏûÑÏùòÏùò SQL Ïã§Ìñâ - Management API ÏÇ¨Ïö©"""
    import httpx

    admin_secret = os.getenv("ADMIN_SECRET", "sori-admin-2024")
    if secret != admin_secret:
        raise HTTPException(status_code=403, detail="Unauthorized")

    if not sql:
        raise HTTPException(status_code=400, detail="sql parameter required")

    mgmt_token = access_token or os.getenv("SUPABASE_ACCESS_TOKEN")
    if not mgmt_token:
        raise HTTPException(status_code=400, detail=ERROR_ACCESS_TOKEN_REQUIRED)

    project_ref = "nrtkbulkzhhlstaomvas"

    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.post(
            f"https://api.supabase.com/v1/projects/{project_ref}/database/query",
            headers={
                "Authorization": f"Bearer {mgmt_token}",
                "Content-Type": CONTENT_TYPE_JSON
            },
            json={"query": sql}
        )

        if response.status_code != 201:
            return {"success": False, "error": response.text, "status": response.status_code}

        return {"success": True, "result": response.json()}


@app.post("/api/admin/fix-notification-triggers")
async def fix_notification_triggers(secret: str = None, access_token: str = None):
    """ÏïåÎ¶º Ìä∏Î¶¨Í±∞ ÏàòÏ†ï - posts ÌÖåÏù¥Î∏îÏö©"""
    import httpx

    admin_secret = os.getenv("ADMIN_SECRET", "sori-admin-2024")
    if secret != admin_secret:
        raise HTTPException(status_code=403, detail="Unauthorized")

    mgmt_token = access_token or os.getenv("SUPABASE_ACCESS_TOKEN")
    if not mgmt_token:
        raise HTTPException(status_code=400, detail=ERROR_ACCESS_TOKEN_REQUIRED)

    project_ref = "nrtkbulkzhhlstaomvas"

    sql_commands = [
        # Fix like notification trigger
        """CREATE OR REPLACE FUNCTION create_post_like_notification()
RETURNS TRIGGER AS $$
DECLARE
    post_owner_id UUID;
BEGIN
    SELECT user_id INTO post_owner_id FROM posts WHERE id = NEW.post_id;
    IF post_owner_id IS NOT NULL AND post_owner_id != NEW.user_id THEN
        INSERT INTO notifications (user_id, actor_id, type, reference_id, reference_type, content)
        VALUES (post_owner_id, NEW.user_id, 'like', NEW.post_id, 'post', 'liked your post');
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER""",

        # Fix comment notification trigger
        """CREATE OR REPLACE FUNCTION create_post_comment_notification()
RETURNS TRIGGER AS $$
DECLARE
    post_owner_id UUID;
BEGIN
    SELECT user_id INTO post_owner_id FROM posts WHERE id = NEW.post_id;
    IF post_owner_id IS NOT NULL AND post_owner_id != NEW.user_id THEN
        INSERT INTO notifications (user_id, actor_id, type, reference_id, reference_type, content)
        VALUES (post_owner_id, NEW.user_id, 'comment', NEW.post_id, 'post', 'commented on your post');
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER""",

        # Recreate triggers
        "DROP TRIGGER IF EXISTS on_post_like_notify ON post_likes",
        """CREATE TRIGGER on_post_like_notify
    AFTER INSERT ON post_likes
    FOR EACH ROW EXECUTE FUNCTION create_post_like_notification()""",

        "DROP TRIGGER IF EXISTS on_post_comment_notify ON post_comments",
        """CREATE TRIGGER on_post_comment_notify
    AFTER INSERT ON post_comments
    FOR EACH ROW EXECUTE FUNCTION create_post_comment_notification()"""
    ]

    results = []
    async with httpx.AsyncClient(timeout=30.0) as client:
        for i, sql in enumerate(sql_commands):
            response = await client.post(
                f"https://api.supabase.com/v1/projects/{project_ref}/database/query",
                headers={
                    "Authorization": f"Bearer {mgmt_token}",
                    "Content-Type": CONTENT_TYPE_JSON
                },
                json={"query": sql}
            )

            results.append({
                "index": i,
                "success": response.status_code == 201,
                "status": response.status_code,
                "error": response.text if response.status_code != 201 else None
            })

    return {"results": results, "total": len(sql_commands), "success": all(r["success"] for r in results)}


@app.post("/api/admin/fix-advisor")
async def fix_advisor_warnings(secret: str = None, access_token: str = None):
    """Supabase Advisor Í≤ΩÍ≥† ÏàòÏ†ï - Management APIÎ°ú SQL Ïã§Ìñâ"""
    import httpx

    admin_secret = os.getenv("ADMIN_SECRET", "sori-admin-2024")
    if secret != admin_secret:
        raise HTTPException(status_code=403, detail="Unauthorized")

    # Supabase Management API ÌÜ†ÌÅ∞ (ÌôòÍ≤ΩÎ≥ÄÏàò ÎòêÎäî ÌååÎùºÎØ∏ÌÑ∞)
    mgmt_token = access_token or os.getenv("SUPABASE_ACCESS_TOKEN")
    if not mgmt_token:
        raise HTTPException(status_code=400, detail=ERROR_ACCESS_TOKEN_REQUIRED)

    project_ref = "nrtkbulkzhhlstaomvas"

    # SQL Î™ÖÎ†πÏñ¥Îì§ (ÌïòÎÇòÏî© Ïã§Ìñâ)
    sql_commands = [
        "DROP TABLE IF EXISTS music_tracks CASCADE",
        "DROP TABLE IF EXISTS music_albums CASCADE",
        "DROP TABLE IF EXISTS music_artists CASCADE",
        "DROP TABLE IF EXISTS artist_relations CASCADE",
        "DROP FUNCTION IF EXISTS search_music_artists(text, integer)",
        "DROP FUNCTION IF EXISTS get_artist_full_data(text)",
        "DROP FUNCTION IF EXISTS normalize_music_text() CASCADE",
        'DROP POLICY IF EXISTS "Users can insert their own profile." ON profiles',
        'DROP POLICY IF EXISTS "Users can update own profile." ON profiles',
        'CREATE POLICY "Users can insert their own profile." ON profiles FOR INSERT WITH CHECK ((select auth.uid()) = id)',
        'CREATE POLICY "Users can update own profile." ON profiles FOR UPDATE USING ((select auth.uid()) = id)',
        'DROP POLICY IF EXISTS "Users can insert their own playlists." ON playlists',
        'DROP POLICY IF EXISTS "Users can update their own playlists." ON playlists',
        'CREATE POLICY "Users can insert their own playlists." ON playlists FOR INSERT WITH CHECK ((select auth.uid()) = user_id)',
        'CREATE POLICY "Users can update their own playlists." ON playlists FOR UPDATE USING ((select auth.uid()) = user_id)',
        'DROP POLICY IF EXISTS "Search cache is viewable by everyone" ON music_search_cache',
        'DROP POLICY IF EXISTS "Service role can manage music_search_cache" ON music_search_cache',
        'CREATE POLICY "Public read access" ON music_search_cache FOR SELECT USING (true)',
        "CREATE SCHEMA IF NOT EXISTS extensions",
        "DROP EXTENSION IF EXISTS pg_trgm",
        "CREATE EXTENSION IF NOT EXISTS pg_trgm SCHEMA extensions",
        """CREATE OR REPLACE FUNCTION public.handle_new_user()
RETURNS trigger LANGUAGE plpgsql SECURITY DEFINER SET search_path = public AS $$
BEGIN
  INSERT INTO public.profiles (id, full_name, avatar_url)
  VALUES (new.id, new.raw_user_meta_data->>'full_name', new.raw_user_meta_data->>'avatar_url');
  RETURN new;
END;
$$"""
    ]

    results = []

    async with httpx.AsyncClient(timeout=30.0) as client:
        for sql in sql_commands:
            try:
                response = await client.post(
                    f"https://api.supabase.com/v1/projects/{project_ref}/database/query",
                    headers={
                        "Authorization": f"Bearer {mgmt_token}",
                        "Content-Type": CONTENT_TYPE_JSON
                    },
                    json={"query": sql}
                )

                if response.status_code == 201 or response.status_code == 200:
                    results.append({"sql": sql[:50] + "...", "status": "success"})
                else:
                    results.append({"sql": sql[:50] + "...", "status": "error", "code": response.status_code, "error": response.text})

            except Exception as e:
                results.append({"sql": sql[:50] + "...", "status": "error", "error": str(e)})

    success_count = len([r for r in results if r["status"] == "success"])
    return {
        "status": "completed",
        "message": f"{success_count}/{len(sql_commands)} commands executed",
        "results": results
    }


@app.post("/api/admin/run-migrations")
async def run_migrations(secret: str = None, access_token: str = None):
    """SNS Í∏∞Îä• ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Ïã§Ìñâ - follows, likes, stories, messages, hashtags, reposts, comments, notifications"""
    import httpx

    admin_secret = os.getenv("ADMIN_SECRET", "sori-admin-2024")
    if secret != admin_secret:
        raise HTTPException(status_code=403, detail="Unauthorized")

    mgmt_token = access_token or os.getenv("SUPABASE_ACCESS_TOKEN")
    if not mgmt_token:
        raise HTTPException(status_code=400, detail=ERROR_ACCESS_TOKEN_REQUIRED)

    project_ref = "nrtkbulkzhhlstaomvas"

    sql_commands = [
        # =====================================================
        # FOLLOWS TABLE - ÌåîÎ°úÏö∞ ÏãúÏä§ÌÖú
        # =====================================================
        """CREATE TABLE IF NOT EXISTS follows (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            follower_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            following_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            created_at TIMESTAMPTZ DEFAULT NOW(),
            UNIQUE(follower_id, following_id),
            CHECK (follower_id != following_id)
        )""",
        "ALTER TABLE follows ENABLE ROW LEVEL SECURITY",
        'DROP POLICY IF EXISTS "Anyone can view follows" ON follows',
        """CREATE POLICY "Anyone can view follows" ON follows FOR SELECT USING (true)""",
        'DROP POLICY IF EXISTS "Users can follow others" ON follows',
        """CREATE POLICY "Users can follow others" ON follows FOR INSERT WITH CHECK (auth.uid() = follower_id)""",
        'DROP POLICY IF EXISTS "Users can unfollow" ON follows',
        """CREATE POLICY "Users can unfollow" ON follows FOR DELETE USING (auth.uid() = follower_id)""",
        "CREATE INDEX IF NOT EXISTS idx_follows_follower ON follows(follower_id)",
        "CREATE INDEX IF NOT EXISTS idx_follows_following ON follows(following_id)",

        # Profiles - followers/following count columns
        "ALTER TABLE profiles ADD COLUMN IF NOT EXISTS followers_count INTEGER DEFAULT 0",
        "ALTER TABLE profiles ADD COLUMN IF NOT EXISTS following_count INTEGER DEFAULT 0",
        "ALTER TABLE profiles ADD COLUMN IF NOT EXISTS bio TEXT",
        "ALTER TABLE profiles ADD COLUMN IF NOT EXISTS created_at TIMESTAMPTZ DEFAULT NOW()",

        # Follow count trigger function
        """CREATE OR REPLACE FUNCTION update_follow_counts()
        RETURNS TRIGGER AS $$
        BEGIN
            IF TG_OP = 'INSERT' THEN
                UPDATE profiles SET followers_count = followers_count + 1 WHERE id = NEW.following_id;
                UPDATE profiles SET following_count = following_count + 1 WHERE id = NEW.follower_id;
                RETURN NEW;
            ELSIF TG_OP = 'DELETE' THEN
                UPDATE profiles SET followers_count = GREATEST(0, followers_count - 1) WHERE id = OLD.following_id;
                UPDATE profiles SET following_count = GREATEST(0, following_count - 1) WHERE id = OLD.follower_id;
                RETURN OLD;
            END IF;
            RETURN NULL;
        END;
        $$ LANGUAGE plpgsql SECURITY DEFINER""",
        "DROP TRIGGER IF EXISTS on_follow_change ON follows",
        """CREATE TRIGGER on_follow_change
            AFTER INSERT OR DELETE ON follows
            FOR EACH ROW EXECUTE FUNCTION update_follow_counts()""",

        # =====================================================
        # LIKES TABLE - Ï¢ãÏïÑÏöî ÏãúÏä§ÌÖú
        # =====================================================
        """CREATE TABLE IF NOT EXISTS likes (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            post_id UUID NOT NULL REFERENCES playlists(id) ON DELETE CASCADE,
            created_at TIMESTAMPTZ DEFAULT NOW(),
            UNIQUE(user_id, post_id)
        )""",
        "ALTER TABLE likes ENABLE ROW LEVEL SECURITY",
        'DROP POLICY IF EXISTS "Anyone can view likes" ON likes',
        """CREATE POLICY "Anyone can view likes" ON likes FOR SELECT USING (true)""",
        'DROP POLICY IF EXISTS "Users can like posts" ON likes',
        """CREATE POLICY "Users can like posts" ON likes FOR INSERT WITH CHECK (auth.uid() = user_id)""",
        'DROP POLICY IF EXISTS "Users can unlike posts" ON likes',
        """CREATE POLICY "Users can unlike posts" ON likes FOR DELETE USING (auth.uid() = user_id)""",
        "CREATE INDEX IF NOT EXISTS idx_likes_user ON likes(user_id)",
        "CREATE INDEX IF NOT EXISTS idx_likes_post ON likes(post_id)",

        # Playlists - like_count column
        "ALTER TABLE playlists ADD COLUMN IF NOT EXISTS like_count INTEGER DEFAULT 0",

        # Like count trigger function
        """CREATE OR REPLACE FUNCTION update_like_counts()
        RETURNS TRIGGER AS $$
        BEGIN
            IF TG_OP = 'INSERT' THEN
                UPDATE playlists SET like_count = like_count + 1 WHERE id = NEW.post_id;
                RETURN NEW;
            ELSIF TG_OP = 'DELETE' THEN
                UPDATE playlists SET like_count = GREATEST(0, like_count - 1) WHERE id = OLD.post_id;
                RETURN OLD;
            END IF;
            RETURN NULL;
        END;
        $$ LANGUAGE plpgsql SECURITY DEFINER""",
        "DROP TRIGGER IF EXISTS on_like_change ON likes",
        """CREATE TRIGGER on_like_change
            AFTER INSERT OR DELETE ON likes
            FOR EACH ROW EXECUTE FUNCTION update_like_counts()""",

        # =====================================================
        # CONVERSATIONS TABLES (must be created before messages can reference them)
        # =====================================================
        """CREATE TABLE IF NOT EXISTS conversations (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            created_at TIMESTAMPTZ DEFAULT NOW(),
            updated_at TIMESTAMPTZ DEFAULT NOW()
        )""",
        """CREATE TABLE IF NOT EXISTS conversation_participants (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
            user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            last_read_at TIMESTAMPTZ DEFAULT NOW(),
            UNIQUE(conversation_id, user_id)
        )""",
        """CREATE TABLE IF NOT EXISTS messages (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
            sender_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            content TEXT NOT NULL,
            shared_track_id TEXT,
            shared_track_title TEXT,
            shared_track_artist TEXT,
            shared_track_thumbnail TEXT,
            created_at TIMESTAMPTZ DEFAULT NOW()
        )""",
        "ALTER TABLE conversations ENABLE ROW LEVEL SECURITY",
        "ALTER TABLE conversation_participants ENABLE ROW LEVEL SECURITY",
        "ALTER TABLE messages ENABLE ROW LEVEL SECURITY",
        'DROP POLICY IF EXISTS "Users can view own conversations" ON conversations',
        """CREATE POLICY "Users can view own conversations" ON conversations FOR SELECT USING (
            EXISTS (SELECT 1 FROM conversation_participants WHERE conversation_id = id AND user_id = auth.uid())
        )""",
        'DROP POLICY IF EXISTS "Users can create conversations" ON conversations',
        """CREATE POLICY "Users can create conversations" ON conversations FOR INSERT WITH CHECK (true)""",
        'DROP POLICY IF EXISTS "Users can view own participations" ON conversation_participants',
        """CREATE POLICY "Users can view own participations" ON conversation_participants FOR SELECT USING (
            EXISTS (SELECT 1 FROM conversation_participants cp WHERE cp.conversation_id = conversation_id AND cp.user_id = auth.uid())
        )""",
        'DROP POLICY IF EXISTS "Users can create participations" ON conversation_participants',
        """CREATE POLICY "Users can create participations" ON conversation_participants FOR INSERT WITH CHECK (true)""",
        'DROP POLICY IF EXISTS "Users can update own participation" ON conversation_participants',
        """CREATE POLICY "Users can update own participation" ON conversation_participants FOR UPDATE USING (auth.uid() = user_id)""",
        'DROP POLICY IF EXISTS "Users can view messages in own conversations" ON messages',
        """CREATE POLICY "Users can view messages in own conversations" ON messages FOR SELECT USING (
            EXISTS (SELECT 1 FROM conversation_participants WHERE conversation_id = messages.conversation_id AND user_id = auth.uid())
        )""",
        'DROP POLICY IF EXISTS "Users can send messages" ON messages',
        """CREATE POLICY "Users can send messages" ON messages FOR INSERT WITH CHECK (auth.uid() = sender_id)""",
        "CREATE INDEX IF NOT EXISTS idx_messages_conversation ON messages(conversation_id)",
        "CREATE INDEX IF NOT EXISTS idx_conversation_participants_user ON conversation_participants(user_id)",
        "CREATE INDEX IF NOT EXISTS idx_conversation_participants_conv ON conversation_participants(conversation_id)",

        # =====================================================
        # STORIES TABLE
        # =====================================================
        # Stories table
        """CREATE TABLE IF NOT EXISTS stories (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            content_type TEXT NOT NULL CHECK (content_type IN ('track', 'playlist', 'text')),
            video_id TEXT,
            playlist_id UUID REFERENCES playlists(id) ON DELETE SET NULL,
            title TEXT,
            artist TEXT,
            cover_url TEXT,
            text_content TEXT,
            background_color TEXT DEFAULT '#000000',
            created_at TIMESTAMPTZ DEFAULT NOW(),
            expires_at TIMESTAMPTZ DEFAULT (NOW() + INTERVAL '24 hours'),
            is_active BOOLEAN DEFAULT TRUE
        )""",
        # Story views
        """CREATE TABLE IF NOT EXISTS story_views (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            story_id UUID NOT NULL REFERENCES stories(id) ON DELETE CASCADE,
            viewer_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            viewed_at TIMESTAMPTZ DEFAULT NOW(),
            UNIQUE(story_id, viewer_id)
        )""",
        # Stories RLS
        "ALTER TABLE stories ENABLE ROW LEVEL SECURITY",
        "ALTER TABLE story_views ENABLE ROW LEVEL SECURITY",
        'DROP POLICY IF EXISTS "Anyone can view active stories" ON stories',
        """CREATE POLICY "Anyone can view active stories" ON stories FOR SELECT USING (is_active = TRUE AND expires_at > NOW())""",
        'DROP POLICY IF EXISTS "Users can create own stories" ON stories',
        """CREATE POLICY "Users can create own stories" ON stories FOR INSERT WITH CHECK (auth.uid() = user_id)""",
        'DROP POLICY IF EXISTS "Users can delete own stories" ON stories',
        """CREATE POLICY "Users can delete own stories" ON stories FOR DELETE USING (auth.uid() = user_id)""",
        'DROP POLICY IF EXISTS "Anyone can create story views" ON story_views',
        """CREATE POLICY "Anyone can create story views" ON story_views FOR INSERT WITH CHECK (viewer_id = auth.uid())""",

        # Comments table
        """CREATE TABLE IF NOT EXISTS comments (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            post_id UUID NOT NULL REFERENCES playlists(id) ON DELETE CASCADE,
            user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            parent_id UUID REFERENCES comments(id) ON DELETE CASCADE,
            content TEXT NOT NULL,
            created_at TIMESTAMPTZ DEFAULT NOW(),
            updated_at TIMESTAMPTZ DEFAULT NOW()
        )""",
        "ALTER TABLE comments ENABLE ROW LEVEL SECURITY",
        'DROP POLICY IF EXISTS "Anyone can view comments" ON comments',
        """CREATE POLICY "Anyone can view comments" ON comments FOR SELECT USING (true)""",
        'DROP POLICY IF EXISTS "Users can create comments" ON comments',
        """CREATE POLICY "Users can create comments" ON comments FOR INSERT WITH CHECK (auth.uid() = user_id)""",
        'DROP POLICY IF EXISTS "Users can delete own comments" ON comments',
        """CREATE POLICY "Users can delete own comments" ON comments FOR DELETE USING (auth.uid() = user_id)""",

        # Add comment_count to playlists if not exists
        "ALTER TABLE playlists ADD COLUMN IF NOT EXISTS comment_count INTEGER DEFAULT 0",

        # Notifications table
        """CREATE TABLE IF NOT EXISTS notifications (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            type TEXT NOT NULL,
            actor_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
            post_id UUID REFERENCES playlists(id) ON DELETE CASCADE,
            comment_id UUID,
            message TEXT,
            is_read BOOLEAN DEFAULT FALSE,
            created_at TIMESTAMPTZ DEFAULT NOW()
        )""",
        "ALTER TABLE notifications ENABLE ROW LEVEL SECURITY",
        'DROP POLICY IF EXISTS "Users can view own notifications" ON notifications',
        """CREATE POLICY "Users can view own notifications" ON notifications FOR SELECT USING (auth.uid() = user_id)""",
        'DROP POLICY IF EXISTS "System can create notifications" ON notifications',
        """CREATE POLICY "System can create notifications" ON notifications FOR INSERT WITH CHECK (true)""",
        'DROP POLICY IF EXISTS "Users can update own notifications" ON notifications',
        """CREATE POLICY "Users can update own notifications" ON notifications FOR UPDATE USING (auth.uid() = user_id)""",

        # Conversations table
        """CREATE TABLE IF NOT EXISTS conversations (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            created_at TIMESTAMPTZ DEFAULT NOW(),
            updated_at TIMESTAMPTZ DEFAULT NOW()
        )""",
        """CREATE TABLE IF NOT EXISTS conversation_participants (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
            user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            last_read_at TIMESTAMPTZ DEFAULT NOW(),
            UNIQUE(conversation_id, user_id)
        )""",
        """CREATE TABLE IF NOT EXISTS messages (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            conversation_id UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
            sender_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            content TEXT NOT NULL,
            shared_track_id TEXT,
            shared_track_title TEXT,
            shared_track_artist TEXT,
            shared_track_thumbnail TEXT,
            created_at TIMESTAMPTZ DEFAULT NOW()
        )""",
        "ALTER TABLE conversations ENABLE ROW LEVEL SECURITY",
        "ALTER TABLE conversation_participants ENABLE ROW LEVEL SECURITY",
        "ALTER TABLE messages ENABLE ROW LEVEL SECURITY",
        'DROP POLICY IF EXISTS "Users can view own conversations" ON conversations',
        """CREATE POLICY "Users can view own conversations" ON conversations FOR SELECT USING (
            EXISTS (SELECT 1 FROM conversation_participants WHERE conversation_id = id AND user_id = auth.uid())
        )""",
        'DROP POLICY IF EXISTS "Users can view own participations" ON conversation_participants',
        """CREATE POLICY "Users can view own participations" ON conversation_participants FOR SELECT USING (
            EXISTS (SELECT 1 FROM conversation_participants cp WHERE cp.conversation_id = conversation_id AND cp.user_id = auth.uid())
        )""",
        'DROP POLICY IF EXISTS "Users can view messages in own conversations" ON messages',
        """CREATE POLICY "Users can view messages in own conversations" ON messages FOR SELECT USING (
            EXISTS (SELECT 1 FROM conversation_participants WHERE conversation_id = messages.conversation_id AND user_id = auth.uid())
        )""",
        'DROP POLICY IF EXISTS "Users can send messages" ON messages',
        """CREATE POLICY "Users can send messages" ON messages FOR INSERT WITH CHECK (auth.uid() = sender_id)""",

        # Hashtags table
        """CREATE TABLE IF NOT EXISTS hashtags (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            name TEXT NOT NULL UNIQUE,
            post_count INTEGER DEFAULT 0,
            created_at TIMESTAMPTZ DEFAULT NOW()
        )""",
        """CREATE TABLE IF NOT EXISTS post_hashtags (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            post_id UUID NOT NULL REFERENCES playlists(id) ON DELETE CASCADE,
            hashtag_id UUID NOT NULL REFERENCES hashtags(id) ON DELETE CASCADE,
            created_at TIMESTAMPTZ DEFAULT NOW(),
            UNIQUE(post_id, hashtag_id)
        )""",
        "ALTER TABLE hashtags ENABLE ROW LEVEL SECURITY",
        "ALTER TABLE post_hashtags ENABLE ROW LEVEL SECURITY",
        'DROP POLICY IF EXISTS "Anyone can view hashtags" ON hashtags',
        """CREATE POLICY "Anyone can view hashtags" ON hashtags FOR SELECT USING (true)""",
        'DROP POLICY IF EXISTS "Anyone can view post_hashtags" ON post_hashtags',
        """CREATE POLICY "Anyone can view post_hashtags" ON post_hashtags FOR SELECT USING (true)""",

        # Reposts table
        """CREATE TABLE IF NOT EXISTS reposts (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
            post_id UUID NOT NULL REFERENCES playlists(id) ON DELETE CASCADE,
            quote TEXT,
            created_at TIMESTAMPTZ DEFAULT NOW(),
            UNIQUE(user_id, post_id)
        )""",
        "ALTER TABLE playlists ADD COLUMN IF NOT EXISTS repost_count INTEGER DEFAULT 0",
        "ALTER TABLE reposts ENABLE ROW LEVEL SECURITY",
        'DROP POLICY IF EXISTS "Anyone can view reposts" ON reposts',
        """CREATE POLICY "Anyone can view reposts" ON reposts FOR SELECT USING (true)""",
        'DROP POLICY IF EXISTS "Users can create reposts" ON reposts',
        """CREATE POLICY "Users can create reposts" ON reposts FOR INSERT WITH CHECK (auth.uid() = user_id)""",
        'DROP POLICY IF EXISTS "Users can delete own reposts" ON reposts',
        """CREATE POLICY "Users can delete own reposts" ON reposts FOR DELETE USING (auth.uid() = user_id)""",

        # Indexes
        "CREATE INDEX IF NOT EXISTS idx_stories_user ON stories(user_id)",
        "CREATE INDEX IF NOT EXISTS idx_stories_expires ON stories(expires_at)",
        "CREATE INDEX IF NOT EXISTS idx_comments_post ON comments(post_id)",
        "CREATE INDEX IF NOT EXISTS idx_notifications_user ON notifications(user_id)",
        "CREATE INDEX IF NOT EXISTS idx_messages_conversation ON messages(conversation_id)",
        "CREATE INDEX IF NOT EXISTS idx_hashtags_name ON hashtags(name)",
        "CREATE INDEX IF NOT EXISTS idx_reposts_user ON reposts(user_id)",
    ]

    results = []

    async with httpx.AsyncClient(timeout=30.0) as client:
        for sql in sql_commands:
            try:
                response = await client.post(
                    f"https://api.supabase.com/v1/projects/{project_ref}/database/query",
                    headers={
                        "Authorization": f"Bearer {mgmt_token}",
                        "Content-Type": CONTENT_TYPE_JSON
                    },
                    json={"query": sql}
                )

                if response.status_code in [200, 201]:
                    results.append({"sql": sql[:60] + "...", "status": "success"})
                else:
                    results.append({"sql": sql[:60] + "...", "status": "error", "code": response.status_code, "error": response.text[:200]})

            except Exception as e:
                results.append({"sql": sql[:60] + "...", "status": "error", "error": str(e)})

    success_count = len([r for r in results if r["status"] == "success"])
    return {
        "status": "completed",
        "message": f"{success_count}/{len(sql_commands)} commands executed",
        "results": results
    }


# =============================================================================
# ÏóîÌä∏Î¶¨ Ìè¨Ïù∏Ìä∏
# =============================================================================

@app.get("/api/search/summary")
async def search_summary(
    request: Request,
    q: str,
    background_tasks: BackgroundTasks,
    country: str = None,
    force_refresh: bool = False
):
    """
    [Optimized] ÏïÑÌã∞Ïä§Ìä∏ Í≤ÄÏÉâ Î∞è Ï†ÑÏ≤¥ ÎîîÏä§ÏΩîÍ∑∏ÎûòÌîº Î∞òÌôò
    - Non-blocking search
    - Exact artist match priority
    - Immediate response without extra fetches
    """
    if not country:
        country = request.headers.get("CF-IPCountry", "US")

    cache_key = f"summary:{country}:{q}"

    # 1Îã®Í≥Ñ: DB/Redis ÌôïÏù∏
    if not force_refresh:
        cached = cache_get(cache_key)
        if cached:
            logger.info(f"Redis cache hit: {cache_key}")
            cached["source"] = "redis"
            return cached

        artist_browse_ids = db_get_artists_by_keyword(q, country)
        if artist_browse_ids:
            logger.info(f"DB hit for keyword: {q} ({len(artist_browse_ids)} artists)")
            artists_data = []
            all_songs = []
            all_albums = []
            needs_background_update = False

            for browse_id in artist_browse_ids:
                artist_full = db_get_full_artist_data(browse_id)
                if artist_full:
                    artists_data.append(artist_full)
                    # Ïù∏Í∏∞Í≥° 5Í∞úÎßå (ÎÇòÎ®∏ÏßÄÎäî YouTube IFrame APIÏóêÏÑú Î°úÎìú)
                    for song in artist_full.get("topSongs", [])[:5]:
                        song["artist_bid"] = browse_id
                        song["resultType"] = "song"
                        all_songs.append(song)
                    for album in artist_full.get("albums", []):
                        album["artist_bid"] = browse_id
                        all_albums.append(album)
                    if db_check_artist_needs_sync(browse_id, days=7):
                        needs_background_update = True
                        background_tasks.add_task(background_update_artist, browse_id, country)

            if artists_data:
                result = {
                    "keyword": q,
                    "country": country,
                    "artists": artists_data,
                    "songs": all_songs,
                    "albums": [],
                    "albums2": all_albums,
                    "allTracks": [t for a in artists_data for t in a.get("allTracks", [])],
                    "source": "database",
                    "updating": needs_background_update
                }
                cache_set(cache_key, result, ttl=1800)
                return result

    # 2Îã®Í≥Ñ: ytmusicapi (Optimized)
    logger.info(f"Fetching from ytmusicapi (Optimized): {q}")
    try:
        ytmusic = get_ytmusic(country)
        
        # Parallel Search (Ïù∏Í∏∞Í≥° 5Í∞úÎßå)
        future_artists = run_in_thread(ytmusic.search, q, filter="artists", limit=5)
        future_songs = run_in_thread(ytmusic.search, q, filter="songs", limit=5)
        artists_results, direct_song_results = await asyncio.gather(future_artists, future_songs)
        
        artists_search = artists_results or []
        if not artists_search:
             general_results = await run_in_thread(ytmusic.search, q, limit=40)
             artists_search = [r for r in general_results if r.get("resultType") == "artist"][:5]

        songs_search = []
        direct_song_results = direct_song_results or []
        for song in direct_song_results:
            if isinstance(song, dict) and song.get("videoId"):
                song_copy = dict(song)
                song_copy["resultType"] = "song"
                song_copy["from_direct_search"] = True
                songs_search.append(song_copy)

        # [ÏàòÏ†ï] Î≥µÏû°Ìïú ÎπÑÍµê Î°úÏßÅ Ï†úÍ±∞ -> Ïú†ÌäúÎ∏å Í≤ÄÏÉâ Í≤∞Í≥º 1ÏàúÏúÑ Ïã†Î¢∞ (Simple is Best)
        # ytmusicapiÎäî Ïù¥ÎØ∏ Í¥ÄÎ†®ÏÑ± ÏàúÏúºÎ°ú Ï†ïÎ†¨Îêú Í≤∞Í≥ºÎ•º Î∞òÌôòÌï®.
        if artists_search:
            best_artist = artists_search[0]
        else:
            best_artist = None

        artists_data = []
        if best_artist and isinstance(best_artist, dict):
            artist_id = best_artist.get("browseId")
            artist_name = best_artist.get("artist") or best_artist.get("name") or ""
            
            # [Î°§Î∞± & ÏïàÏ†ïÌôî] "Ï†ÑÏ≤¥Î≥¥Í∏∞(View All)" Î≥ëÎ†¨ ÏöîÏ≤≠ Ï†úÍ±∞
            # Í≥ºÎèÑÌïú ÎèôÏãú ÏöîÏ≤≠ÏúºÎ°ú Ïù∏Ìïú ÏÑúÎ≤Ñ Î©àÏ∂§ ÌòÑÏÉÅ Ìï¥Í≤∞.
            # ÎåÄÏã† get_artist Í≤∞Í≥ºÏóê Ìè¨Ìï®Îêú Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞(Î≥¥ÌÜµ 10Í∞ú ÎÇ¥Ïô∏)Î•º Ï∂©Ïã§Ìûà Î≥¥Ïó¨Ï§å.
            
            # [ÌïÑÏàò] ÏïÑÌã∞Ïä§Ìä∏ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞ (Ïù¥Í≤å ÏóÜÏúºÎ©¥ Ïï®Î≤î/Í≥° Ï†ïÎ≥¥Í∞Ä ÏóÜÏùå!)
            try:
                artist_detail = await run_in_thread(ytmusic.get_artist, artist_id)
            except Exception as e:
                logger.error(f"Failed to get artist detail: {e}")
                artist_detail = {}
            
            # 1. Ïï®Î≤î Ï†ïÎ≥¥ ÌååÏã±
            albums_list = []
            if artist_detail and "albums" in artist_detail and "results" in artist_detail["albums"]:
                for alb in artist_detail["albums"]["results"]:
                    albums_list.append({
                        "browseId": alb.get("browseId"),
                        "title": alb.get("title"),
                        "thumbnails": alb.get("thumbnails", []),
                        "year": alb.get("year", ""),
                        "artist_bid": artist_id
                    })
            
            # 2. Ïã±Í∏Ä Ï†ïÎ≥¥ ÌååÏã±
            if "singles" in artist_detail and "results" in artist_detail["singles"]:
                for single in artist_detail["singles"]["results"]:
                    albums_list.append({
                        "browseId": single.get("browseId"),
                        "title": single.get("title"),
                        "thumbnails": single.get("thumbnails", []),
                        "year": single.get("year", ""),
                        "type": "Single",
                        "artist_bid": artist_id
                    })

            # 3. Í¥ÄÎ†® ÏïÑÌã∞Ïä§Ìä∏
            related_list = []
            if "related" in artist_detail and "results" in artist_detail["related"]:
                related_list = artist_detail["related"]["results"]

            # 4. Í≥µÏãù Ïù∏Í∏∞Í≥° (Top Songs) + ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ID Ï∂îÏ∂ú
            top_songs = []
            songs_playlist_id = None
            songs_browse_id = None

            if "songs" in artist_detail and isinstance(artist_detail["songs"], dict):
                songs_section = artist_detail["songs"]

                # ÌïµÏã¨: "Î™®Îëê ÌëúÏãú" Î≤ÑÌäºÏùò ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ ID Ï∂îÏ∂ú
                songs_browse_id = songs_section.get("browseId")
                if songs_browse_id and songs_browse_id.startswith("VL"):
                    songs_playlist_id = songs_browse_id[2:]  # "VL" Ï†úÍ±∞
                elif songs_browse_id:
                    songs_playlist_id = songs_browse_id

                # Ïù∏Í∏∞Í≥° 5Í∞úÎßå Ï∂îÏ∂ú (Îπ†Î•∏ ÏùëÎãµ)
                for s in songs_section.get("results", [])[:5]:
                    s["artist_bid"] = artist_id
                    s["resultType"] = "song"
                    top_songs.append(s)

            # ÏùëÎãµ Îç∞Ïù¥ÌÑ∞ Íµ¨ÏÑ± - songsPlaylistIdÎßå Î∞òÌôò (YouTube IFrame APIÏö©)
            artists_data.append({
                "browseId": artist_id,
                "artist": artist_name,
                "name": artist_name,
                "thumbnails": best_artist.get("thumbnails") or [],
                "subscribers": artist_detail.get("subscribers", ""),
                "description": artist_detail.get("description", "")[:200] + "..." if artist_detail.get("description") else "",
                "topSongs": top_songs,
                "related": related_list,
                "albums": albums_list,
                "allTracks": [],
                "songsPlaylistId": songs_playlist_id  # YouTube IFrame APIÏö© ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏ IDÎßå
            })
            
            # Ïù∏Í∏∞Í≥° 5Í∞úÎßå Î∞òÌôò (ÎÇòÎ®∏ÏßÄÎäî YouTube IFrame APIÏóêÏÑú Î°úÎìú)
            songs_search = top_songs[:5]

            # Î∞±Í∑∏ÎùºÏö¥Îìú Ï†ÄÏû• (Ï†ÑÏ≤¥ Ìä∏Îûô FetchÎäî Ïó¨Í∏∞ÏÑú ÏàòÌñâ)
            if artist_id:
                async def save_db_background():
                    try:
                        # Ïù¥ÎØ∏ Í∞ÄÏ†∏Ïò® albums_listÎ•º ÌôúÏö©Ìï† Ïàò ÏûàÎèÑÎ°ù Ìï®Ïàò ÏàòÏ†ïÏù¥ ÌïÑÏöîÌïòÎÇò,
                        # Î≥µÏû°ÏÑ±ÏùÑ ÌîºÌïòÍ∏∞ ÏúÑÌï¥ ÏùºÎã® Í∏∞Ï°¥ Ìï®Ïàò Ìò∏Ï∂ú (ÏÑúÎ≤Ñ Î∂ÄÌïò Î∂ÑÏÇ∞)
                        await save_full_artist_data_background(artist_id, artist_detail, country)
                        db_save_search_keyword(q, country, artist_id)
                    except Exception as e:
                        logger.warning(f"Background save error: {e}")
                
                background_tasks.add_task(save_db_background)
        else:
             # Best ArtistÎ•º Î™ª Ï∞æÏùÄ Í≤ΩÏö∞ (ÎìúÎ¨æ)
             pass

        result = {
            "keyword": q,
            "country": country,
            "artists": artists_data,
            "songs": songs_search,
            "albums": [], 
            "albums2": albums_list if artists_data else [],
            "allTracks": [],
            "source": "ytmusicapi-full-fetch"
        }
        cache_set(cache_key, result, ttl=1800)
        return result

    except Exception as e:
        logger.error(f"Summary search error: {e}")
        raise HTTPException(status_code=500, detail=str(e))



@app.post("/api/provision/artist")
async def provision_artist_agent(request: Request):
    """
    Search artist -> Generate AI Persona -> Save to DB
    """
    try:
        body = await request.json()
        artist_name = body.get("artistName")
        country = body.get("country", "US")
        
        if not artist_name:
            raise HTTPException(status_code=400, detail="artistName required")

        ytmusic = get_ytmusic(country)
        
        # 1. Search Artist
        search_results = await run_in_thread(ytmusic.search, artist_name, filter="artists", limit=1)
        if not search_results:
            raise HTTPException(status_code=404, detail=ERROR_ARTIST_NOT_FOUND)
            
        artist_info = search_results[0]
        browse_id = artist_info.get("browseId")
        name = artist_info.get("artist") or artist_info.get("name")
        
        # 2. Get Details (Description & Songs)
        details = await run_in_thread(ytmusic.get_artist, browse_id)
        description = details.get("description", "")
        songs_list = details.get("songs", {}).get("results", [])
        
        # 3. Generate Persona
        persona = await run_in_thread(generate_artist_persona, name, description, songs_list)
        
        if not persona:
            # Fallback if AI fails
            persona = {
                "system_prompt": f"You are {name}. You are a famous musician.",
                "greeting": f"Hi, I'm {name}!",
                "tone": "Casual",
                "mbti": "Unknown"
            }
            
        # 4. Save to DB (music_artists table)
        # We store persona in a new column or reuse an existing JSON field if schema is rigid.
        # Assuming we can update 'music_artists'
        
        # Ensure artist exists first
        db_save_artist(artist_info)
        
        # Update with persona
        if supabase_client:
             # Check if 'persona' column exists, if not we might fail. 
             # But we can try to store in 'description' or a flexible field if needed.
             # For now, let's assume we can add data. Use raw SQL if needed, but client is safer.
             # Ideally we should have a 'personas' table.
             
             # Create/Update 'artist_personas' table (if exists) or 'music_artists'
             # Let's try to upsert a dedicated 'artist_agents' table if possible,
             # but to be safe without migration, let's return it to frontend to handle chat state locally first.
             pass

        return {
            "status": "success",
            "browseId": browse_id,
            "name": name,
            "persona": persona,
            "avatar": get_best_thumbnail(details.get("thumbnails", []))
        }

    except Exception as e:
        logger.error(f"Provision error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat/artist")
async def chat_artist_endpoint(request: Request):
    """
    Chat with an AI Artist Persona
    """
    try:
        body = await request.json()
        persona = body.get("persona")
        history = body.get("history", []) # List of {role: user/model, content: str}
        message = body.get("message")
        browse_id = body.get("browseId")
        
        if not persona or not message:
            raise HTTPException(status_code=400, detail="Missing persona or message")
            
        # Context gathering (Factual Data)
        artist_context = None
        if browse_id:
            try:
                # Fetch full artist data from DB (songs, albums)
                full_data = await run_in_thread(db_get_full_artist_data, browse_id)
                
                # Fetch REAL-TIME news (Search Grounding)
                # Only if message implies a question about schedule/news/updates to save cost/latency,
                # BUT user complained about accuracy, so we fetch it to be safe or maybe a quick check.
                # Let's fetch it. It might add 1-2s latency but ensures accuracy.
                # Use artist name from persona or DB
                artist_name_for_search = full_data.get("name") if full_data else "the artist"
                news_items = await run_in_thread(search_artist_news, artist_name_for_search)
                
                if full_data:
                    artist_context = {
                        "top_songs": full_data.get("topSongs", []),
                        "albums": full_data.get("albums", []),
                        "news": news_items
                    }
                    
                    # Fallback: If no real news found, try latest album as news
                    if not news_items:
                        albums = full_data.get("albums", [])
                        if albums and len(albums) > 0:
                            latest_album = albums[0]
                            artist_context["news"] = [{
                                "title": f"Latest Music Release: {latest_album.get('title')}",
                                "snippet": f"Check out my latest release '{latest_album.get('title')}' from {latest_album.get('year')}!"
                            }]
            except Exception as context_error:
                logger.warning(f"Failed to fetch artist context for chat: {context_error}")

        reply = await run_in_thread(chat_with_artist, persona, history, message, artist_context)
        return {"reply": reply}
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# Virtual Member System - Artist Auto-Registration
# =============================================================================

@app.post("/api/virtual-members/create")
async def create_virtual_member(request: Request):
    """
    Create a virtual member from an artist in music_artists table.
    This creates a real auth.users entry and profiles entry.
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        body = await request.json()
        browse_id = body.get("browseId")

        if not browse_id:
            raise HTTPException(status_code=400, detail="browseId required")

        # 1. Check if already exists in profiles
        existing = supabase_client.table("profiles").select("id").eq("artist_browse_id", browse_id).execute()
        if existing.data and len(existing.data) > 0:
            return {"status": "exists", "profileId": existing.data[0]["id"]}

        # 2. Get artist info from music_artists
        artist = supabase_client.table("music_artists").select("*").eq("browse_id", browse_id).single().execute()
        if not artist.data:
            raise HTTPException(status_code=404, detail="Artist not found in music_artists")

        artist_data = artist.data
        artist_name = artist_data.get("name", "Unknown Artist")
        thumbnail_url = artist_data.get("thumbnail_url", "")

        # 3. Create auth user via Supabase Admin API
        import uuid
        import httpx

        virtual_email = f"{browse_id}@sori.virtual"
        random_password = str(uuid.uuid4())

        # Supabase Admin API call (using async httpx)
        supabase_url = os.getenv("SUPABASE_URL")
        service_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

        async with httpx.AsyncClient() as client:
            create_user_response = await client.post(
                f"{supabase_url}/auth/v1/admin/users",
                headers={
                    "apikey": service_key,
                    "Authorization": f"Bearer {service_key}",
                    "Content-Type": CONTENT_TYPE_JSON
                },
                json={
                    "email": virtual_email,
                    "password": random_password,
                    "email_confirm": True,
                    "user_metadata": {
                        "full_name": artist_name,
                        "avatar_url": thumbnail_url,
                        "member_type": "artist",
                        "artist_browse_id": browse_id
                    }
                }
            )

        if create_user_response.status_code not in [200, 201]:
            error_detail = create_user_response.json()
            # If user already exists, try to find them
            if "already" in str(error_detail).lower():
                return {"status": "exists", "message": "User already exists"}
            raise HTTPException(status_code=500, detail=f"Failed to create user: {error_detail}")

        user_data = create_user_response.json()
        user_id = user_data.get("id")

        # 4. Update profiles table with artist info
        # (Supabase trigger may have already created a profile, so we update)
        supabase_client.table("profiles").upsert({
            "id": user_id,
            "username": artist_name.lower().replace(" ", "_").replace(".", "")[:30],
            "full_name": artist_name,
            "avatar_url": thumbnail_url,
            "member_type": "artist",
            "artist_browse_id": browse_id,
            "is_verified": True,
            "bio": f"Official SORI profile for {artist_name}"
        }).execute()

        # 5. Generate AI persona
        try:
            top_songs = []
            if artist_data.get("songs_playlist_id"):
                # Fetch top songs for persona generation
                ytmusic = get_ytmusic("US")
                playlist_data = ytmusic.get_playlist(artist_data["songs_playlist_id"])
                top_songs = playlist_data.get("tracks", [])[:10]

            persona = await run_in_thread(
                generate_artist_persona,
                artist_name,
                artist_data.get("description") or f"{artist_name} is a popular music artist.",
                top_songs
            )

            if persona:
                supabase_client.table("profiles").update({
                    "ai_persona": persona
                }).eq("id", user_id).execute()
        except Exception as e:
            logger.warning(f"Persona generation failed: {e}")

        logger.info(f"Virtual member created: {artist_name} ({browse_id})")

        return {
            "status": "created",
            "profileId": user_id,
            "name": artist_name,
            "browseId": browse_id
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Virtual member creation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/virtual-members/migrate-all")
async def migrate_all_artists(request: Request, background_tasks: BackgroundTasks):
    """
    Migrate all existing artists from music_artists to virtual members.
    Runs in background to avoid timeout.
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        # Get all artists that don't have a profile yet
        artists = supabase_client.table("music_artists").select("browse_id, name").execute()

        if not artists.data:
            return {"status": "no_artists", "count": 0}

        # Check which ones already have profiles
        existing = supabase_client.table("profiles").select("artist_browse_id").not_.is_("artist_browse_id", "null").execute()
        existing_ids = set([p["artist_browse_id"] for p in existing.data]) if existing.data else set()

        to_migrate = [a for a in artists.data if a["browse_id"] not in existing_ids]

        async def migrate_artists():
            for artist in to_migrate:
                try:
                    # Call create endpoint internally
                    import aiohttp
                    async with aiohttp.ClientSession() as session:
                        async with session.post(
                            f"http://localhost:{os.getenv('PORT', 8080)}/api/virtual-members/create",
                            json={"browseId": artist["browse_id"]}
                        ) as resp:
                            await resp.json()
                except Exception as e:
                    logger.error(f"Migration failed for {artist['name']}: {e}")
                await asyncio.sleep(0.5)  # Rate limit

        background_tasks.add_task(migrate_artists)

        return {
            "status": "started",
            "total_artists": len(artists.data),
            "already_migrated": len(existing_ids),
            "to_migrate": len(to_migrate)
        }

    except Exception as e:
        logger.error(f"Migration error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/virtual-members/list")
async def list_virtual_members():
    """
    List all virtual members (artists with profiles).
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        result = supabase_client.table("profiles").select(
            "id, username, full_name, avatar_url, artist_browse_id, is_verified, created_at"
        ).eq("member_type", "artist").order("created_at", desc=True).execute()

        return {
            "status": "success",
            "count": len(result.data) if result.data else 0,
            "members": result.data or []
        }
    except Exception as e:
        logger.error(f"List virtual members error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# AI Activity Engine - Automated Artist Activities
# =============================================================================

@app.post("/api/cron/artist-activity")
async def run_artist_activity(request: Request, background_tasks: BackgroundTasks):
    """
    Ïä§ÎßàÌä∏ Ìè¨Ïä§ÌåÖ ÌÅ¨Î°† Ïû° - AIÍ∞Ä ÏïÑÌã∞Ïä§Ìä∏ ÏÉÅÌô©ÏùÑ Î∂ÑÏÑùÌïú ÌõÑ Ìè¨Ïä§ÌåÖ

    Ìè¨Ïä§ÌåÖ Ï†Ñ AIÍ∞Ä Îã§ÏùåÏùÑ Î∂ÑÏÑù:
    1. Í≥†Ïù∏/ÏùÄÌá¥/Ìï¥Ï≤¥ Ïó¨Î∂Ä ‚Üí Ìï¥ÎãπÏãú Ìè¨Ïä§ÌåÖ Ïä§ÌÇµ
    2. ÏµúÍ∑º ÌôúÎèô ÏÉÅÌô© ‚Üí ÎßûÏ∂§ ÏΩòÌÖêÏ∏† ÏÉùÏÑ±
    3. Ïã†Í≥°/Ìà¨Ïñ¥ Ï†ïÎ≥¥ ‚Üí Ïã§Ï†ú Ï†ïÎ≥¥ Í∏∞Î∞ò Ìè¨Ïä§ÌåÖ

    Artists post in their native language.
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        # Configuration - ÏΩúÎìú Ïä§ÌÉÄÌä∏: 50Î™Ö/Ìöå, ÌïòÎ£® 600 Ìè¨Ïä§ÌåÖ
        MAX_POSTS_PER_RUN = 50

        results = {
            "posts_created": 0,
            "stories_created": 0,  # Greetings moved to stories
            "skipped_artists": [],
            "languages_used": [],
            "context_topics": [],
            "errors": []
        }

        # 1. Get random virtual members (artists with profiles)
        artists_result = supabase_client.table("profiles").select(
            "id, username, full_name, avatar_url, artist_browse_id, ai_persona"
        ).eq("member_type", "artist").execute()

        if not artists_result.data or len(artists_result.data) == 0:
            return {"status": "no_artists", "message": "No virtual members found"}

        artists = artists_result.data
        random.shuffle(artists)

        posts_created = 0
        artist_index = 0

        # 2. Process artists until we have enough posts
        while posts_created < MAX_POSTS_PER_RUN and artist_index < len(artists):
            artist = artists[artist_index]
            artist_index += 1

            try:
                persona = artist.get("ai_persona") or {}
                artist_name = artist.get("full_name") or artist.get("username")
                browse_id = artist.get("artist_browse_id")

                # ========== STEP 1: GATHER ARTIST CONTEXT ==========
                # AIÍ∞Ä Ìè¨Ïä§ÌåÖ Ï†Ñ ÏïÑÌã∞Ïä§Ìä∏ ÏÉÅÌô©ÏùÑ Ï¢ÖÌï©Ï†ÅÏúºÎ°ú Î∂ÑÏÑù
                context = await run_in_thread(gather_artist_context, artist_name)
                logger.info(f"[CONTEXT] {artist_name}: status={context.get('status')}, can_post={context.get('can_post')}, topic={context.get('suggested_topic')}")

                # Í≥†Ïù∏/ÏùÄÌá¥/Ìï¥Ï≤¥ ÏïÑÌã∞Ïä§Ìä∏Îäî Ìè¨Ïä§ÌåÖ Ïä§ÌÇµ
                if not context.get("can_post", True):
                    skip_reason = context.get("skip_reason", "Unknown")
                    results["skipped_artists"].append({
                        "artist": artist_name,
                        "reason": skip_reason,
                        "status": context.get("status")
                    })
                    logger.warning(f"[SKIP] {artist_name}: {skip_reason}")
                    continue  # Îã§Ïùå ÏïÑÌã∞Ïä§Ìä∏Î°ú

                # ========== STEP 2: GET ARTIST LANGUAGE ==========
                artist_language = "en"
                if browse_id:
                    music_artist = supabase_client.table("music_artists").select(
                        "primary_language"
                    ).eq("browse_id", browse_id).limit(1).execute()
                    if music_artist.data:
                        artist_language = music_artist.data[0].get("primary_language") or "en"

                # ========== STEP 3: GET IMAGE & VIDEO FROM REAL DATA ==========
                # Default to avatar (upscaled to high resolution)
                cover_url = upscale_thumbnail_url(artist.get("avatar_url"), size=544)
                video_id = None  # YouTube video ID for iframe embed
                song_title = None  # Track title for recommendation posts

                if browse_id:
                    # Get latest album with thumbnail
                    albums = supabase_client.table("music_albums").select(
                        "title, thumbnail_url, year"
                    ).eq("artist_browse_id", browse_id).order(
                        "year", desc=True
                    ).limit(3).execute()

                    # Get popular tracks WITH video_id for YouTube embed
                    tracks = supabase_client.table("music_tracks").select(
                        "title, thumbnail_url, video_id"
                    ).eq("artist_browse_id", browse_id).limit(10).execute()

                    # Select a random track for video embed
                    if tracks.data and len(tracks.data) > 0:
                        tracks_with_video = [t for t in tracks.data if t.get("video_id")]
                        if tracks_with_video:
                            selected_track = random.choice(tracks_with_video)
                            video_id = selected_track.get("video_id")
                            song_title = selected_track.get("title")
                            if selected_track.get("thumbnail_url"):
                                cover_url = upscale_thumbnail_url(selected_track["thumbnail_url"], size=544)
                            logger.info(f"Selected track for {artist_name}: {song_title} (video_id: {video_id})")

                    # Use album art if no track selected yet
                    if not video_id and albums.data and len(albums.data) > 0:
                        album = random.choice(albums.data)
                        if album.get("thumbnail_url"):
                            cover_url = upscale_thumbnail_url(album["thumbnail_url"], size=544)
                            logger.info(f"Using real album art for {artist_name}: {album.get('title')}")

                # ========== STEP 4: GENERATE CONTEXTUAL POST ==========
                # AIÍ∞Ä ÏïÑÌã∞Ïä§Ìä∏ ÏÉÅÌô©Ïóê ÎßûÎäî Ìè¨Ïä§Ìä∏ ÏÉùÏÑ±
                # Add selected track info to context for AI
                if song_title and video_id:
                    context["selected_track"] = {
                        "title": song_title,
                        "video_id": video_id
                    }
                    # If no specific news, suggest music recommendation post
                    if context.get("suggested_topic") == "fan_thanks":
                        context["suggested_topic"] = "music_recommendation"
                        context["post_context"] = f"Recommend your song '{song_title}' to fans. Share why you love this track."

                # ========== QUALITY CONTROL: fan_thanks ‚Üí Story only ==========
                # Ïù∏ÏÇ¨Í∏ÄÏùÄ Ïä§ÌÜ†Î¶¨Î°úÎßå, Ìè¨Ïä§ÌåÖÏóêÎäî Ï†àÎåÄ Ïù∏ÏÇ¨Í∏Ä Í∏àÏßÄ
                suggested_topic = context.get("suggested_topic", "fan_thanks")
                if suggested_topic == "fan_thanks" and not video_id:
                    # Create a STORY instead of a post for greetings
                    try:
                        expires_at = (datetime.now(timezone.utc) + timedelta(hours=24)).isoformat()
                        fandom_name = persona.get("fandom_name", "fans") if persona else "fans"

                        # Greeting messages by language
                        greetings = {
                            "ko": f"üíï ÏÇ¨ÎûëÌï¥Ïöî {fandom_name}!",
                            "ja": f"üíï ÊÑõ„Åó„Å¶„Çã {fandom_name}!",
                            "zh": f"üíï Áà±‰Ω†‰ª¨ {fandom_name}!",
                            "es": f"üíï ¬°Os quiero {fandom_name}!",
                        }
                        greeting_text = greetings.get(artist_language, f"üíï Love you {fandom_name}!")

                        story_data = {
                            "user_id": artist["id"],
                            "content_type": "greeting",
                            "text_content": greeting_text,
                            "cover_url": cover_url,
                            "background_color": "#1a1a2e",
                            "expires_at": expires_at,
                            "is_active": True
                        }
                        supabase_client.table("stories").insert(story_data).execute()
                        results["stories_created"] += 1
                        logger.info(f"[STORY] {artist_name}: Greeting ‚Üí Story (not post)")
                    except Exception as story_err:
                        logger.error(f"Story creation error: {story_err}")
                    continue  # Skip post, move to next artist

                post_data = await run_in_thread(
                    generate_contextual_post,
                    artist_name,
                    persona,
                    artist_language,
                    context
                )

                if post_data and post_data.get("caption"):
                    post_type = post_data.get("type", "fan")
                    caption = post_data["caption"]
                    context_topic = post_data.get("context_used", "fan_thanks")

                    # Insert post with REAL image AND YouTube video_id
                    new_post = {
                        "user_id": artist["id"],
                        "caption": caption,
                        "language": artist_language,
                        "cover_url": cover_url,
                        "video_id": video_id,  # YouTube video ID for iframe embed
                        "title": song_title or post_type,  # Use song title if available
                        "artist": artist_name,
                        "is_public": True,
                        "like_count": 0,
                        "comment_count": 0,
                        "repost_count": 0
                    }

                    supabase_client.table("posts").insert(new_post).execute()
                    posts_created += 1
                    results["posts_created"] = posts_created
                    results["languages_used"].append(artist_language)
                    results["context_topics"].append(context_topic)
                    logger.info(f"[POST] {artist_name} ({artist_language}, topic={context_topic}): {caption[:50]}...")

            except Exception as e:
                results["errors"].append(f"Post error for {artist.get('full_name')}: {str(e)}")
                logger.error(f"Post creation error: {e}")

        return {
            "status": "success",
            "results": results
        }

    except Exception as e:
        logger.error(f"Artist activity cron error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# YouTube Charts ÏïÑÌã∞Ïä§Ìä∏ ÏûêÎèô ÏàòÏßë Cron Job
# =============================================================================

# ÏßÄÏõê Íµ≠Í∞Ä Î™©Î°ù (YouTube Music Charts)
# YouTube Music ÏßÄÏõê Íµ≠Í∞Ä Ï†ÑÏ≤¥ Î™©Î°ù (100+ countries)
CHART_COUNTRIES = [
    # Global
    "ZZ",
    # Americas (Î∂ÅÎØ∏/Ï§ëÎÇ®ÎØ∏)
    "US", "CA", "MX", "BR", "AR", "CL", "CO", "PE", "VE", "EC",
    "BO", "PY", "UY", "CR", "PA", "GT", "HN", "SV", "NI", "DO",
    "PR", "JM", "TT", "BB", "CU",
    # Europe (Ïú†ÎüΩ)
    "GB", "DE", "FR", "ES", "IT", "NL", "PL", "RU", "TR", "SE",
    "NO", "DK", "FI", "BE", "AT", "CH", "PT", "IE", "GR", "CZ",
    "HU", "RO", "UA", "BG", "HR", "SK", "SI", "LT", "LV", "EE",
    "MT", "CY", "LU", "IS", "RS", "BA", "ME", "MK", "AL", "BY",
    # Middle East (Ï§ëÎèô)
    "SA", "AE", "EG", "IL", "JO", "LB", "KW", "QA", "BH", "OM",
    "IQ", "IR",
    # Africa (ÏïÑÌîÑÎ¶¨Ïπ¥)
    "ZA", "NG", "KE", "TZ", "UG", "ZW", "GH", "MA", "TN", "DZ",
    "CI", "SN", "CM", "ET", "AO", "MZ",
    # Asia (ÏïÑÏãúÏïÑ)
    "KR", "JP", "IN", "ID", "TH", "VN", "PH", "TW", "SG", "MY",
    "HK", "BD", "PK", "LK", "NP", "MM", "KH", "LA", "MN", "CN",
    # Oceania (Ïò§ÏÑ∏ÏïÑÎãàÏïÑ)
    "AU", "NZ", "FJ", "PG"
]


@app.post("/api/cron/collect-chart-artists")
async def collect_chart_artists(request: Request, background_tasks: BackgroundTasks):
    """
    YouTube ChartsÏóêÏÑú Íµ≠Í∞ÄÎ≥Ñ Top ÏïÑÌã∞Ïä§Ìä∏Î•º ÏàòÏßëÌïòÏó¨ DBÏóê Ï†ÄÏû•

    60Í∞úÍµ≠ √ó ÏïΩ 20Î™Ö(Top Artists) = ÏïΩ 1,200Î™Ö
    Ï§ëÎ≥µ Ï†úÍ±∞ ÌõÑ ÏïΩ 500~800Î™ÖÏùò Ïú†ÎãàÌÅ¨ ÏïÑÌã∞Ïä§Ìä∏

    Ìò∏Ï∂ú Î∞©Î≤ï:
    - POST /api/cron/collect-chart-artists
    - body: {"countries": ["KR", "US"]} (ÏÑ†ÌÉù, Í∏∞Î≥∏Í∞íÏùÄ Î™®Îì† Íµ≠Í∞Ä)
    - body: {"countries": ["KR"], "limit": 10} (Íµ≠Í∞ÄÎãπ ÏàòÏßëÌï† ÏïÑÌã∞Ïä§Ìä∏ Ïàò)
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        body = {}
        try:
            body = await request.json()
        except (json.JSONDecodeError, ValueError):
            pass

        # ÏàòÏßëÌï† Íµ≠Í∞Ä Î™©Î°ù (Í∏∞Î≥∏: Ï†ÑÏ≤¥)
        countries = body.get("countries", CHART_COUNTRIES)
        # Íµ≠Í∞ÄÎãπ ÏàòÏßëÌï† ÏïÑÌã∞Ïä§Ìä∏ Ïàò (Í∏∞Î≥∏: 50Î™Ö)
        artist_limit = body.get("limit", 50)
        # Íµ≠Í∞Ä Í∞Ñ ÎîúÎ†àÏù¥ (Ï¥à)
        delay_seconds = body.get("delay", 2)

        results = {
            "countries_processed": 0,
            "artists_found": 0,
            "artists_saved": 0,
            "artists_skipped": 0,
            "errors": []
        }

        all_artist_ids = set()  # Ï§ëÎ≥µ Ï≤¥ÌÅ¨Ïö©

        for country in countries:
            try:
                logger.info(f"[CHART] Processing country: {country}")

                # 1. Ï∞®Ìä∏ Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞
                ytmusic = get_ytmusic(country)
                charts = ytmusic.get_charts(country=country)

                if not charts or "artists" not in charts:
                    logger.warning(f"No artists in {country} charts")
                    continue

                artists = charts.get("artists", [])[:artist_limit]
                results["countries_processed"] += 1

                # 2. Í∞Å ÏïÑÌã∞Ïä§Ìä∏ Ï≤òÎ¶¨
                for artist in artists:
                    browse_id = artist.get("browseId")
                    artist_name = artist.get("title", "Unknown")

                    if not browse_id:
                        continue

                    results["artists_found"] += 1

                    # Ï§ëÎ≥µ Ï≤¥ÌÅ¨
                    if browse_id in all_artist_ids:
                        results["artists_skipped"] += 1
                        continue
                    all_artist_ids.add(browse_id)

                    # DBÏóê Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäîÏßÄ Ï≤¥ÌÅ¨
                    existing = supabase_client.table("music_artists").select(
                        "browse_id"
                    ).eq("browse_id", browse_id).limit(1).execute()

                    if existing.data and len(existing.data) > 0:
                        results["artists_skipped"] += 1
                        logger.info(f"[SKIP] Already exists: {artist_name}")
                        continue

                    # 3. ÏïÑÌã∞Ïä§Ìä∏ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ Í∞ÄÏ†∏Ïò§Í∏∞ & Ï†ÄÏû•
                    try:
                        artist_info = ytmusic.get_artist(browse_id)
                        if artist_info:
                            # Î∞±Í∑∏ÎùºÏö¥ÎìúÎ°ú Ï†ÄÏû• (Ïï®Î≤î, Ìä∏Îûô Ìè¨Ìï®)
                            background_tasks.add_task(
                                save_full_artist_data_background,
                                browse_id,
                                artist_info,
                                country
                            )
                            results["artists_saved"] += 1
                            logger.info(f"[SAVE] {artist_name} ({country})")

                            # Rate limiting: ÏïÑÌã∞Ïä§Ìä∏ÎßàÎã§ 0.5Ï¥à ÎåÄÍ∏∞
                            await asyncio.sleep(0.5)
                    except Exception as e:
                        logger.error(f"Error fetching artist {artist_name}: {e}")
                        results["errors"].append(f"{artist_name}: {str(e)}")

                # Íµ≠Í∞Ä Í∞Ñ ÎîúÎ†àÏù¥
                await asyncio.sleep(delay_seconds)

            except Exception as e:
                logger.error(f"Error processing country {country}: {e}")
                results["errors"].append(f"{country}: {str(e)}")

        return {
            "status": "success",
            "results": results
        }

    except Exception as e:
        logger.error(f"Chart collection error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/cron/collect-chart-artists-batch")
async def collect_chart_artists_batch(request: Request, background_tasks: BackgroundTasks):
    """
    Íµ≠Í∞ÄÎ•º Î∞∞ÏπòÎ°ú ÎÇòÎà†ÏÑú ÏàòÏßë (Rate Limiting Î∞©ÏßÄ)

    Vercel CronÏóêÏÑú 6ÏãúÍ∞ÑÎßàÎã§ Ìò∏Ï∂ú:
    - batch=0: 0~14Î≤à Íµ≠Í∞Ä (15Í∞úÍµ≠)
    - batch=1: 15~29Î≤à Íµ≠Í∞Ä
    - batch=2: 30~44Î≤à Íµ≠Í∞Ä
    - batch=3: 45~59Î≤à Íµ≠Í∞Ä

    4Ïùº = 1 ÏÇ¨Ïù¥ÌÅ¥ (60Í∞úÍµ≠ Ï†ÑÏ≤¥)
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        body = {}
        try:
            body = await request.json()
        except (json.JSONDecodeError, ValueError):
            pass

        # Î∞∞Ïπò Î≤àÌò∏ (0~3)
        batch = body.get("batch", 0)
        batch_size = 15

        start_idx = batch * batch_size
        end_idx = min(start_idx + batch_size, len(CHART_COUNTRIES))

        if start_idx >= len(CHART_COUNTRIES):
            return {"status": "no_more_batches", "batch": batch}

        countries_to_process = CHART_COUNTRIES[start_idx:end_idx]

        logger.info(f"[BATCH {batch}] Processing countries: {countries_to_process}")

        # ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú collect_chart_artists Î°úÏßÅ Ïã§Ìñâ
        results = {
            "batch": batch,
            "countries": countries_to_process,
            "artists_found": 0,
            "artists_saved": 0,
            "artists_skipped": 0,
            "errors": []
        }

        all_artist_ids = set()

        for country in countries_to_process:
            try:
                ytmusic = get_ytmusic(country)
                charts = ytmusic.get_charts(country=country)

                if not charts or "artists" not in charts:
                    continue

                artists = charts.get("artists", [])[:30]  # Íµ≠Í∞ÄÎãπ 30Î™Ö

                for artist in artists:
                    browse_id = artist.get("browseId")
                    artist_name = artist.get("title", "Unknown")

                    if not browse_id or browse_id in all_artist_ids:
                        continue

                    all_artist_ids.add(browse_id)
                    results["artists_found"] += 1

                    # DB Ï≤¥ÌÅ¨
                    existing = supabase_client.table("music_artists").select(
                        "browse_id"
                    ).eq("browse_id", browse_id).limit(1).execute()

                    if existing.data and len(existing.data) > 0:
                        results["artists_skipped"] += 1
                        continue

                    try:
                        artist_info = ytmusic.get_artist(browse_id)
                        if artist_info:
                            # ÎèôÍ∏∞ Ï†ÄÏû• (Cloud RunÏóêÏÑú Î∞±Í∑∏ÎùºÏö¥Îìú ÏûëÏóÖÏù¥ Ï§ëÎã®ÎêòÎäî Î¨∏Ï†ú Ìï¥Í≤∞)
                            save_full_artist_data_background(browse_id, artist_info, country)
                            results["artists_saved"] += 1
                            logger.info(f"[BATCH {batch}] Saved to DB: {artist_name}")
                            await asyncio.sleep(0.3)
                    except Exception as e:
                        logger.error(f"[BATCH {batch}] Save error: {artist_name} - {e}")
                        results["errors"].append(f"{artist_name}: {str(e)}")

                await asyncio.sleep(2)  # Íµ≠Í∞Ä Í∞Ñ ÎîúÎ†àÏù¥

            except Exception as e:
                results["errors"].append(f"{country}: {str(e)}")

        return {
            "status": "success",
            "results": results,
            "next_batch": batch + 1 if end_idx < len(CHART_COUNTRIES) else 0
        }

    except Exception as e:
        logger.error(f"Batch collection error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Í∏∞Ï°¥ ÏïÑÌã∞Ïä§Ìä∏ Îç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏ Cron Job
# =============================================================================

@app.post("/api/cron/update-existing-artists")
async def update_existing_artists(request: Request):
    """
    Ïä§ÎßàÌä∏ Ïö∞ÏÑ†ÏàúÏúÑ Í∏∞Î∞ò ÏïÑÌã∞Ïä§Ìä∏ ÏóÖÎç∞Ïù¥Ìä∏

    Ïö∞ÏÑ†ÏàúÏúÑ Ìã∞Ïñ¥:
    - HOT: last_viewed_at 7Ïùº Ïù¥ÎÇ¥ + last_synced_at 7Ïùº Ï¥àÍ≥º ‚Üí Ï¶âÏãú ÏóÖÎç∞Ïù¥Ìä∏
    - ACTIVE: last_viewed_at 30Ïùº Ïù¥ÎÇ¥ + last_synced_at 30Ïùº Ï¥àÍ≥º ‚Üí Îã§Ïùå Ïö∞ÏÑ†
    - DORMANT: Í∑∏ Ïô∏ ‚Üí On-DemandÏóêÏÑúÎßå Ï≤òÎ¶¨ (Ïó¨Í∏∞ÏÑú Ïä§ÌÇµ)

    Î∞∞Ïπò ÌÅ¨Í∏∞: 200Î™Ö (Í∏∞Ï°¥ 20Î™ÖÏóêÏÑú 10Î∞∞ Ï¶ùÍ∞Ä)
    """
    try:
        body = await request.json()
    except (json.JSONDecodeError, ValueError):
        body = {}

    secret = body.get("secret", "")
    if secret != os.getenv("CRON_SECRET", "dev-secret"):
        raise HTTPException(status_code=401, detail="Unauthorized")

    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_AVAILABLE)

    results = {
        "hot_updated": 0,
        "active_updated": 0,
        "artists_skipped": 0,
        "errors": []
    }

    try:
        # Î∞∞Ïπò ÌÅ¨Í∏∞ (1ÌöåÎãπ ÏóÖÎç∞Ïù¥Ìä∏Ìï† ÏïÑÌã∞Ïä§Ìä∏ Ïàò) - 10Î∞∞ Ï¶ùÍ∞Ä
        batch_size = body.get("limit", 200)
        now = datetime.now(timezone.utc)

        # ÏãúÍ∞Ñ Í∏∞Ï§Ä
        seven_days_ago = (now - timedelta(days=7)).isoformat()
        thirty_days_ago = (now - timedelta(days=30)).isoformat()

        artists_to_update = []

        # 1Îã®Í≥Ñ: HOT Ìã∞Ïñ¥ - ÏµúÍ∑º 7Ïùº ÎÇ¥ Ï°∞ÌöåÎê® + 7Ïùº Ïù¥ÏÉÅ ÎØ∏ÎèôÍ∏∞Ìôî
        hot_artists = supabase_client.table("music_artists").select(
            "browse_id, name, primary_language, last_synced_at, last_viewed_at"
        ).gte("last_viewed_at", seven_days_ago).lt(
            "last_synced_at", seven_days_ago
        ).order("last_viewed_at", desc=True).limit(batch_size).execute()

        if hot_artists.data:
            for artist in hot_artists.data:
                artist["tier"] = "HOT"
                artists_to_update.append(artist)

        logger.info(f"[UPDATE] HOT tier: {len(hot_artists.data or [])} artists")

        # 2Îã®Í≥Ñ: ACTIVE Ìã∞Ïñ¥ - Î∞∞Ïπò Ïó¨Ïú†Î∂Ñ Ï±ÑÏö∞Í∏∞
        if len(artists_to_update) < batch_size:
            remaining = batch_size - len(artists_to_update)
            existing_ids = [a["browse_id"] for a in artists_to_update]

            active_artists = supabase_client.table("music_artists").select(
                "browse_id, name, primary_language, last_synced_at, last_viewed_at"
            ).gte("last_viewed_at", thirty_days_ago).lt(
                "last_viewed_at", seven_days_ago
            ).lt("last_synced_at", thirty_days_ago).order(
                "last_viewed_at", desc=True
            ).limit(remaining).execute()

            if active_artists.data:
                for artist in active_artists.data:
                    if artist["browse_id"] not in existing_ids:
                        artist["tier"] = "ACTIVE"
                        artists_to_update.append(artist)

            logger.info(f"[UPDATE] ACTIVE tier: {len(active_artists.data or [])} artists")

        # 3Îã®Í≥Ñ: DORMANTÎäî Ïä§ÌÇµ (On-DemandÏóêÏÑú Ï≤òÎ¶¨)
        logger.info(f"[UPDATE] Total to update: {len(artists_to_update)} artists (DORMANT skipped)")

        for artist in artists_to_update:
            browse_id = artist["browse_id"]
            artist_name = artist["name"]
            tier = artist.get("tier", "UNKNOWN")
            country = artist.get("primary_language", "US")

            # Ïñ∏Ïñ¥ ÏΩîÎìúÎ•º Íµ≠Í∞Ä ÏΩîÎìúÎ°ú Îß§Ìïë
            lang_to_country = {
                "ko": "KR", "ja": "JP", "zh": "TW", "es": "ES",
                "pt": "BR", "de": "DE", "fr": "FR", "it": "IT",
                "ru": "RU", "th": "TH", "vi": "VN", "id": "ID",
                "hi": "IN", "ar": "SA", "tr": "TR", "en": "US"
            }
            country_code = lang_to_country.get(country, "US")

            try:
                ytmusic = get_ytmusic(country_code)
                artist_info = ytmusic.get_artist(browse_id)

                if artist_info:
                    # ÎèôÍ∏∞ Ï†ÄÏû• (Îç∞Ïù¥ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏)
                    save_full_artist_data_background(browse_id, artist_info, country_code)

                    # Ìã∞Ïñ¥Î≥Ñ Ïπ¥Ïö¥Ìä∏
                    if tier == "HOT":
                        results["hot_updated"] += 1
                    else:
                        results["active_updated"] += 1

                    logger.info(f"[UPDATE-{tier}] Updated: {artist_name}")
                    await asyncio.sleep(0.3)  # Rate limiting Í∞úÏÑ† (0.5 ‚Üí 0.3)
                else:
                    results["artists_skipped"] += 1
                    logger.warning(f"[UPDATE] No data for: {artist_name}")

            except Exception as e:
                results["errors"].append(f"{artist_name}: {str(e)}")
                logger.error(f"[UPDATE] Error updating {artist_name}: {e}")

            await asyncio.sleep(0.5)  # Rate limiting (1Ï¥à ‚Üí 0.5Ï¥àÎ°ú Îã®Ï∂ï)

        total_updated = results["hot_updated"] + results["active_updated"]
        return {
            "status": "success",
            "results": results,
            "message": f"Updated {total_updated} artists (HOT: {results['hot_updated']}, ACTIVE: {results['active_updated']})"
        }

    except Exception as e:
        logger.error(f"Update existing artists error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Í∞ÄÏÉÅÌöåÏõê ÏûêÎèô ÏÉùÏÑ± Cron Job (30Î∂ÑÎßàÎã§)
# =============================================================================

@app.post("/api/cron/create-virtual-members")
async def create_virtual_members_cron(request: Request):
    """
    music_artistsÏóê ÏûàÏßÄÎßå profilesÏóê ÏóÜÎäî ÏïÑÌã∞Ïä§Ìä∏Î•º Í∞ÄÏÉÅÌöåÏõêÏúºÎ°ú ÏÉùÏÑ±

    Îß§ 30Î∂ÑÎßàÎã§ Ïã§Ìñâ:
    - music_artists ÌÖåÏù¥Î∏îÏóêÏÑú artist_browse_idÍ∞Ä profilesÏóê ÏóÜÎäî ÏïÑÌã∞Ïä§Ìä∏ Í≤ÄÏÉâ
    - Î∞∞ÏπòÎãπ ÏµúÎåÄ 50Î™Ö ÏÉùÏÑ± (Rate limiting)
    - ÌåîÎ°úÏõå ÏàòÍ∞Ä ÎßéÏùÄ ÏïÑÌã∞Ïä§Ìä∏ Ïö∞ÏÑ† ÏÉùÏÑ±
    """
    try:
        body = await request.json()
    except (json.JSONDecodeError, ValueError):
        body = {}

    secret = body.get("secret", "")
    if secret != os.getenv("CRON_SECRET", "dev-secret"):
        raise HTTPException(status_code=401, detail="Unauthorized")

    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_AVAILABLE)

    results = {
        "created": 0,
        "skipped": 0,
        "errors": [],
        "created_artists": []
    }

    try:
        batch_size = body.get("limit", 50)

        # 1. music_artistsÏóêÏÑú Î™®Îì† browse_id Í∞ÄÏ†∏Ïò§Í∏∞
        all_artists = supabase_client.table("music_artists").select(
            "browse_id, name, thumbnail_url, subscribers, subscribers_count"
        ).not_.is_("browse_id", "null").order(
            "subscribers_count", desc=True  # Íµ¨ÎèÖÏûê ÎßéÏùÄ Ïàú (Ïà´Ïûê Ï†ïÎ†¨)
        ).limit(500).execute()

        if not all_artists.data:
            return {
                "status": "no_artists",
                "message": "No artists in music_artists table",
                "results": results
            }

        # 2. profilesÏóêÏÑú Ïù¥ÎØ∏ Í∞ÄÏÉÅÌöåÏõêÏù∏ artist_browse_id Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞
        existing_profiles = supabase_client.table("profiles").select(
            "artist_browse_id"
        ).eq("member_type", "artist").not_.is_("artist_browse_id", "null").execute()

        existing_browse_ids = set()
        if existing_profiles.data:
            existing_browse_ids = {p["artist_browse_id"] for p in existing_profiles.data}

        # 3. ÏïÑÏßÅ Í∞ÄÏÉÅÌöåÏõêÏù¥ ÏïÑÎãå ÏïÑÌã∞Ïä§Ìä∏ ÌïÑÌÑ∞ÎßÅ
        artists_to_create = []
        for artist in all_artists.data:
            if artist["browse_id"] not in existing_browse_ids:
                artists_to_create.append(artist)
                if len(artists_to_create) >= batch_size:
                    break

        logger.info(f"[VIRTUAL MEMBER] Found {len(artists_to_create)} artists to create (out of {len(all_artists.data)} total)")

        # 4. Í∞ÄÏÉÅÌöåÏõê ÏÉùÏÑ±
        for artist in artists_to_create:
            browse_id = artist["browse_id"]
            artist_name = artist["name"]
            thumbnail_url = artist.get("thumbnail_url", "")

            try:
                result = create_virtual_member_sync(browse_id, artist_name, thumbnail_url)
                if result:
                    results["created"] += 1
                    results["created_artists"].append(artist_name)
                    logger.info(f"[VIRTUAL MEMBER] Created: {artist_name}")
                else:
                    results["skipped"] += 1
            except Exception as e:
                results["errors"].append(f"{artist_name}: {str(e)}")
                logger.warning(f"[VIRTUAL MEMBER] Error creating {artist_name}: {e}")

            # Rate limiting: 0.5Ï¥à Í∞ÑÍ≤©
            await asyncio.sleep(0.5)

        logger.info(f"[VIRTUAL MEMBER] Completed: {results['created']} created, {results['skipped']} skipped")

        return {
            "status": "success",
            "message": f"Created {results['created']} virtual members",
            "results": results
        }

    except Exception as e:
        logger.error(f"Create virtual members error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# ÌîºÎùºÎØ∏ÎìúÏãù Related Artists ÌôïÏû• Cron Job
# =============================================================================

@app.post("/api/cron/expand-related-artists")
async def expand_related_artists(request: Request):
    """
    ÌîºÎùºÎØ∏ÎìúÏãù Ïú†ÏÇ¨ ÏïÑÌã∞Ïä§Ìä∏ ÌôïÏû•
    - Í∏∞Ï°¥ ÏïÑÌã∞Ïä§Ìä∏Ïùò related_artists_jsonÏóêÏÑú ÏÉà ÏïÑÌã∞Ïä§Ìä∏ Î∞úÍµ¥
    - Ï§ëÎ≥µ Ï†úÍ±∞, Rate limiting Ï†ÅÏö©
    - ÌïòÎ£® ÏµúÎåÄ 200Î™Ö Ï†úÌïú
    """
    try:
        body = await request.json()
    except (json.JSONDecodeError, ValueError):
        body = {}

    secret = body.get("secret", "")
    if secret != os.getenv("CRON_SECRET", "dev-secret"):
        raise HTTPException(status_code=401, detail="Unauthorized")

    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_AVAILABLE)

    results = {
        "artists_discovered": 0,
        "artists_registered": 0,
        "artists_skipped": 0,
        "errors": []
    }

    try:
        # Î∞∞Ïπò ÌÅ¨Í∏∞ (1ÌöåÎãπ Ï≤òÎ¶¨Ìï† Í∏∞Ï°¥ ÏïÑÌã∞Ïä§Ìä∏ Ïàò)
        batch_size = body.get("limit", 20)
        # Í∏∞Ï°¥ ÏïÑÌã∞Ïä§Ìä∏Îãπ Ï≤òÎ¶¨Ìï† related Ïàò
        related_limit = body.get("related_limit", 10)

        # ÏµúÍ∑º Îì±Î°ùÎêú ÏïÑÌã∞Ïä§Ìä∏Ïùò related_artists_json Ï°∞Ìöå
        artists_with_related = supabase_client.table("music_artists").select(
            "browse_id, name, related_artists_json, primary_language"
        ).not_.is_("related_artists_json", "null").order(
            "created_at", desc=True
        ).limit(batch_size).execute()

        if not artists_with_related.data:
            return {"status": "no_artists_with_related", "results": results}

        # Ïù¥ÎØ∏ Îì±Î°ùÎêú Î™®Îì† browse_id Ï°∞Ìöå (Ï§ëÎ≥µ Ï≤¥ÌÅ¨Ïö©)
        existing_artists = supabase_client.table("music_artists").select(
            "browse_id"
        ).execute()
        existing_ids = set(a["browse_id"] for a in existing_artists.data) if existing_artists.data else set()

        logger.info(f"[EXPAND] Processing {len(artists_with_related.data)} artists, {len(existing_ids)} already exist")

        new_artist_ids = []

        # Í∞Å ÏïÑÌã∞Ïä§Ìä∏Ïùò related ÏàòÏßë
        for artist in artists_with_related.data:
            related_list = artist.get("related_artists_json") or []

            if not isinstance(related_list, list):
                continue

            for related in related_list[:related_limit]:
                if not isinstance(related, dict):
                    continue

                browse_id = related.get("browseId")
                if not browse_id or browse_id in existing_ids:
                    results["artists_skipped"] += 1
                    continue

                # Ïù¥ÎØ∏ Ïù¥Î≤à Î∞∞ÏπòÏóêÏÑú Ï∂îÍ∞ÄÌïú Í≤ΩÏö∞ Ïä§ÌÇµ
                if browse_id in new_artist_ids:
                    continue

                new_artist_ids.append(browse_id)
                results["artists_discovered"] += 1

                # ÏùºÏùº ÏµúÎåÄ Ï†úÌïú (50Î™Ö)
                if results["artists_discovered"] >= 50:
                    break

            if results["artists_discovered"] >= 50:
                break

        logger.info(f"[EXPAND] Discovered {len(new_artist_ids)} new artists to register")

        # ÏÉà ÏïÑÌã∞Ïä§Ìä∏ Îì±Î°ù
        for browse_id in new_artist_ids:
            try:
                # Rate limiting
                await asyncio.sleep(1)

                ytmusic = get_ytmusic("US")
                artist_info = ytmusic.get_artist(browse_id)

                if artist_info:
                    # ÎèôÍ∏∞ Ï†ÄÏû•
                    save_full_artist_data_background(browse_id, artist_info, "US")
                    results["artists_registered"] += 1
                    artist_name = artist_info.get("name", "Unknown")
                    logger.info(f"[EXPAND] Registered: {artist_name}")
                else:
                    results["artists_skipped"] += 1

            except Exception as e:
                results["errors"].append(f"{browse_id}: {str(e)}")
                logger.error(f"[EXPAND] Error registering {browse_id}: {e}")

        return {
            "status": "success",
            "results": results,
            "message": f"Discovered {results['artists_discovered']}, Registered {results['artists_registered']}"
        }

    except Exception as e:
        logger.error(f"Expand related artists error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/admin/check-artist-status")
async def check_artist_status_endpoint(name: str):
    """
    AIÎ°ú ÏïÑÌã∞Ïä§Ìä∏ ÏÉÅÌÉú ÌôïÏù∏ (Í≥†Ïù∏ Ïó¨Î∂Ä, ÌôúÎèô ÏÉÅÌÉú Îì±)

    Example: /api/admin/check-artist-status?name=Î∞ïÏÉÅÍ∑ú
    """
    if not name:
        raise HTTPException(status_code=400, detail="name parameter required")

    result = await run_in_thread(check_artist_status, name)
    return {
        "artist": name,
        "result": result
    }


@app.delete("/api/admin/delete-virtual-member-posts")
async def delete_virtual_member_posts():
    """
    Í∞ÄÏÉÅÌöåÏõê(ÏïÑÌã∞Ïä§Ìä∏)Îì§Ïù¥ ÏûëÏÑ±Ìïú Î™®Îì† Ìè¨Ïä§Ìä∏ ÏÇ≠Ï†ú
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        # 1. Í∞ÄÏÉÅÌöåÏõê ID Î™©Î°ù Ï°∞Ìöå
        artists = supabase_client.table("profiles").select("id, full_name").eq(
            "member_type", "artist"
        ).execute()

        if not artists.data:
            return {"status": "no_artists", "deleted": 0}

        artist_ids = [a["id"] for a in artists.data]
        artist_names = [a.get("full_name", "Unknown") for a in artists.data]

        # 2. Ìï¥Îãπ ÏïÑÌã∞Ïä§Ìä∏Îì§Ïùò Ìè¨Ïä§Ìä∏ ÏÇ≠Ï†ú
        deleted_count = 0
        for artist_id in artist_ids:
            result = supabase_client.table("posts").delete().eq(
                "user_id", artist_id
            ).execute()
            if result.data:
                deleted_count += len(result.data)

        return {
            "status": "success",
            "deleted": deleted_count,
            "artists_checked": artist_names
        }

    except Exception as e:
        logger.error(f"Delete virtual member posts error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/admin/migrate-artist-languages")
async def migrate_artist_languages(request: Request):
    """
    ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò: Í∏∞Ï°¥ ÏïÑÌã∞Ïä§Ìä∏Îì§Ïùò primary_language ÏóÖÎç∞Ïù¥Ìä∏

    Î™®Îì† music_artists Î†àÏΩîÎìúÎ•º Ï°∞ÌöåÌïòÏó¨:
    1. Ïù¥Î¶ÑÏóêÏÑú Ïñ∏Ïñ¥ Í∞êÏßÄ
    2. K-pop Í∑∏Î£π Î™©Î°ù Ï≤¥ÌÅ¨
    3. primary_language ÌïÑÎìú ÏóÖÎç∞Ïù¥Ìä∏
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        # Î™®Îì† ÏïÑÌã∞Ïä§Ìä∏ Ï°∞Ìöå
        artists = supabase_client.table("music_artists").select(
            "browse_id, name, description, primary_language"
        ).execute()

        if not artists.data:
            return {"status": "no_artists", "updated": 0}

        updated = 0
        results = []

        for artist in artists.data:
            name = artist.get("name") or ""
            description = artist.get("description") or ""
            current_lang = artist.get("primary_language")

            # Ïñ∏Ïñ¥ Í∞êÏßÄ
            detected_lang = detect_artist_language(name, description)

            # Î≥ÄÍ≤ΩÏù¥ ÌïÑÏöîÌïú Í≤ΩÏö∞Îßå ÏóÖÎç∞Ïù¥Ìä∏
            if detected_lang != current_lang:
                supabase_client.table("music_artists").update({
                    "primary_language": detected_lang
                }).eq("browse_id", artist["browse_id"]).execute()

                updated += 1
                results.append({
                    "name": name,
                    "old": current_lang,
                    "new": detected_lang
                })
                logger.info(f"Updated language: {name} -> {detected_lang}")

        return {
            "status": "success",
            "total_artists": len(artists.data),
            "updated": updated,
            "changes": results[:50]  # ÏµúÎåÄ 50Í∞úÎßå Î∞òÌôò
        }

    except Exception as e:
        logger.error(f"Migration error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/messages/auto-reply")
async def auto_reply_to_virtual_member(request: Request):
    """
    REACTIVE AI Response: Called when a user sends a message to a virtual member.
    The AI responds ONLY when the user initiates the conversation.

    Flow:
    1. User sends message to virtual member (artist)
    2. Frontend calls this endpoint with conversation_id and the user's message
    3. AI generates a response based on artist persona
    4. Response is inserted into the conversation
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        body = await request.json()
        conversation_id = body.get("conversationId")
        user_message = body.get("userMessage")
        recipient_id = body.get("recipientId")  # The virtual member's user ID

        if not conversation_id or not user_message or not recipient_id:
            raise HTTPException(status_code=400, detail="conversationId, userMessage, and recipientId required")

        # 1. Check if recipient is a virtual member (artist)
        recipient = supabase_client.table("profiles").select(
            "id, username, full_name, ai_persona, member_type, artist_browse_id"
        ).eq("id", recipient_id).single().execute()

        if not recipient.data:
            raise HTTPException(status_code=404, detail="Recipient not found")

        if recipient.data.get("member_type") != "artist":
            # Not a virtual member, no auto-reply needed
            return {"status": "skipped", "reason": "Recipient is not a virtual member"}

        # 2. Get artist info for AI response
        artist_name = recipient.data.get("full_name") or recipient.data.get("username")
        persona = recipient.data.get("ai_persona") or {}

        # If no persona exists, create a basic one
        if not persona:
            persona = {
                "system_prompt": f"You are {artist_name}, a music artist chatting with a fan. Be friendly, warm, and authentic. Keep responses short (1-2 sentences).",
                "tone": "friendly, warm, casual",
                "greeting": f"Hey! Thanks for reaching out! üíï"
            }

        # 3. Get recent conversation history for context
        history = []
        try:
            history_result = supabase_client.table("messages").select(
                "sender_id, content"
            ).eq("conversation_id", conversation_id).order(
                "created_at", desc=False
            ).limit(10).execute()

            if history_result.data:
                for msg in history_result.data:
                    role = "model" if msg["sender_id"] == recipient_id else "user"
                    history.append({"role": role, "content": msg["content"]})
        except Exception as hist_error:
            logger.warning(f"Could not fetch history: {hist_error}")

        # 4. Generate AI response using chat function
        response_text = await run_in_thread(
            chat_with_artist,
            persona,
            history,
            user_message
        )

        if not response_text or response_text == "...":
            return {"status": "error", "message": "Failed to generate AI response"}

        # 4. Insert AI response into conversation
        message_result = supabase_client.table("messages").insert({
            "conversation_id": conversation_id,
            "sender_id": recipient_id,  # The virtual member is the sender
            "content": response_text
        }).execute()

        logger.info(f"AI reply from {artist_name}: {response_text[:50]}...")

        return {
            "status": "success",
            "response": response_text,
            "messageId": message_result.data[0]["id"] if message_result.data else None
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Auto-reply error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/comments/auto-reply")
async def auto_reply_to_artist_post(request: Request):
    """
    REACTIVE AI Response: When a user comments on an artist's post,
    the artist may reply back.

    Flow:
    1. User comments on artist's post
    2. Frontend calls this endpoint
    3. AI decides whether to reply and generates response
    4. Reply is inserted as a comment
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        body = await request.json()
        post_id = body.get("postId")
        user_comment = body.get("userComment")
        post_owner_id = body.get("postOwnerId")

        if not post_id or not user_comment or not post_owner_id:
            raise HTTPException(status_code=400, detail="postId, userComment, and postOwnerId required")

        # 1. Check if post owner is a virtual member
        owner = supabase_client.table("profiles").select(
            "id, username, full_name, ai_persona, member_type"
        ).eq("id", post_owner_id).single().execute()

        if not owner.data or owner.data.get("member_type") != "artist":
            return {"status": "skipped", "reason": "Post owner is not a virtual member"}

        # 2. Get artist info
        artist_name = owner.data.get("full_name") or owner.data.get("username")
        persona = owner.data.get("ai_persona") or {}

        # 3. Generate AI reply comment
        reply_text = await run_in_thread(
            generate_artist_comment,
            artist_name,
            persona,
            user_comment
        )

        if not reply_text:
            return {"status": "skipped", "reason": "AI chose not to reply"}

        # 4. Insert reply comment
        comment_result = supabase_client.table("post_comments").insert({
            "post_id": post_id,
            "user_id": post_owner_id,
            "content": reply_text
        }).execute()

        # Update comment count
        post = supabase_client.table("posts").select("comment_count").eq("id", post_id).single().execute()
        if post.data:
            supabase_client.table("posts").update({
                "comment_count": (post.data.get("comment_count") or 0) + 1
            }).eq("id", post_id).execute()

        logger.info(f"AI comment from {artist_name}: {reply_text[:50]}...")

        return {
            "status": "success",
            "reply": reply_text,
            "commentId": comment_result.data[0]["id"] if comment_result.data else None
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Comment auto-reply error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/cron/test-activity")
async def test_artist_activity():
    """
    Test endpoint to manually trigger one artist post.
    For debugging/demo purposes. Supports multilingual posts.
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        # Get a random virtual member
        artists_result = supabase_client.table("profiles").select(
            "id, username, full_name, avatar_url, ai_persona, artist_browse_id"
        ).eq("member_type", "artist").limit(10).execute()

        if not artists_result.data:
            return {"status": "error", "message": "No virtual members found. Run /api/virtual-members/migrate-all first."}

        artist = random.choice(artists_result.data)
        persona = artist.get("ai_persona") or {}
        artist_name = artist.get("full_name") or artist.get("username")
        browse_id = artist.get("artist_browse_id")

        # Get artist's primary language
        artist_language = "en"
        if browse_id:
            music_artist = supabase_client.table("music_artists").select(
                "primary_language"
            ).eq("browse_id", browse_id).limit(1).execute()
            if music_artist.data:
                artist_language = music_artist.data[0].get("primary_language") or "en"

        # Generate multilingual post
        post_data = await run_in_thread(
            generate_artist_post_multilingual,
            artist_name,
            persona,
            artist_language
        )

        if not post_data:
            return {"status": "error", "message": "Failed to generate post content"}

        # Insert post with language
        new_post = {
            "user_id": artist["id"],
            "caption": post_data.get("caption", "Hello from AI!"),
            "language": artist_language,
            "is_public": True,
            "like_count": 0,
            "comment_count": 0,
            "repost_count": 0
        }

        result = supabase_client.table("posts").insert(new_post).execute()

        return {
            "status": "success",
            "artist": artist_name,
            "language": artist_language,
            "post": post_data,
            "db_result": result.data[0] if result.data else None
        }

    except Exception as e:
        logger.error(f"Test activity error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# Translation API Endpoints
# =============================================================================

@app.post("/api/translate")
async def translate_post_content(request: Request):
    """
    Translate post content to target language with caching.

    Request body:
    - post_id: UUID of the post (optional, for caching)
    - text: Text to translate
    - target_language: ISO 639-1 code (e.g., 'ko', 'en', 'ja')
    - source_language: Optional source language code (auto-detected if not provided)

    Returns:
    - translated_text: The translated text
    - source_language: Detected/provided source language
    - cached: Whether the result was from cache
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        body = await request.json()
        text = body.get("text", "").strip()
        target_lang = body.get("target_language", "en")
        source_lang = body.get("source_language")
        post_id = body.get("post_id")

        if not text:
            raise HTTPException(status_code=400, detail="text is required")

        if not target_lang:
            raise HTTPException(status_code=400, detail="target_language is required")

        # Check cache if post_id provided
        if post_id:
            cache_result = supabase_client.table("post_translations").select(
                "translated_text, source_language"
            ).eq("post_id", post_id).eq("target_language", target_lang).limit(1).execute()

            if cache_result.data:
                return {
                    "translated_text": cache_result.data[0]["translated_text"],
                    "source_language": cache_result.data[0]["source_language"],
                    "cached": True
                }

        # Detect source language if not provided
        if not source_lang:
            source_lang = await run_in_thread(detect_language, text)

        # If already in target language, return original
        if source_lang == target_lang:
            return {
                "translated_text": text,
                "source_language": source_lang,
                "cached": False,
                "same_language": True
            }

        # Translate using AI
        translated = await run_in_thread(translate_text, text, source_lang, target_lang)

        if not translated:
            raise HTTPException(status_code=500, detail="Translation failed")

        # Cache the result if post_id provided
        if post_id:
            try:
                supabase_client.table("post_translations").upsert({
                    "post_id": post_id,
                    "source_language": source_lang,
                    "target_language": target_lang,
                    "original_text": text,
                    "translated_text": translated
                }, on_conflict="post_id,target_language").execute()
            except Exception as cache_error:
                logger.warning(f"Failed to cache translation: {cache_error}")

        return {
            "translated_text": translated,
            "source_language": source_lang,
            "cached": False
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Translation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/translate/cached/{post_id}")
async def get_cached_translation(post_id: str, target_language: str = "en"):
    """
    Get cached translation for a specific post.
    Returns null if not cached.
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        result = supabase_client.table("post_translations").select(
            "translated_text, source_language, created_at"
        ).eq("post_id", post_id).eq("target_language", target_language).limit(1).execute()

        if result.data:
            return {
                "translated_text": result.data[0]["translated_text"],
                "source_language": result.data[0]["source_language"],
                "cached_at": result.data[0]["created_at"]
            }

        return {"translated_text": None}

    except Exception as e:
        logger.error(f"Get cached translation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/artist/check-new-releases")
async def check_artist_new_releases(request: Request):
    """
    Check for new album/single releases for an artist.

    Request body:
    - artist_browse_id: YouTube Music artist browse ID (required)
    - country: Country code for region-specific results (default: US)

    Returns:
    - new_albums: List of newly detected albums
    - new_singles: List of newly detected singles
    - has_new_releases: Boolean indicating if any new releases found
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    try:
        body = await request.json()
        artist_browse_id = body.get("artist_browse_id", "").strip()
        country = body.get("country", "US")

        if not artist_browse_id:
            raise HTTPException(status_code=400, detail="artist_browse_id is required")

        ytmusic = get_ytmusic(country)

        # Get current releases from YouTube Music
        artist_info = await run_in_thread(ytmusic.get_artist, artist_browse_id)
        if not artist_info:
            raise HTTPException(status_code=404, detail=ERROR_ARTIST_NOT_FOUND)

        current_albums = []
        current_singles = []

        # Extract albums
        albums_section = artist_info.get("albums", {})
        if isinstance(albums_section, dict):
            browse_id = albums_section.get("browseId")
            params = albums_section.get("params")
            if browse_id and params:
                try:
                    album_list = await run_in_thread(ytmusic.get_artist_albums, browse_id, params)
                    for album in (album_list or []):
                        if isinstance(album, dict) and album.get("browseId"):
                            current_albums.append({
                                "id": album.get("browseId"),
                                "title": album.get("title"),
                                "year": album.get("year"),
                                "thumbnails": album.get("thumbnails", [])
                            })
                except Exception as e:
                    logger.warning(f"Failed to get albums: {e}")
                    for album in albums_section.get("results", []):
                        if isinstance(album, dict) and album.get("browseId"):
                            current_albums.append({
                                "id": album.get("browseId"),
                                "title": album.get("title"),
                                "year": album.get("year"),
                                "thumbnails": album.get("thumbnails", [])
                            })

        # Extract singles
        singles_section = artist_info.get("singles", {})
        if isinstance(singles_section, dict):
            browse_id = singles_section.get("browseId")
            params = singles_section.get("params")
            if browse_id and params:
                try:
                    single_list = await run_in_thread(ytmusic.get_artist_albums, browse_id, params)
                    for single in (single_list or []):
                        if isinstance(single, dict) and single.get("browseId"):
                            current_singles.append({
                                "id": single.get("browseId"),
                                "title": single.get("title"),
                                "year": single.get("year"),
                                "thumbnails": single.get("thumbnails", [])
                            })
                except Exception as e:
                    logger.warning(f"Failed to get singles: {e}")
                    for single in singles_section.get("results", []):
                        if isinstance(single, dict) and single.get("browseId"):
                            current_singles.append({
                                "id": single.get("browseId"),
                                "title": single.get("title"),
                                "year": single.get("year"),
                                "thumbnails": single.get("thumbnails", [])
                            })

        # Get stored release data
        stored_result = supabase_client.table("artist_releases").select(
            "known_album_ids, known_single_ids"
        ).eq("artist_browse_id", artist_browse_id).limit(1).execute()

        known_album_ids = set()
        known_single_ids = set()

        if stored_result.data:
            known_album_ids = set(stored_result.data[0].get("known_album_ids", []) or [])
            known_single_ids = set(stored_result.data[0].get("known_single_ids", []) or [])

        # Find new releases
        current_album_ids = {a["id"] for a in current_albums}
        current_single_ids = {s["id"] for s in current_singles}

        new_album_ids = current_album_ids - known_album_ids
        new_single_ids = current_single_ids - known_single_ids

        new_albums = [a for a in current_albums if a["id"] in new_album_ids]
        new_singles = [s for s in current_singles if s["id"] in new_single_ids]

        # Update stored data
        update_data = {
            "artist_browse_id": artist_browse_id,
            "known_album_ids": list(current_album_ids),
            "known_single_ids": list(current_single_ids),
            "last_checked_at": datetime.now(timezone.utc).isoformat()
        }

        if new_albums or new_singles:
            update_data["last_new_release_at"] = datetime.now(timezone.utc).isoformat()

        supabase_client.table("artist_releases").upsert(
            update_data,
            on_conflict="artist_browse_id"
        ).execute()

        return {
            "artist_name": artist_info.get("name", "Unknown"),
            "new_albums": new_albums,
            "new_singles": new_singles,
            "has_new_releases": len(new_albums) > 0 or len(new_singles) > 0,
            "total_albums": len(current_albums),
            "total_singles": len(current_singles)
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Check new releases error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/cron/check-all-artist-releases")
async def check_all_artist_releases(request: Request, background_tasks: BackgroundTasks):
    """
    Cron job: Check all followed artists for new releases.
    Runs in background and creates posts for new releases.
    """
    if not supabase_client:
        raise HTTPException(status_code=500, detail=ERROR_DB_NOT_CONFIGURED)

    # Verify admin access
    body = await request.json() if request.headers.get("content-type") == "application/json" else {}
    secret = body.get("secret") or request.query_params.get("secret")

    if secret != os.getenv("CRON_SECRET", "dev-secret"):
        raise HTTPException(status_code=403, detail="Invalid secret")

    async def process_releases():
        try:
            # Get all virtual member artists with browse_id
            artists_result = supabase_client.table("music_artists").select(
                "browse_id, name, primary_language"
            ).not_.is_("browse_id", "null").limit(50).execute()

            if not artists_result.data:
                logger.info("No artists to check for new releases")
                return

            for artist in artists_result.data:
                browse_id = artist.get("browse_id")
                if not browse_id:
                    continue

                try:
                    # Check for new releases
                    ytmusic = get_ytmusic("US")
                    artist_info = ytmusic.get_artist(browse_id)

                    if not artist_info:
                        continue

                    # Similar logic as above endpoint...
                    # (Simplified for background task)
                    logger.info(f"Checked releases for {artist.get('name')}")

                except Exception as e:
                    logger.warning(f"Failed to check releases for {browse_id}: {e}")
                    continue

                # Small delay between artists
                await asyncio.sleep(0.5)

        except Exception as e:
            logger.error(f"Background release check error: {e}")

    background_tasks.add_task(process_releases)
    return {"status": "started", "message": "Release check started in background"}


@app.post("/api/detect-language")
async def detect_language_endpoint(request: Request):
    """
    Detect the language of given text.

    Request body:
    - text: Text to analyze

    Returns:
    - language: ISO 639-1 code
    - language_name: Full language name
    """
    try:
        body = await request.json()
        text = body.get("text", "").strip()

        if not text:
            raise HTTPException(status_code=400, detail="text is required")

        lang_code = await run_in_thread(detect_language, text)

        # Language names mapping
        lang_names = {
            'en': 'English', 'ko': 'Korean', 'ja': 'Japanese', 'zh': 'Chinese',
            'es': 'Spanish', 'fr': 'French', 'de': 'German', 'pt': 'Portuguese',
            'id': 'Indonesian', 'ar': 'Arabic', 'hi': 'Hindi', 'ru': 'Russian',
            'tr': 'Turkish', 'vi': 'Vietnamese', 'th': 'Thai', 'nl': 'Dutch',
            'pl': 'Polish', 'it': 'Italian', 'sw': 'Swahili', 'yo': 'Yoruba',
            'zu': 'Zulu', 'am': 'Amharic', 'ha': 'Hausa', 'ig': 'Igbo'
        }

        return {
            "language": lang_code,
            "language_name": lang_names.get(lang_code, lang_code.upper())
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Language detection error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8080))
    uvicorn.run(app, host="0.0.0.0", port=port)
